{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # ACE2 Offline Learning — Interactive Demo\n",
    "\n",
    "\n",
    "\n",
    " This notebook walks through the ACE2 pipeline engine for **offline**\n",
    "\n",
    " (multi-epoch, batch) learning.  It covers:\n",
    "\n",
    "\n",
    "\n",
    " 1. Defining a task environment\n",
    "\n",
    " 2. Building an `OfflineACE` runner (two ways)\n",
    "\n",
    " 3. Running single-epoch and multi-epoch training\n",
    "\n",
    " 4. Inspecting results and the learned skillbook\n",
    "\n",
    " 5. Checkpointing and persistence\n",
    "\n",
    " 6. Stepping through the pipeline manually\n",
    "\n",
    "\n",
    "\n",
    " **Requirements:** `uv sync` (or `pip install -e .` from the repo root).\n",
    "\n",
    " Set your LLM API key before running:\n",
    "\n",
    " ```bash\n",
    "\n",
    " export OPENAI_API_KEY=\"sk-...\"\n",
    "\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/Desktop/projects/Kayba/agentic-context-engine/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imports OK\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Ensure the project root is on sys.path so `ace`, `ace2`, and `pipeline`\n",
    "# are importable regardless of where the notebook kernel starts.\n",
    "_here = Path(__file__).resolve().parent if \"__file__\" in dir() else Path.cwd()\n",
    "_root = _here\n",
    "for _p in [_here] + list(_here.parents):\n",
    "    if (_p / \"pipeline\" / \"__init__.py\").exists():\n",
    "        _root = _p\n",
    "        break\n",
    "sys.path.insert(0, str(_root))\n",
    "\n",
    "# Load .env from project root (BEDROCK_API_KEY, OPENAI_API_KEY, etc.)\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(_root / \".env\")\n",
    "\n",
    "from ace.adaptation import EnvironmentResult, Sample, TaskEnvironment\n",
    "from ace.skillbook import Skillbook\n",
    "from ace2.pipelines import OfflineACE\n",
    "\n",
    "print(\"Imports OK\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 2. Define a Task Environment\n",
    "\n",
    "\n",
    "\n",
    " A `TaskEnvironment` evaluates the agent's answer against each sample's\n",
    "\n",
    " ground truth.  The ACE loop uses this feedback to drive reflection and\n",
    "\n",
    " skill updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment defined\n"
     ]
    }
   ],
   "source": [
    "class CapitalCityEnvironment(TaskEnvironment):\n",
    "    \"\"\"Score agent answers against capital-city ground truth.\"\"\"\n",
    "\n",
    "    def evaluate(self, sample: Sample, agent_output) -> EnvironmentResult:\n",
    "        expected = (sample.ground_truth or \"\").strip().lower()\n",
    "        predicted = agent_output.final_answer.strip().lower()\n",
    "        correct = predicted == expected\n",
    "        return EnvironmentResult(\n",
    "            feedback=\"Correct!\" if correct else f\"Wrong. Expected: {sample.ground_truth}\",\n",
    "            ground_truth=sample.ground_truth,\n",
    "            metrics={\"accuracy\": 1.0 if correct else 0.0},\n",
    "        )\n",
    "\n",
    "env = CapitalCityEnvironment()\n",
    "print(\"Environment defined\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 3. Prepare Training Samples\n",
    "\n",
    "\n",
    "\n",
    " Each `Sample` has a question the agent must answer and a ground-truth\n",
    "\n",
    " label the environment uses for scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 5 training samples\n"
     ]
    }
   ],
   "source": [
    "samples = [\n",
    "    Sample(question=\"What is the capital of France?\", ground_truth=\"Paris\"),\n",
    "    Sample(question=\"What is the capital of Japan?\", ground_truth=\"Tokyo\"),\n",
    "    Sample(question=\"What is the capital of Brazil?\", ground_truth=\"Brasilia\"),\n",
    "    Sample(question=\"What is the capital of Australia?\", ground_truth=\"Canberra\"),\n",
    "    Sample(question=\"What is the capital of Nigeria?\", ground_truth=\"Abuja\"),\n",
    "]\n",
    "\n",
    "print(f\"Prepared {len(samples)} training samples\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 4. Build OfflineACE — The Easy Way\n",
    "\n",
    "\n",
    "\n",
    " `from_client` creates the Agent, Reflector, and SkillManager internally\n",
    "\n",
    " from a single LLM client.  This is the fastest way to get started."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import os\nfrom ace.llm_providers.litellm_client import LiteLLMClient\n\nMODEL = \"us.anthropic.claude-haiku-4-5-20251001-v1:0\"\nAPI_KEY = os.getenv(\"BEDROCK_API_KEY\")\n\nclient = LiteLLMClient(model=MODEL, api_key=API_KEY)\nskillbook = Skillbook()\n\nace = OfflineACE.from_client(client, skillbook=skillbook)\n\nprint(f\"OfflineACE ready  |  pipeline steps: {len(ace._steps)}\")\nprint(f\"Pipeline provides: {ace.provides}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 5. Run — Single Epoch\n",
    "\n",
    "\n",
    "\n",
    " One pass over every sample: Agent answers, environment evaluates,\n",
    "\n",
    " Reflector analyses, SkillManager updates the skillbook.\n",
    "\n",
    "\n",
    "\n",
    " ```\n",
    "\n",
    " AgentStep → EvaluateStep → ReflectStep → UpdateStep\n",
    "\n",
    " ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Started logging traces to the \"ace-roles\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=019c7a3c-72e0-711d-83b8-816b7291a7cb&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n",
      "\u001b[92m09:48:27 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:48:34 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Started logging traces to the \"ace-framework\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=069981fe-204b-74ed-8000-d501cbced07d&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n",
      "\u001b[92m09:48:34 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:48:44 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Started logging traces to the \"ace-roles\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=019c7a3c-8ae5-78f1-8b8a-498faaf04853&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n",
      "\u001b[92m09:48:44 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:48:54 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:48:54 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n",
      "OPIK: Started logging traces to the \"ace-framework\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=069981ff-60d5-7fe9-8000-9c60955ce0b1&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:49:02 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Started logging traces to the \"ace-roles\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=019c7a3c-b32f-70c2-a8c4-862b84ba501b&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n",
      "\u001b[92m09:49:02 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:49:09 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Started logging traces to the \"ace-framework\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=06998200-570f-7a6c-8000-ae6b3b0a4c74&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n",
      "\u001b[92m09:49:09 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:49:20 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Started logging traces to the \"ace-roles\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=019c7a3d-153a-731e-865b-d08b2d4df177&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n",
      "\u001b[92m09:49:20 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:49:29 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:49:29 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Started logging traces to the \"ace-framework\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=06998201-9a04-7a4d-8000-26e5d20c8ccd&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n",
      "\u001b[92m09:49:37 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Started logging traces to the \"ace-roles\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=019c7a3d-3f9e-761d-b25d-e4cb24398584&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n",
      "\u001b[92m09:49:37 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n",
      "OPIK: Started logging traces to the \"ace-framework\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=06998202-1cdc-70d7-8000-98d3b5c37c77&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:49:44 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Started logging traces to the \"ace-roles\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=019c7a3d-8402-728b-a93d-88e6b69f7dad&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n",
      "\u001b[92m09:49:44 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:49:56 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Started logging traces to the \"ace-framework\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=06998203-4fdf-7df9-8000-c80992dcc0c0&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n",
      "\u001b[92m09:49:57 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:50:08 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:50:08 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:50:17 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Started logging traces to the \"ace-roles\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=019c7a3d-cef8-7cb5-8900-af57348cd293&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n",
      "\u001b[92m09:50:17 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Started logging traces to the \"ace-framework\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=06998204-991a-79e7-8000-08f3e1232fd7&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n",
      "\u001b[92m09:50:24 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Started logging traces to the \"ace-roles\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=019c7a3e-1f59-7989-86d2-f3212083c969&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n",
      "\u001b[92m09:50:24 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:50:34 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:50:34 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n",
      "OPIK: Started logging traces to the \"ace-framework\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=06998205-af18-7d36-8000-54b8b294ff00&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:50:43 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:50:43 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:50:50 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Started logging traces to the \"ace-roles\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=019c7a3e-6332-728f-b3e7-00a54be8ca07&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n",
      "\u001b[92m09:50:50 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:50:58 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:50:58 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n",
      "OPIK: Started logging traces to the \"ace-framework\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=06998207-25ac-747b-8000-c9b345dfa07b&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:51:10 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Started logging traces to the \"ace-roles\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=019c7a3e-bea4-702e-91f3-544456d039ae&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n",
      "\u001b[92m09:51:10 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n",
      "OPIK: Started logging traces to the \"ace-framework\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=06998207-ecb1-7197-8000-6c70ea0e1aff&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:51:20 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:51:20 - LiteLLM:INFO\u001b[0m: utils.py:3419 - \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] \n",
      "LiteLLM completion() model= us.anthropic.claude-haiku-4-5-20251001-v1:0; provider = bedrock\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[92m09:51:28 - LiteLLM:INFO\u001b[0m: utils.py:1308 - Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO     [LiteLLM] Wrapper: Completed Call, calling success_handler\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Started logging traces to the \"ace-roles\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=019c7a3e-ef39-7887-8c16-894f9b0c462c&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 5 sample(s)\n",
      "\n",
      "  [1] Q: What is the capital of France?\n",
      "       A: Paris is the capital of France.  |  Wrong. Expected: Paris\n",
      "  [2] Q: What is the capital of Japan?\n",
      "       A: Tokyo is the capital of Japan.  |  Wrong. Expected: Tokyo\n",
      "  [3] Q: What is the capital of Brazil?\n",
      "       A: The capital of Brazil is Brasília. It was purpose-built and inaugurated in 1960, replacing Rio de Janeiro as the capital city.  |  Wrong. Expected: Brasilia\n",
      "  [4] Q: What is the capital of Australia?\n",
      "       A: Canberra  |  Correct!\n",
      "  [5] Q: What is the capital of Nigeria?\n",
      "       A: Abuja  |  Correct!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OPIK: Started logging traces to the \"ace-framework\" project at https://www.comet.com/opik/api/v1/session/redirect/projects/?trace_id=06998209-0949-7ffb-8000-8049698bf10a&path=aHR0cHM6Ly93d3cuY29tZXQuY29tL29waWsvYXBpLw==.\n"
     ]
    }
   ],
   "source": [
    "results = ace.run(samples, env, epochs=1)\n",
    "\n",
    "print(f\"Processed {len(results)} sample(s)\\n\")\n",
    "for i, r in enumerate(results, 1):\n",
    "    if r.error:\n",
    "        print(f\"  [{i}] ERROR: {r.error}\")\n",
    "    else:\n",
    "        answer = r.output.agent_output.final_answer\n",
    "        feedback = r.output.environment_result.feedback\n",
    "        print(f\"  [{i}] Q: {r.sample.question}\")\n",
    "        print(f\"       A: {answer}  |  {feedback}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\nSkillbook after 1 epoch:\")\n",
    "print(f\"  {skillbook.stats()}\\n\")\n",
    "print(skillbook)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 6. Run — Multi-Epoch Training\n",
    "\n",
    "\n",
    "\n",
    " Multiple epochs let the agent revisit samples with an updated skillbook.\n",
    "\n",
    " Skills accumulate and refine across passes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start fresh for a clean multi-epoch demo\n",
    "client2 = LiteLLMClient(model=MODEL, api_key=API_KEY)\n",
    "skillbook2 = Skillbook()\n",
    "ace2 = OfflineACE.from_client(client2, skillbook=skillbook2)\n",
    "\n",
    "results = ace2.run(samples, env, epochs=3)\n",
    "\n",
    "print(f\"Total results across 3 epochs: {len(results)}\")\n",
    "\n",
    "correct = sum(\n",
    "    1 for r in results\n",
    "    if r.error is None\n",
    "    and r.output.environment_result.metrics.get(\"accuracy\", 0) == 1.0\n",
    ")\n",
    "print(f\"Correct answers: {correct}/{len(results)}\")\n",
    "print(f\"Skills learned:  {skillbook2.stats()['skills']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 7. Build OfflineACE — The Flexible Way\n",
    "\n",
    "\n",
    "\n",
    " `from_roles` lets you customise each role individually: different prompt\n",
    "\n",
    " templates, deduplication config, reflection window size, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ace.roles import Agent, Reflector, SkillManager\n",
    "\n",
    "client3 = LiteLLMClient(model=MODEL, api_key=API_KEY)\n",
    "skillbook3 = Skillbook()\n",
    "\n",
    "ace3 = OfflineACE.from_roles(\n",
    "    agent=Agent(client3),\n",
    "    reflector=Reflector(client3),\n",
    "    skill_manager=SkillManager(client3),\n",
    "    skillbook=skillbook3,\n",
    "    reflection_window=5,       # keep last 5 reflections in the rolling window\n",
    "    max_refinement_rounds=1,   # reflector passes per sample\n",
    ")\n",
    "\n",
    "results = ace3.run(samples[:2], env, epochs=1)\n",
    "\n",
    "for r in results:\n",
    "    if r.error is None:\n",
    "        print(f\"  Q: {r.sample.question}\")\n",
    "        print(f\"  A: {r.output.agent_output.final_answer}\")\n",
    "        print(f\"  Insight: {r.output.reflection.key_insight}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 8. Checkpointing\n",
    "\n",
    "\n",
    "\n",
    " Save the skillbook every N successful samples so you can resume after\n",
    "\n",
    " interruption or compare skillbook evolution over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tempfile\n",
    "\n",
    "client4 = LiteLLMClient(model=MODEL, api_key=API_KEY)\n",
    "skillbook4 = Skillbook()\n",
    "ace4 = OfflineACE.from_client(client4, skillbook=skillbook4)\n",
    "\n",
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    results = ace4.run(\n",
    "        samples,\n",
    "        env,\n",
    "        epochs=1,\n",
    "        checkpoint_interval=2,   # save every 2 successful samples\n",
    "        checkpoint_dir=tmpdir,\n",
    "    )\n",
    "\n",
    "    saved = sorted(Path(tmpdir).glob(\"*.json\"))\n",
    "    print(\"Checkpoint files:\")\n",
    "    for f in saved:\n",
    "        print(f\"  {f.name}  ({f.stat().st_size} bytes)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 9. Persistence — Save & Reload\n",
    "\n",
    "\n",
    "\n",
    " Save the learned skillbook to disk and reload it in a future session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tempfile.TemporaryDirectory() as tmpdir:\n",
    "    path = Path(tmpdir) / \"learned_skillbook.json\"\n",
    "\n",
    "    # Save\n",
    "    skillbook2.save_to_file(str(path))\n",
    "    print(f\"Saved to {path.name}  ({path.stat().st_size} bytes)\")\n",
    "\n",
    "    # Reload\n",
    "    reloaded = Skillbook.from_file(str(path))\n",
    "    print(f\"Reloaded: {reloaded.stats()}\")\n",
    "    print(f\"Skills match: {reloaded.stats() == skillbook2.stats()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 10. Manual Pipeline Walkthrough\n",
    "\n",
    "\n",
    "\n",
    " Under the hood, `OfflineACE.run()` builds a `StepContext` for each\n",
    "\n",
    " sample and calls the four-step pipeline.  Here we do it by hand to\n",
    "\n",
    " show what each step produces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pipeline import StepContext\n",
    "from ace2.steps import AgentStep, EvaluateStep, ReflectStep, UpdateStep\n",
    "from ace2.pipelines import ace_pipeline\n",
    "\n",
    "client5 = LiteLLMClient(model=MODEL, api_key=API_KEY)\n",
    "skillbook5 = Skillbook()\n",
    "\n",
    "pipe = ace_pipeline(\n",
    "    Agent(client5),\n",
    "    Reflector(client5),\n",
    "    SkillManager(client5),\n",
    ")\n",
    "\n",
    "sample = samples[0]\n",
    "ctx = StepContext(\n",
    "    sample=sample,\n",
    "    skillbook=skillbook5,\n",
    "    environment=env,\n",
    "    epoch=1,\n",
    "    total_epochs=1,\n",
    "    step_index=1,\n",
    "    total_steps=1,\n",
    "    recent_reflections=(),\n",
    ")\n",
    "\n",
    "print(f\"Before pipeline:\")\n",
    "print(f\"  Skills: {skillbook5.stats()['skills']}\")\n",
    "print(f\"  agent_output: {ctx.agent_output}\")\n",
    "print()\n",
    "\n",
    "# Run the full pipeline on a single context\n",
    "out = pipe(ctx)\n",
    "\n",
    "print(f\"After pipeline:\")\n",
    "print(f\"  Agent answer:     {out.agent_output.final_answer}\")\n",
    "print(f\"  Env feedback:     {out.environment_result.feedback}\")\n",
    "print(f\"  Reflector insight: {out.reflection.key_insight}\")\n",
    "print(f\"  Skills now:       {skillbook5.stats()['skills']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## 11. Error Handling\n",
    "\n",
    "\n",
    "\n",
    " Failed samples are captured in `SampleResult.error` — the pipeline\n",
    "\n",
    " never drops a sample silently.  Other samples continue processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_samples = [\n",
    "    samples[0],\n",
    "    Sample(question=\"\", ground_truth=\"\"),  # might trigger edge cases\n",
    "    samples[1],\n",
    "]\n",
    "\n",
    "client6 = LiteLLMClient(model=MODEL, api_key=API_KEY)\n",
    "ace6 = OfflineACE.from_client(client6)\n",
    "\n",
    "results = ace6.run(bad_samples, env, epochs=1)\n",
    "\n",
    "for i, r in enumerate(results, 1):\n",
    "    status = \"OK\" if r.error is None else f\"FAIL ({r.failed_at})\"\n",
    "    answer = r.output.agent_output.final_answer if r.output else \"—\"\n",
    "    print(f\"  [{i}] {status:20s}  answer={answer}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ---\n",
    "\n",
    " ## Summary\n",
    "\n",
    "\n",
    "\n",
    " | What | How |\n",
    "\n",
    " |------|-----|\n",
    "\n",
    " | Quick start | `OfflineACE.from_client(llm_client)` |\n",
    "\n",
    " | Custom roles | `OfflineACE.from_roles(agent=..., reflector=..., skill_manager=...)` |\n",
    "\n",
    " | Single epoch | `ace.run(samples, env, epochs=1)` |\n",
    "\n",
    " | Multi-epoch | `ace.run(samples, env, epochs=3)` |\n",
    "\n",
    " | Checkpointing | `ace.run(..., checkpoint_interval=10, checkpoint_dir=\"./ckpts\")` |\n",
    "\n",
    " | Save skillbook | `skillbook.save_to_file(\"path.json\")` |\n",
    "\n",
    " | Load skillbook | `Skillbook.from_file(\"path.json\")` |\n",
    "\n",
    " | Manual stepping | Build pipeline with `ace_pipeline()`, call `pipe(ctx)` |\n",
    "\n",
    " | Inspect results | `result.output.agent_output`, `.environment_result`, `.reflection` |\n",
    "\n",
    "\n",
    "\n",
    " The pipeline runs: **AgentStep → EvaluateStep → ReflectStep → UpdateStep**\n",
    "\n",
    "\n",
    "\n",
    " ReflectStep and UpdateStep run in a background thread pool by default\n",
    "\n",
    " (async boundary), so the agent returns fast while learning continues."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ace-framework (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}