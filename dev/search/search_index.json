{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"ACE Framework","text":"<p>Agentic Context Engineering \u2014 a framework for self-improving language model agents.</p> <p>ACE enables AI agents to learn from their own execution feedback through three collaborative roles: Agent, Reflector, and SkillManager.</p>"},{"location":"#how-it-works","title":"How It Works","text":"<pre><code>Sample \u2192 Agent \u2192 Environment \u2192 Reflector \u2192 SkillManager \u2192 Skillbook\n                    (feedback)   (analyzes)   (updates)     (context)\n</code></pre> <p>The Skillbook accumulates strategies across runs, making every subsequent agent call smarter.</p>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Quick Start \u2014 up and running in minutes</li> <li>Complete ACE Guide \u2014 deep dive into the framework</li> <li>API Reference \u2014 full class and method documentation</li> </ul>"},{"location":"#install","title":"Install","text":"<pre><code>pip install ace-framework\n</code></pre>"},{"location":"#paper","title":"Paper","text":"<p>This framework implements the method from:</p> <p>Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models arXiv:2510.04618</p>"},{"location":"API_REFERENCE/","title":"\ud83d\udcda ACE Framework API Reference","text":"<p>Complete API documentation for the ACE Framework.</p>"},{"location":"API_REFERENCE/#core-components","title":"Core Components","text":""},{"location":"API_REFERENCE/#agent","title":"Agent","text":"<p>The Agent produces answers using the current skillbook of strategies.</p> <pre><code>from ace import Agent, LiteLLMClient\n\nclient = LiteLLMClient(model=\"gpt-4\")\nagent = Agent(client)\n\noutput = agent.generate(\n    question=\"What is 2+2?\",\n    context=\"Show your work\",\n    skillbook=skillbook,\n    reflection=None  # Optional reflection from previous attempt\n)\n\n# Output contains:\n# - output.final_answer: The generated answer\n# - output.reasoning: Step-by-step reasoning\n# - output.skill_ids: List of skillbook strategies used\n</code></pre>"},{"location":"API_REFERENCE/#reflector","title":"Reflector","text":"<p>The Reflector analyzes what went right or wrong and tags which strategies helped or hurt.</p> <pre><code>from ace import Reflector\n\nreflector = Reflector(client)\n\nreflection = reflector.reflect(\n    question=\"What is 2+2?\",\n    agent_output=output,\n    skillbook=skillbook,\n    ground_truth=\"4\",\n    feedback=\"Correct!\"\n)\n\n# Reflection contains:\n# - reflection.reasoning: Analysis of the outcome\n# - reflection.error_identification: What went wrong (if anything)\n# - reflection.root_cause_analysis: Why it went wrong\n# - reflection.correct_approach: What should have been done\n# - reflection.key_insight: Main lesson learned\n# - reflection.skill_tags: List of (skill_id, tag) pairs\n</code></pre>"},{"location":"API_REFERENCE/#skillmanager","title":"SkillManager","text":"<p>The SkillManager transforms reflections into skillbook updates.</p> <pre><code>from ace import SkillManager\n\nskill_manager = SkillManager(client)\n\nskill_manager_output = skill_manager.update_skills(\n    reflection=reflection,\n    skillbook=skillbook,\n    question_context=\"Math problems\",\n    progress=\"3/5 correct\"\n)\n\n# Apply the updates\nskillbook.apply_update(skill_manager_output.update)\n</code></pre>"},{"location":"API_REFERENCE/#skillbook-management","title":"Skillbook Management","text":""},{"location":"API_REFERENCE/#creating-a-skillbook","title":"Creating a Skillbook","text":"<pre><code>from ace import Skillbook\n\nskillbook = Skillbook()\n\n# Add a strategy\nskill = skillbook.add_skill(\n    section=\"Math Strategies\",\n    content=\"Break complex problems into smaller steps\",\n    metadata={\"helpful\": 5, \"harmful\": 0, \"neutral\": 1}\n)\n</code></pre>"},{"location":"API_REFERENCE/#saving-and-loading","title":"Saving and Loading","text":"<pre><code># Save to file\nskillbook.save_to_file(\"my_strategies.json\")\n\n# Load from file\nloaded_skillbook = Skillbook.load_from_file(\"my_strategies.json\")\n</code></pre>"},{"location":"API_REFERENCE/#skillbook-statistics","title":"Skillbook Statistics","text":"<pre><code>stats = skillbook.stats()\n# Returns:\n# {\n#   \"sections\": 3,\n#   \"skills\": 15,\n#   \"tags\": {\n#     \"helpful\": 45,\n#     \"harmful\": 5,\n#     \"neutral\": 10\n#   }\n# }\n</code></pre>"},{"location":"API_REFERENCE/#skill-deduplication","title":"Skill Deduplication","text":"<p>Optional feature to detect and consolidate similar skills using embeddings.</p>"},{"location":"API_REFERENCE/#deduplicationconfig","title":"DeduplicationConfig","text":"<pre><code>from ace import DeduplicationConfig\n\nconfig = DeduplicationConfig(\n    enabled=True,                              # Default: True\n    embedding_model=\"text-embedding-3-small\",  # OpenAI embedding\n    similarity_threshold=0.85,                 # Pairs above this are similar\n    within_section_only=True                   # Compare within same section\n)\n\n# Use with any integration\nfrom ace import ACELiteLLM\nagent = ACELiteLLM(model=\"gpt-4o-mini\", dedup_config=config)\n</code></pre>"},{"location":"API_REFERENCE/#adapters","title":"Adapters","text":""},{"location":"API_REFERENCE/#offlineace","title":"OfflineACE","text":"<p>Train on a batch of samples.</p> <pre><code>from ace import OfflineACE\nfrom ace import Sample\n\nadapter = OfflineACE(\n    agent=agent,\n    reflector=reflector,\n    skill_manager=skill_manager\n)\n\nsamples = [\n    Sample(\n        question=\"What is 2+2?\",\n        context=\"Calculate\",\n        ground_truth=\"4\"\n    ),\n    # More samples...\n]\n\nresults = adapter.run(\n    samples=samples,\n    environment=environment,\n    epochs=3,\n    verbose=True\n)\n</code></pre>"},{"location":"API_REFERENCE/#onlineace","title":"OnlineACE","text":"<p>Learn from tasks one at a time.</p> <pre><code>from ace import OnlineACE\n\nadapter = OnlineACE(\n    skillbook=existing_skillbook,\n    agent=agent,\n    reflector=reflector,\n    skill_manager=skill_manager\n)\n\nfor task in tasks:\n    result = adapter.process(task, environment)\n    # Skillbook updates automatically after each task\n</code></pre>"},{"location":"API_REFERENCE/#integrations","title":"Integrations","text":"<p>ACE provides ready-to-use integrations with popular agentic frameworks. These classes wrap external agents with ACE learning capabilities.</p>"},{"location":"API_REFERENCE/#acelitellm","title":"ACELiteLLM","text":"<p>Quick-start integration for simple conversational agents.</p> <pre><code>from ace import ACELiteLLM\n\n# Create an ACE-powered conversational agent\nagent = ACELiteLLM(model=\"gpt-4o-mini\")\n\n# Ask questions - agent learns from each interaction\nanswer1 = agent.ask(\"What is the capital of France?\")\nanswer2 = agent.ask(\"What about Spain?\")\n\n# Save learned strategies\nagent.skillbook.save_to_file(\"learned_strategies.json\")\n\n# Load and continue learning\nagent2 = ACELiteLLM(model=\"gpt-4o-mini\", skillbook_path=\"learned_strategies.json\")\n</code></pre> <p>Parameters: - <code>model</code>: LiteLLM model identifier (e.g., \"gpt-4o-mini\", \"claude-3-5-sonnet\") - <code>skillbook</code>: Optional existing Skillbook to start with - <code>ace_model</code>: Model for Reflector/SkillManager (defaults to same as main model) - <code>**llm_kwargs</code>: Additional arguments passed to LiteLLMClient</p>"},{"location":"API_REFERENCE/#aceagent-browser-use","title":"ACEAgent (browser-use)","text":"<p>Self-improving browser automation agent.</p> <pre><code>from ace import ACEAgent\nfrom browser_use import ChatBrowserUse\n\n# Create browser agent\nllm = ChatBrowserUse(model=\"gpt-4o\")\nagent = ACEAgent(llm=llm)\n\n# Run browser tasks - learns from successes and failures\nawait agent.run(task=\"Find the top post on Hacker News\")\nawait agent.run(task=\"Search for ACE framework on GitHub\")\n\n# Skillbook improves with each task\nprint(f\"Learned {len(agent.skillbook.skills())} strategies\")\n</code></pre> <p>Parameters: - <code>llm</code>: Browser-use ChatBrowserUse instance - <code>skillbook</code>: Optional existing Skillbook - <code>ace_model</code>: Model for learning (defaults to \"gpt-4o-mini\")</p> <p>Requires: <code>pip install browser-use</code> (optional dependency)</p>"},{"location":"API_REFERENCE/#acelangchain","title":"ACELangChain","text":"<p>Wrap LangChain chains and agents with ACE learning.</p> <pre><code>from ace import ACELangChain\nfrom langchain_openai import ChatOpenAI\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\n\n# Create LangChain chain\nllm = ChatOpenAI(temperature=0)\nprompt = PromptTemplate.from_template(\"Answer this question: {question}\")\nchain = LLMChain(llm=llm, prompt=prompt)\n\n# Wrap with ACE\nace_chain = ACELangChain(runnable=chain)\n\n# Use like normal LangChain - but with learning!\nresult1 = ace_chain.invoke({\"question\": \"What is 2+2?\"})\nresult2 = ace_chain.invoke({\"question\": \"What is 10*5?\"})\n\n# Access learned skillbook\nace_chain.save_skillbook(\"langchain_learned.json\")\n</code></pre> <p>Parameters: - <code>runnable</code>: Any LangChain Runnable (chains, agents, etc.) - <code>skillbook</code>: Optional existing Skillbook - <code>ace_model</code>: Model for learning (defaults to \"gpt-4o-mini\") - <code>environment</code>: Custom evaluation environment (optional)</p> <p>Requires: <code>pip install ace-framework[langchain]</code></p> <p>See also: Integration Guide for advanced patterns and custom integrations.</p>"},{"location":"API_REFERENCE/#environments","title":"Environments","text":""},{"location":"API_REFERENCE/#creating-environments","title":"Creating Environments","text":"<p>All environments should extend the <code>TaskEnvironment</code> base class.</p>"},{"location":"API_REFERENCE/#simple-environment-example","title":"Simple Environment Example","text":"<p>Basic environment that compares output to ground truth using substring matching:</p> <pre><code>from ace import TaskEnvironment, EnvironmentResult\n\nclass SimpleEnvironment(TaskEnvironment):\n    \"\"\"Basic environment for testing - checks if ground truth appears in answer.\"\"\"\n\n    def evaluate(self, sample, agent_output):\n        # Simple substring matching (case-insensitive)\n        correct = str(sample.ground_truth).lower() in str(agent_output.final_answer).lower()\n\n        return EnvironmentResult(\n            feedback=\"Correct!\" if correct else \"Incorrect\",\n            ground_truth=sample.ground_truth,\n        )\n\n# Usage\nenv = SimpleEnvironment()\nresult = env.evaluate(sample, agent_output)\n</code></pre>"},{"location":"API_REFERENCE/#custom-environments","title":"Custom Environments","text":"<pre><code>from ace import TaskEnvironment, EnvironmentResult\n\nclass CodeEnvironment(TaskEnvironment):\n    def evaluate(self, sample, output):\n        # Run the code\n        success = execute_code(output.final_answer)\n\n        return EnvironmentResult(\n            feedback=\"Tests passed\" if success else \"Tests failed\",\n            ground_truth=sample.ground_truth,\n            metrics={\"pass_rate\": 1.0 if success else 0.0}\n        )\n</code></pre>"},{"location":"API_REFERENCE/#llm-clients","title":"LLM Clients","text":""},{"location":"API_REFERENCE/#litellmclient","title":"LiteLLMClient","text":"<p>Support for 100+ LLM providers.</p> <pre><code>from ace import LiteLLMClient\n\n# Basic usage\nclient = LiteLLMClient(model=\"gpt-4\")\n\n# With configuration\nclient = LiteLLMClient(\n    model=\"gpt-4\",\n    temperature=0.7,\n    max_tokens=1000,\n    fallbacks=[\"claude-3-haiku\", \"gpt-3.5-turbo\"]\n)\n\n# Generate completion\nresponse = client.complete(\"What is the meaning of life?\")\nprint(response.text)\n</code></pre>"},{"location":"API_REFERENCE/#langchainlitellmclient","title":"LangChainLiteLLMClient","text":"<p>Integration with LangChain.</p> <pre><code>from ace.llm_providers import LangChainLiteLLMClient\n\nclient = LangChainLiteLLMClient(\n    model=\"gpt-4\",\n    tags=[\"production\"],\n    metadata={\"user\": \"alice\"}\n)\n</code></pre>"},{"location":"API_REFERENCE/#types","title":"Types","text":""},{"location":"API_REFERENCE/#sample","title":"Sample","text":"<pre><code>from ace import Sample\n\nsample = Sample(\n    question=\"Your question here\",\n    context=\"Optional context or requirements\",\n    ground_truth=\"Expected answer (optional)\"\n)\n</code></pre>"},{"location":"API_REFERENCE/#agentoutput","title":"AgentOutput","text":"<pre><code>@dataclass\nclass AgentOutput:\n    reasoning: str\n    final_answer: str\n    skill_ids: List[str]\n    raw: Dict[str, Any]\n</code></pre>"},{"location":"API_REFERENCE/#reflectoroutput","title":"ReflectorOutput","text":"<pre><code>@dataclass\nclass ReflectorOutput:\n    reasoning: str\n    error_identification: str\n    root_cause_analysis: str\n    correct_approach: str\n    key_insight: str\n    skill_tags: List[SkillTag]\n    raw: Dict[str, Any]\n</code></pre>"},{"location":"API_REFERENCE/#environmentresult","title":"EnvironmentResult","text":"<pre><code>@dataclass\nclass EnvironmentResult:\n    feedback: str\n    ground_truth: Optional[str] = None\n    metrics: Optional[Dict[str, float]] = None\n</code></pre>"},{"location":"API_REFERENCE/#update-operations","title":"Update Operations","text":""},{"location":"API_REFERENCE/#updateoperation-types","title":"UpdateOperation Types","text":"<ul> <li><code>ADD</code>: Add new skill to skillbook</li> <li><code>UPDATE</code>: Update existing skill content</li> <li><code>TAG</code>: Update helpful/harmful/neutral counts</li> <li><code>REMOVE</code>: Remove skill from skillbook</li> </ul> <pre><code>from ace.updates import UpdateOperation\n\nop = UpdateOperation(\n    type=\"ADD\",\n    section=\"Math Strategies\",\n    content=\"Always check your work\",\n    skill_id=\"math-00001\"\n)\n</code></pre>"},{"location":"API_REFERENCE/#prompts","title":"Prompts","text":""},{"location":"API_REFERENCE/#using-default-prompts","title":"Using Default Prompts","text":"<pre><code>from ace.prompts import AGENT_PROMPT, REFLECTOR_PROMPT, SKILL_MANAGER_PROMPT\n\nagent = Agent(client, prompt_template=AGENT_PROMPT)\n</code></pre>"},{"location":"API_REFERENCE/#using-v21-prompts-recommended","title":"Using v2.1 Prompts (Recommended)","text":"<p>ACE v2.1 prompts show +17% success rate improvement vs v1.0.</p> <pre><code>from ace.prompts_v2_1 import PromptManager\n\nmanager = PromptManager(default_version=\"2.1\")\n\nagent = Agent(\n    client,\n    prompt_template=manager.get_agent_prompt(domain=\"math\")\n)\n</code></pre> <p>Note: v2.0 prompts (<code>ace.prompts_v2</code>) are deprecated. Use v2.1 for best performance.</p>"},{"location":"API_REFERENCE/#custom-prompts","title":"Custom Prompts","text":"<pre><code>custom_prompt = '''\nSkillbook: {skillbook}\nQuestion: {question}\nContext: {context}\n\nGenerate a JSON response with:\n- reasoning: Your step-by-step thought process\n- skill_ids: List of skillbook IDs you used\n- final_answer: Your answer\n'''\n\nagent = Agent(client, prompt_template=custom_prompt)\n</code></pre>"},{"location":"API_REFERENCE/#async-operations","title":"Async Operations","text":"<pre><code>import asyncio\n\nasync def main():\n    # Async completion\n    response = await client.acomplete(\"What is 2+2?\")\n\n    # Async adapter operations also supported\n    # (Implementation depends on adapter async support)\n\nasyncio.run(main())\n</code></pre>"},{"location":"API_REFERENCE/#streaming","title":"Streaming","text":"<pre><code># Stream responses token by token\nfor chunk in client.complete_with_stream(\"Write a story\"):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"API_REFERENCE/#error-handling","title":"Error Handling","text":"<pre><code>from ace.exceptions import ACEException\n\ntry:\n    output = agent.generate(...)\nexcept ACEException as e:\n    print(f\"ACE error: {e}\")\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n</code></pre>"},{"location":"API_REFERENCE/#configuration","title":"Configuration","text":""},{"location":"API_REFERENCE/#environment-variables","title":"Environment Variables","text":"<pre><code># OpenAI\nexport OPENAI_API_KEY=\"your-key\"\n\n# Anthropic\nexport ANTHROPIC_API_KEY=\"your-key\"\n\n# Google\nexport GOOGLE_API_KEY=\"your-key\"\n\n# Custom endpoint\nexport LITELLM_API_BASE=\"https://your-endpoint.com\"\n</code></pre>"},{"location":"API_REFERENCE/#logging","title":"Logging","text":"<pre><code>import logging\n\n# Enable debug logging\nlogging.basicConfig(level=logging.DEBUG)\n\n# Or just for ACE\nlogging.getLogger(\"ace\").setLevel(logging.DEBUG)\n</code></pre>"},{"location":"API_REFERENCE/#best-practices","title":"Best Practices","text":"<ol> <li>Start with SimpleEnvironment: Get basic training working first</li> <li>Use fallback models: Ensure reliability in production</li> <li>Save skillbooks regularly: Preserve learned strategies</li> <li>Monitor costs: Track token usage with metrics</li> <li>Test with dummy mode: Validate logic without API calls</li> <li>Use appropriate epochs: 2-3 epochs usually sufficient</li> <li>Implement custom environments: Tailor evaluation to your task</li> </ol>"},{"location":"API_REFERENCE/#examples","title":"Examples","text":"<p>See the examples directory for complete working examples:</p> <p>Core Examples: - <code>simple_ace_example.py</code> - Basic usage - <code>skillbook_persistence.py</code> - Save/load strategies</p> <p>By Category: - langchain/ - LangChain integration examples - prompts/ - Prompt engineering examples - browser-use/ - Browser automation</p>"},{"location":"COMPLETE_GUIDE_TO_ACE/","title":"Agentic Context Engineering: Complete Guide","text":"<p>How ACE enables AI agents to improve through in-context learning instead of fine-tuning.</p>"},{"location":"COMPLETE_GUIDE_TO_ACE/#what-is-agentic-context-engineering","title":"What is Agentic Context Engineering?","text":"<p>Agentic Context Engineering (ACE) is a framework introduced by researchers at Stanford University and SambaNova Systems that enables AI agents to improve performance by dynamically curating their own context through execution feedback.</p> <p>Key Innovation: Instead of updating model weights through expensive fine-tuning cycles, ACE treats context as a living \"skillbook\" that evolves based on what strategies actually work in practice.</p> <p>Research Paper: Agentic Context Engineering (arXiv:2510.04618)</p>"},{"location":"COMPLETE_GUIDE_TO_ACE/#the-core-problem","title":"The Core Problem","text":"<p>Modern AI agents face a fundamental limitation: they don't learn from execution history. When an agent makes a mistake, developers must manually intervene\u2014editing prompts, adjusting parameters, or fine-tuning the model.</p> <p>Traditional approaches have major drawbacks: - Repetitive failures: Agents lack institutional memory - Manual intervention: Doesn't scale as complexity increases - Expensive adaptation: Fine-tuning costs $10,000+ per cycle and takes weeks - Black box improvement: Unclear what changed or why</p>"},{"location":"COMPLETE_GUIDE_TO_ACE/#how-ace-works","title":"How ACE Works","text":"<p>ACE introduces a three-agent architecture where specialized roles collaborate to build and maintain a dynamic knowledge base called the \"skillbook.\"</p>"},{"location":"COMPLETE_GUIDE_TO_ACE/#the-three-agents","title":"The Three Agents","text":"<p>1. Agent - Task Execution - Performs the actual work using strategies from the skillbook - Operates like a traditional agent but with access to learned knowledge</p> <p>2. Reflector - Performance Analysis - Analyzes execution outcomes without human supervision - Identifies which strategies worked, which failed, and why - Generates insights that inform skillbook updates</p> <p>3. SkillManager - Knowledge Management - Adds new strategies based on successful executions - Removes or marks strategies that consistently fail - Merges semantically similar strategies to prevent redundancy</p>"},{"location":"COMPLETE_GUIDE_TO_ACE/#the-skillbook","title":"The Skillbook","text":"<p>The skillbook stores learned strategies as structured \"skills\"\u2014discrete pieces of knowledge with metadata:</p> <pre><code>{\n  \"content\": \"When querying financial data, filter by date range first to reduce result set size\",\n  \"helpful_count\": 12,\n  \"harmful_count\": 1,\n  \"section\": \"task_guidance\"\n}\n</code></pre>"},{"location":"COMPLETE_GUIDE_TO_ACE/#the-learning-cycle","title":"The Learning Cycle","text":"<ol> <li>Execution: Agent receives a task and retrieves relevant skillbook skills</li> <li>Action: Agent executes using retrieved strategies</li> <li>Reflection: Reflector analyzes the execution outcome</li> <li>Curation: SkillManager updates the skillbook with update operations</li> <li>Iteration: Process repeats, skillbook grows more refined over time</li> </ol>"},{"location":"COMPLETE_GUIDE_TO_ACE/#insight-levels","title":"Insight Levels","text":"<p>The Reflector can analyze execution at three different levels of scope, producing insights of varying depth:</p> Level Scope What's Analyzed Learning Quality Micro Single interaction + environment Request \u2192 response \u2192 ground truth/feedback Learns from correctness Meso Full agent run Reasoning traces (thoughts, tool calls, observations) Learns from execution patterns Macro Cross-run analysis Patterns across multiple executions Comprehensive (future) <p>Micro-level insights come from the full ACE adaptation loop with environment feedback and ground truth. The Reflector knows whether the answer was correct and learns from that evaluation. Used by OfflineACE and OnlineACE.</p> <p>Meso-level insights come from full agent runs with intermediate steps\u2014the agent's thoughts, tool calls, and observations\u2014but without external ground truth. The Reflector learns from the execution patterns themselves. Used by integration wrappers like ACELangChain with AgentExecutor.</p> <p>Macro-level insights (future) will compare patterns across multiple runs to identify systemic improvements.</p>"},{"location":"COMPLETE_GUIDE_TO_ACE/#key-technical-innovations","title":"Key Technical Innovations","text":""},{"location":"COMPLETE_GUIDE_TO_ACE/#update-operations-preventing-context-collapse","title":"Update Operations (Preventing Context Collapse)","text":"<p>A critical insight from the ACE paper: LLMs exhibit brevity bias when asked to rewrite context. They compress information, losing crucial details.</p> <p>ACE solves this through update operations\u2014incremental modifications that never ask the LLM to regenerate entire contexts:</p> <ul> <li>Add: Insert new skill to skillbook</li> <li>Remove: Delete specific skill by ID</li> <li>Modify: Update specific fields (helpful_count, content refinement)</li> </ul> <p>This preserves the exact wording and structure of learned knowledge.</p>"},{"location":"COMPLETE_GUIDE_TO_ACE/#semantic-deduplication","title":"Semantic Deduplication","text":"<p>As agents learn, they may generate similar but differently-worded strategies. ACE prevents skillbook bloat through embedding-based deduplication, keeping the skillbook concise while capturing diverse knowledge.</p>"},{"location":"COMPLETE_GUIDE_TO_ACE/#hybrid-retrieval","title":"Hybrid Retrieval","text":"<p>Instead of dumping the entire skillbook into context, ACE uses hybrid retrieval to select only the most relevant skills. This:</p> <ul> <li>Keeps context windows manageable</li> <li>Prioritizes proven strategies</li> <li>Reduces token costs</li> </ul>"},{"location":"COMPLETE_GUIDE_TO_ACE/#async-learning-mode","title":"Async Learning Mode","text":"<p>For latency-sensitive applications, ACE supports async learning where the Agent returns immediately while Reflector and SkillManager process in the background:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                       ASYNC LEARNING PIPELINE                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                       \u2502\n\u2502  Sample 1 \u2500\u2500\u25ba Agent \u2500\u2500\u25ba Env \u2500\u2500\u25ba Reflector \u2500\u2510                         \u2502\n\u2502  Sample 2 \u2500\u2500\u25ba Agent \u2500\u2500\u25ba Env \u2500\u2500\u25ba Reflector \u2500\u253c\u2500\u2500\u25ba Queue \u2500\u2500\u25ba SkillManager\u2502\n\u2502  Sample 3 \u2500\u2500\u25ba Agent \u2500\u2500\u25ba Env \u2500\u2500\u25ba Reflector \u2500\u2518           (serialized)   \u2502\n\u2502             (parallel)        (parallel)                              \u2502\n\u2502                                                                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Why this architecture: - Parallel Reflectors: Safe to parallelize (read-only analysis, no skillbook writes) - Serialized SkillManager: Must be sequential (writes to skillbook, handles deduplication) - 3x faster learning: Reflector LLM calls run concurrently</p> <p>Usage: <pre><code>adapter = OfflineACE(\n    skillbook=skillbook,\n    agent=agent,\n    reflector=reflector,\n    skill_manager=skill_manager,\n    async_learning=True,        # Enable async mode\n    max_reflector_workers=3,    # Parallel Reflector threads\n)\n\nresults = adapter.run(samples, environment)  # Fast - learning in background\n\n# Control methods\nadapter.learning_stats       # Check progress\nadapter.wait_for_learning()  # Block until complete\nadapter.stop_async_learning() # Shutdown pipeline\n</code></pre></p>"},{"location":"COMPLETE_GUIDE_TO_ACE/#performance-results","title":"Performance Results","text":"<p>The Stanford team evaluated ACE across multiple benchmarks:</p> <p>AppWorld Agent Benchmark: - +17.1 percentage points improvement vs. base LLM (\u224840% relative improvement) - Tested on complex multi-step tasks requiring tool use and reasoning</p> <p>Finance Domain (FiNER): - +8.6 percentage points improvement on financial reasoning tasks</p> <p>Adaptation Efficiency: - 86.9% lower adaptation latency compared to existing context-adaptation methods</p> <p>Key Insight: Performance improvements compound over time. As the skillbook grows, agents make fewer mistakes on similar tasks, creating a positive feedback loop.</p>"},{"location":"COMPLETE_GUIDE_TO_ACE/#when-to-use-ace","title":"When to Use ACE","text":""},{"location":"COMPLETE_GUIDE_TO_ACE/#best-fit-use-cases","title":"Best Fit Use Cases","text":"<p>Software Development Agents - Learn project-specific patterns (naming conventions, error handling) - Build knowledge of common bugs and solutions - Accumulate code review guidelines</p> <p>Customer Support Automation - Learn which issues need human escalation - Discover effective communication patterns - Build institutional knowledge of edge cases</p> <p>Data Analysis Agents - Learn efficient query patterns - Discover which visualizations work for which data types - Build baseline expectations from execution history</p> <p>Research Assistants - Learn effective search strategies per domain - Discover citation patterns and summarization techniques - Build knowledge of reliable sources</p>"},{"location":"COMPLETE_GUIDE_TO_ACE/#when-not-to-use-ace","title":"When NOT to Use ACE","text":"<p>ACE may not be the right fit when: - Single-use tasks: No benefit from learning if task never repeats - Perfect first-time execution required: ACE learns through iteration - Purely factual retrieval: Traditional RAG may be more appropriate</p>"},{"location":"COMPLETE_GUIDE_TO_ACE/#ace-vs-other-approaches","title":"ACE vs. Other Approaches","text":""},{"location":"COMPLETE_GUIDE_TO_ACE/#vs-fine-tuning","title":"vs. Fine-Tuning","text":"Aspect ACE Fine-Tuning Speed Immediate (after single execution) Days to weeks Cost Inference only $10K+ per iteration Interpretability Readable skillbook Black box weights Reversibility Edit/remove strategies easily Requires retraining"},{"location":"COMPLETE_GUIDE_TO_ACE/#vs-rag","title":"vs. RAG","text":"Aspect ACE RAG Knowledge Source Learned from execution Static documents Update Mechanism Autonomous skill updates Manual updates Content Type Strategies, patterns Facts, references Optimization Self-improving Requires query tuning"},{"location":"COMPLETE_GUIDE_TO_ACE/#getting-started","title":"Getting Started","text":"<p>Ready to build self-learning agents? Check out these resources:</p> <ul> <li>Quick Start Guide - Get running in 5 minutes</li> <li>Integration Guide - Add ACE to existing agents</li> <li>API Reference - Complete API documentation</li> <li>Examples - Ready-to-run code examples</li> </ul>"},{"location":"COMPLETE_GUIDE_TO_ACE/#additional-resources","title":"Additional Resources","text":""},{"location":"COMPLETE_GUIDE_TO_ACE/#research","title":"Research","text":"<ul> <li>Original ACE Paper (arXiv)</li> </ul>"},{"location":"COMPLETE_GUIDE_TO_ACE/#community","title":"Community","text":"<ul> <li>Discord Server</li> <li>GitHub</li> <li>Kayba Website</li> </ul> <p>Last Updated: November 2025</p>"},{"location":"INTEGRATION_GUIDE/","title":"ACE Integration Guide","text":"<p>Comprehensive guide for integrating ACE learning with your agentic system.</p>"},{"location":"INTEGRATION_GUIDE/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Integration vs Full Pipeline</li> <li>The Base Integration Pattern</li> <li>Building a Custom Integration</li> <li>Reference Implementations</li> <li>Integration Patterns</li> <li>Advanced Topics</li> <li>Troubleshooting</li> </ol>"},{"location":"INTEGRATION_GUIDE/#integration-vs-full-pipeline","title":"Integration vs Full Pipeline","text":""},{"location":"INTEGRATION_GUIDE/#decision-tree-which-approach-should-you-use","title":"Decision Tree: Which Approach Should You Use?","text":"<pre><code>Do you have an existing agentic system?\n\u2502\n\u251c\u2500 YES \u2192 Use INTEGRATION PATTERN\n\u2502   \u2502\n\u2502   \u251c\u2500 Browser automation? \u2192 Use ACEAgent (browser-use)\n\u2502   \u251c\u2500 LangChain chains/agents? \u2192 Use ACELangChain\n\u2502   \u2514\u2500 Custom agent? \u2192 Follow this guide\n\u2502\n\u2514\u2500 NO \u2192 Use FULL ACE PIPELINE\n    \u2502\n    \u251c\u2500 Simple tasks (Q&amp;A, classification)? \u2192 Use ACELiteLLM\n    \u2514\u2500 Complex tasks (tools, workflows)? \u2192 Consider LangChain + ACELangChain\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#whats-the-difference","title":"What's the Difference?","text":"<p>INTEGRATION PATTERN (this guide): - Your agent executes tasks (browser-use, LangChain, custom API) - ACE learns from results (doesn't execute) - Components: Skillbook + Reflector + SkillManager (NO Agent) - Use case: Wrapping existing agents with learning</p> <p>FULL ACE PIPELINE (not this guide): - ACE Agent executes tasks - Full ACE components: Skillbook + Agent + Reflector + SkillManager - Use case: Building new agents from scratch - See: <code>ace.integrations.ACELiteLLM</code> class</p>"},{"location":"INTEGRATION_GUIDE/#the-base-integration-pattern","title":"The Base Integration Pattern","text":"<p>All ACE integrations follow a three-step pattern:</p>"},{"location":"INTEGRATION_GUIDE/#step-1-inject-optional-but-recommended","title":"Step 1: INJECT (Optional but Recommended)","text":"<p>Add learned strategies from the skillbook to your agent's input.</p> <pre><code>from ace.integrations.base import wrap_skillbook_context\nfrom ace import Skillbook\n\nskillbook = Skillbook()  # or load existing: Skillbook.load_from_file(\"expert.json\")\ntask = \"Process user request\"\n\n# Inject skillbook context\nif skillbook.skills():\n    enhanced_task = f\"{task}\\n\\n{wrap_skillbook_context(skillbook)}\"\nelse:\n    enhanced_task = task  # No learned strategies yet\n</code></pre> <p>What does <code>wrap_skillbook_context()</code> do? - Formats learned strategies with success rates - Adds usage instructions for the agent - Returns empty string if no skills (safe to call always)</p>"},{"location":"INTEGRATION_GUIDE/#step-2-execute","title":"Step 2: EXECUTE","text":"<p>Your agent runs normally - ACE doesn't interfere.</p> <pre><code># Your agent (any framework/API)\nresult = your_agent.execute(enhanced_task)\n\n# Examples:\n# - Browser-use: await agent.run(task=enhanced_task)\n# - LangChain: chain.invoke({\"input\": enhanced_task})\n# - API: requests.post(\"/execute\", json={\"task\": enhanced_task})\n# - Custom: my_agent.run(enhanced_task)\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#step-3-learn","title":"Step 3: LEARN","text":"<p>ACE analyzes the result and updates the skillbook.</p> <pre><code>from ace import LiteLLMClient, Reflector, SkillManager\nfrom ace.roles import AgentOutput\n\n# Setup ACE learning components (do this once)\nllm = LiteLLMClient(model=\"gpt-4o-mini\", max_tokens=2048)\nreflector = Reflector(llm)\nskill_manager = SkillManager(llm)\n\n# Create adapter for Reflector interface\nagent_output = AgentOutput(\n    reasoning=f\"Task: {task}\",  # What happened\n    final_answer=result.output,  # Agent's output\n    skill_ids=[],  # External agents don't cite skills\n    raw={\"success\": result.success, \"steps\": result.steps}  # Metadata\n)\n\n# Build feedback string\nfeedback = f\"Task {'succeeded' if result.success else 'failed'}. Output: {result.output}\"\n\n# Reflect: Analyze what worked/failed\nreflection = reflector.reflect(\n    question=task,\n    agent_output=agent_output,\n    skillbook=skillbook,\n    ground_truth=None,  # Optional: expected output\n    feedback=feedback\n)\n\n# Update skills: Generate skillbook updates\nskill_manager_output = skill_manager.update_skills(\n    reflection=reflection,\n    skillbook=skillbook,\n    question_context=f\"task: {task}\",\n    progress=f\"Executing: {task}\"\n)\n\n# Apply updates\nskillbook.apply_update(skill_manager_output.update)\n\n# Save for next time\nskillbook.save_to_file(\"learned_strategies.json\")\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#building-a-custom-integration","title":"Building a Custom Integration","text":""},{"location":"INTEGRATION_GUIDE/#wrapper-class-pattern-recommended","title":"Wrapper Class Pattern (Recommended)","text":"<p>Create a wrapper class that bundles your agent with ACE learning:</p> <pre><code>from ace import Skillbook, LiteLLMClient, Reflector, SkillManager\nfrom ace.integrations.base import wrap_skillbook_context\nfrom ace.roles import AgentOutput\n\nclass ACEWrapper:\n    \"\"\"Wraps your custom agent with ACE learning.\"\"\"\n\n    def __init__(\n        self,\n        agent,\n        ace_model: str = \"gpt-4o-mini\",\n        skillbook_path: str = None,\n        is_learning: bool = True\n    ):\n        \"\"\"\n        Args:\n            agent: Your agent instance\n            ace_model: Model for ACE learning (Reflector/SkillManager)\n            skillbook_path: Path to existing skillbook (optional)\n            is_learning: Enable/disable learning\n        \"\"\"\n        self.agent = agent\n        self.is_learning = is_learning\n\n        # Load or create skillbook\n        if skillbook_path:\n            self.skillbook = Skillbook.load_from_file(skillbook_path)\n        else:\n            self.skillbook = Skillbook()\n\n        # Setup ACE learning components\n        self.llm = LiteLLMClient(model=ace_model, max_tokens=2048)\n        self.reflector = Reflector(self.llm)\n        self.skill_manager = SkillManager(self.llm)\n\n    def run(self, task: str):\n        \"\"\"Execute task with ACE learning.\"\"\"\n        # STEP 1: Inject skillbook context\n        enhanced_task = self._inject_context(task)\n\n        # STEP 2: Execute\n        result = self.agent.execute(enhanced_task)\n\n        # STEP 3: Learn (if enabled)\n        if self.is_learning:\n            self._learn(task, result)\n\n        return result\n\n    def _inject_context(self, task: str) -&gt; str:\n        \"\"\"Add skillbook strategies to task.\"\"\"\n        if self.skillbook.skills():\n            return f\"{task}\\n\\n{wrap_skillbook_context(self.skillbook)}\"\n        return task\n\n    def _learn(self, task: str, result):\n        \"\"\"Run ACE learning pipeline.\"\"\"\n        # Adapt result to ACE interface\n        agent_output = AgentOutput(\n            reasoning=f\"Task: {task}\",\n            final_answer=result.output,\n            skill_ids=[],\n            raw={\"success\": result.success}\n        )\n\n        # Build feedback\n        feedback = f\"Task {'succeeded' if result.success else 'failed'}\"\n\n        # Reflect\n        reflection = self.reflector.reflect(\n            question=task,\n            agent_output=agent_output,\n            skillbook=self.skillbook,\n            feedback=feedback\n        )\n\n        # Update skills\n        skill_manager_output = self.skill_manager.update_skills(\n            reflection=reflection,\n            skillbook=self.skillbook,\n            question_context=f\"task: {task}\",\n            progress=task\n        )\n\n        # Update skillbook\n        self.skillbook.apply_update(skill_manager_output.update)\n\n    def save_skillbook(self, path: str):\n        \"\"\"Save learned strategies.\"\"\"\n        self.skillbook.save_to_file(path)\n\n    def load_skillbook(self, path: str):\n        \"\"\"Load existing strategies.\"\"\"\n        self.skillbook = Skillbook.load_from_file(path)\n\n    def enable_learning(self):\n        \"\"\"Enable learning.\"\"\"\n        self.is_learning = True\n\n    def disable_learning(self):\n        \"\"\"Disable learning (execution only).\"\"\"\n        self.is_learning = False\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#usage-example","title":"Usage Example","text":"<pre><code># Your custom agent\nclass MyAgent:\n    def execute(self, task: str):\n        # Your agent logic\n        return {\"output\": \"result\", \"success\": True}\n\n# Wrap with ACE\nmy_agent = MyAgent()\nace_agent = ACEWrapper(my_agent, is_learning=True)\n\n# Use it\nresult = ace_agent.run(\"Process data\")\nprint(f\"Result: {result.output}\")\nprint(f\"Learned {len(ace_agent.skillbook.skills())} strategies\")\n\n# Save learned knowledge\nace_agent.save_skillbook(\"my_agent_learned.json\")\n\n# Next session: Load previous knowledge\nace_agent = ACEWrapper(MyAgent(), skillbook_path=\"my_agent_learned.json\")\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#reference-implementations","title":"Reference Implementations","text":""},{"location":"INTEGRATION_GUIDE/#browser-use-integration","title":"Browser-Use Integration","text":"<p>See <code>ace/integrations/browser_use.py</code> for a complete reference implementation.</p> <p>Key Design Decisions:</p> <ol> <li> <p>Context Injection (line 182-189): <pre><code>if self.is_learning and self.skillbook.skills():\n    skillbook_context = wrap_skillbook_context(self.skillbook)\n    enhanced_task = f\"{current_task}\\n\\n{skillbook_context}\"\n</code></pre></p> </li> <li> <p>Rich Feedback Extraction (line 234-403):</p> </li> <li>Extracts chronological execution trace</li> <li>Includes agent thoughts, actions, results</li> <li> <p>Provides detailed context for Reflector</p> </li> <li> <p>Citation Extraction (line 405-434):</p> </li> <li>Parses agent's reasoning for skill citations</li> <li> <p>Filters invalid IDs (graceful degradation)</p> </li> <li> <p>Learning Pipeline (line 436-510):</p> </li> <li>Creates AgentOutput adapter</li> <li>Passes full trace to Reflector in <code>reasoning</code> field</li> <li>Updates skillbook via SkillManager</li> </ol> <p>Why Browser-Use is a Good Reference: - Shows rich feedback extraction - Handles async execution - Robust error handling - Learning toggle - Skillbook persistence</p>"},{"location":"INTEGRATION_GUIDE/#runnable-examples","title":"Runnable Examples","text":"<p>See these working examples in the repository:</p> <ul> <li>Browser automation: <code>examples/browser-use/simple_ace_agent.py</code></li> <li>Custom integration: <code>examples/custom_integration_example.py</code></li> <li>LangChain chains/agents: <code>examples/langchain/simple_chain_example.py</code></li> </ul> <p>Full list: <code>examples/README.md</code></p>"},{"location":"INTEGRATION_GUIDE/#integration-patterns","title":"Integration Patterns","text":"<p>Common patterns for integrating ACE with different types of agents. Each pattern includes complete code examples, when to use it, and key considerations.</p>"},{"location":"INTEGRATION_GUIDE/#rest-api-based-agents","title":"REST API-Based Agents","text":""},{"location":"INTEGRATION_GUIDE/#when-to-use","title":"When to Use","text":"<ul> <li>Your agent is a REST API service</li> <li>Remote execution (cloud-based agents)</li> <li>Stateless request/response pattern</li> </ul>"},{"location":"INTEGRATION_GUIDE/#pattern","title":"Pattern","text":"<pre><code>from ace import Skillbook, LiteLLMClient, Reflector, SkillManager\nfrom ace.integrations.base import wrap_skillbook_context\nfrom ace.roles import AgentOutput\nimport requests\n\nclass ACEAPIAgent:\n    \"\"\"Wraps REST API agent with ACE learning.\"\"\"\n\n    def __init__(self, api_url: str, api_key: str = None, ace_model: str = \"gpt-4o-mini\"):\n        self.api_url = api_url\n        self.api_key = api_key\n        self.skillbook = Skillbook()\n\n        # ACE components\n        llm = LiteLLMClient(model=ace_model, max_tokens=2048)\n        self.reflector = Reflector(llm)\n        self.skill_manager = SkillManager(llm)\n\n    def execute(self, task: str):\n        \"\"\"Execute task via API with ACE learning.\"\"\"\n        # Inject context\n        if self.skillbook.skills():\n            task = f\"{task}\\n\\n{wrap_skillbook_context(self.skillbook)}\"\n\n        # API call\n        headers = {\"Authorization\": f\"Bearer {self.api_key}\"} if self.api_key else {}\n        response = requests.post(\n            f\"{self.api_url}/execute\",\n            json={\"task\": task},\n            headers=headers,\n            timeout=60\n        )\n\n        # Extract result\n        success = response.status_code == 200\n        output = response.json().get(\"result\", \"\") if success else response.text\n\n        # Learn\n        self._learn(task, output, success)\n\n        return {\"output\": output, \"success\": success}\n\n    def _learn(self, task: str, output: str, success: bool):\n        # Create adapter\n        agent_output = AgentOutput(\n            reasoning=f\"API call for task: {task}\",\n            final_answer=output,\n            skill_ids=[],\n            raw={\"success\": success}\n        )\n\n        # Feedback\n        feedback = f\"API call {'succeeded' if success else 'failed'}. Output: {output[:200]}\"\n\n        # Reflect + Update skills\n        reflection = self.reflector.reflect(\n            question=task,\n            agent_output=agent_output,\n            skillbook=self.skillbook,\n            feedback=feedback\n        )\n\n        skill_manager_output = self.skill_manager.update_skills(\n            reflection=reflection,\n            skillbook=self.skillbook,\n            question_context=f\"API task: {task}\",\n            progress=task\n        )\n\n        self.skillbook.apply_update(skill_manager_output.update)\n\n# Usage\nagent = ACEAPIAgent(api_url=\"https://api.example.com\", api_key=\"...\")\nresult = agent.execute(\"Process user data\")\nagent.skillbook.save_to_file(\"api_agent_learned.json\")\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#key-considerations","title":"Key Considerations","text":"<ul> <li>Handle timeouts and retries</li> <li>Parse API error messages for better feedback</li> <li>Consider rate limiting (don't learn on every call if high volume)</li> </ul>"},{"location":"INTEGRATION_GUIDE/#multi-step-workflow-agents","title":"Multi-Step Workflow Agents","text":""},{"location":"INTEGRATION_GUIDE/#when-to-use_1","title":"When to Use","text":"<ul> <li>Agent executes multiple sequential steps</li> <li>Each step has its own outcome</li> <li>Want to learn from entire workflow</li> </ul>"},{"location":"INTEGRATION_GUIDE/#pattern_1","title":"Pattern","text":"<pre><code>from dataclasses import dataclass\nfrom typing import List\n\n@dataclass\nclass WorkflowStep:\n    action: str\n    outcome: str\n    success: bool\n    duration: float\n\n@dataclass\nclass WorkflowResult:\n    steps: List[WorkflowStep]\n    final_output: str\n    overall_success: bool\n\nclass ACEWorkflowAgent:\n    \"\"\"Wraps multi-step workflow agent with rich trace learning.\"\"\"\n\n    def __init__(self, workflow_agent, ace_model: str = \"gpt-4o-mini\"):\n        self.agent = workflow_agent\n        self.skillbook = Skillbook()\n\n        llm = LiteLLMClient(model=ace_model, max_tokens=2048)\n        self.reflector = Reflector(llm)\n        self.skill_manager = SkillManager(llm)\n\n    def run(self, task: str) -&gt; WorkflowResult:\n        \"\"\"Execute workflow with ACE learning.\"\"\"\n        # Inject context\n        if self.skillbook.skills():\n            task = f\"{task}\\n\\n{wrap_skillbook_context(self.skillbook)}\"\n\n        # Execute workflow (returns WorkflowResult)\n        result = self.agent.execute_workflow(task)\n\n        # Learn from entire workflow\n        self._learn(task, result)\n\n        return result\n\n    def _learn(self, task: str, result: WorkflowResult):\n        \"\"\"Learn from complete workflow trace.\"\"\"\n        # Build rich feedback with all steps\n        feedback_parts = [\n            f\"Workflow {'succeeded' if result.overall_success else 'failed'} \"\n            f\"in {len(result.steps)} steps\\n\"\n        ]\n\n        for i, step in enumerate(result.steps, 1):\n            status = \"\u2713\" if step.success else \"\u2717\"\n            feedback_parts.append(\n                f\"Step {i} [{status}]: {step.action}\\n\"\n                f\"  \u2192 Outcome: {step.outcome}\\n\"\n                f\"  \u2192 Duration: {step.duration:.2f}s\"\n            )\n\n        feedback = \"\\n\".join(feedback_parts)\n\n        # Create adapter with full trace\n        agent_output = AgentOutput(\n            reasoning=feedback,  # Full workflow trace\n            final_answer=result.final_output,\n            skill_ids=[],\n            raw={\n                \"total_steps\": len(result.steps),\n                \"successful_steps\": sum(1 for s in result.steps if s.success),\n                \"total_duration\": sum(s.duration for s in result.steps)\n            }\n        )\n\n        # Reflect + Update skills\n        reflection = self.reflector.reflect(\n            question=task,\n            agent_output=agent_output,\n            skillbook=self.skillbook,\n            feedback=feedback\n        )\n\n        skill_manager_output = self.skill_manager.update_skills(\n            reflection=reflection,\n            skillbook=self.skillbook,\n            question_context=f\"Multi-step workflow: {task}\",\n            progress=f\"Completed {len(result.steps)} steps\"\n        )\n\n        self.skillbook.apply_update(skill_manager_output.update)\n\n# Usage\nworkflow_agent = MyWorkflowAgent()\nace_agent = ACEWorkflowAgent(workflow_agent)\nresult = ace_agent.run(\"Complete data pipeline\")\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#key-considerations_1","title":"Key Considerations","text":"<ul> <li>Include step-by-step trace in feedback for better learning</li> <li>Track timing information to learn performance patterns</li> <li>Distinguish partial failures (some steps succeed) from total failures</li> </ul>"},{"location":"INTEGRATION_GUIDE/#tool-using-agents","title":"Tool-Using Agents","text":""},{"location":"INTEGRATION_GUIDE/#when-to-use_2","title":"When to Use","text":"<ul> <li>Agent has access to external tools/functions</li> <li>Tool selection and usage is part of learning</li> <li>Want to inject context into system message or tool descriptions</li> </ul>"},{"location":"INTEGRATION_GUIDE/#pattern_2","title":"Pattern","text":"<pre><code>class ACEToolAgent:\n    \"\"\"Wraps tool-using agent with ACE learning.\"\"\"\n\n    def __init__(self, agent, ace_model: str = \"gpt-4o-mini\"):\n        self.agent = agent\n        self.skillbook = Skillbook()\n        self.original_system_message = agent.system_message\n\n        llm = LiteLLMClient(model=ace_model, max_tokens=2048)\n        self.reflector = Reflector(llm)\n        self.skill_manager = SkillManager(llm)\n\n    def run(self, task: str):\n        \"\"\"Execute with tool access and ACE learning.\"\"\"\n        # Inject skillbook into system message (not task)\n        if self.skillbook.skills():\n            context = wrap_skillbook_context(self.skillbook)\n            self.agent.system_message = f\"{self.original_system_message}\\n\\n{context}\"\n\n        # Execute (agent selects and uses tools)\n        result = self.agent.execute(task)\n\n        # Restore original system message\n        self.agent.system_message = self.original_system_message\n\n        # Learn\n        self._learn(task, result)\n\n        return result\n\n    def _learn(self, task: str, result):\n        \"\"\"Learn from tool usage patterns.\"\"\"\n        # Extract tool usage information\n        tools_used = result.get(\"tools_used\", [])\n        tool_results = result.get(\"tool_results\", [])\n\n        # Build rich feedback\n        feedback_parts = [\n            f\"Task {'succeeded' if result['success'] else 'failed'}\",\n            f\"Tools used: {', '.join(t['name'] for t in tools_used)}\"\n        ]\n\n        for tool, tool_result in zip(tools_used, tool_results):\n            feedback_parts.append(\n                f\"  {tool['name']}({tool['args']}) \u2192 {tool_result['outcome']}\"\n            )\n\n        feedback = \"\\n\".join(feedback_parts)\n\n        # Adapter\n        agent_output = AgentOutput(\n            reasoning=feedback,\n            final_answer=result[\"output\"],\n            skill_ids=[],\n            raw={\"tools_used\": [t[\"name\"] for t in tools_used]}\n        )\n\n        # Reflect + Update skills\n        reflection = self.reflector.reflect(\n            question=task,\n            agent_output=agent_output,\n            skillbook=self.skillbook,\n            feedback=feedback\n        )\n\n        skill_manager_output = self.skill_manager.update_skills(\n            reflection=reflection,\n            skillbook=self.skillbook,\n            question_context=f\"Tool-using task: {task}\",\n            progress=f\"Used {len(tools_used)} tools\"\n        )\n\n        self.skillbook.apply_update(skill_manager_output.update)\n\n# Usage\ntool_agent = MyToolUsingAgent(tools=[...])\nace_agent = ACEToolAgent(tool_agent)\nresult = ace_agent.run(\"Analyze data and send report\")\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#key-considerations_2","title":"Key Considerations","text":"<ul> <li>Inject context into system message (not task) for better tool selection</li> <li>Track which tools were used for learning tool selection patterns</li> <li>Include tool outcomes in feedback</li> </ul>"},{"location":"INTEGRATION_GUIDE/#async-agents","title":"Async Agents","text":""},{"location":"INTEGRATION_GUIDE/#when-to-use_3","title":"When to Use","text":"<ul> <li>Agent operations are async (browser automation, async APIs)</li> <li>Need non-blocking execution</li> <li>Want to maintain async interface</li> </ul>"},{"location":"INTEGRATION_GUIDE/#pattern_3","title":"Pattern","text":"<pre><code>import asyncio\n\nclass ACEAsyncAgent:\n    \"\"\"Wraps async agent with ACE learning.\"\"\"\n\n    def __init__(self, async_agent, ace_model: str = \"gpt-4o-mini\"):\n        self.agent = async_agent\n        self.skillbook = Skillbook()\n\n        llm = LiteLLMClient(model=ace_model, max_tokens=2048)\n        self.reflector = Reflector(llm)\n        self.skill_manager = SkillManager(llm)\n\n    async def run(self, task: str):\n        \"\"\"Async execution with ACE learning.\"\"\"\n        # Inject context\n        if self.skillbook.skills():\n            task = f\"{task}\\n\\n{wrap_skillbook_context(self.skillbook)}\"\n\n        # Execute (async)\n        result = await self.agent.execute(task)\n\n        # Learn (sync operations in thread)\n        await asyncio.to_thread(self._learn, task, result)\n\n        return result\n\n    def _learn(self, task: str, result):\n        \"\"\"Sync learning pipeline (runs in thread).\"\"\"\n        agent_output = AgentOutput(\n            reasoning=f\"Async task: {task}\",\n            final_answer=result[\"output\"],\n            skill_ids=[],\n            raw={\"success\": result[\"success\"]}\n        )\n\n        feedback = f\"Async task {'succeeded' if result['success'] else 'failed'}\"\n\n        reflection = self.reflector.reflect(\n            question=task,\n            agent_output=agent_output,\n            skillbook=self.skillbook,\n            feedback=feedback\n        )\n\n        skill_manager_output = self.skill_manager.update_skills(\n            reflection=reflection,\n            skillbook=self.skillbook,\n            question_context=task,\n            progress=task\n        )\n\n        self.skillbook.apply_update(skill_manager_output.update)\n\n# Usage\nasync def main():\n    async_agent = MyAsyncAgent()\n    ace_agent = ACEAsyncAgent(async_agent)\n\n    result = await ace_agent.run(\"Fetch and process data\")\n    print(f\"Result: {result}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#key-considerations_3","title":"Key Considerations","text":"<ul> <li>Use <code>asyncio.to_thread()</code> to run sync Reflector/SkillManager in background</li> <li>Don't block async event loop with sync ACE operations</li> <li>Consider batching learning for high-throughput async systems</li> </ul>"},{"location":"INTEGRATION_GUIDE/#chat-based-agents","title":"Chat-Based Agents","text":""},{"location":"INTEGRATION_GUIDE/#when-to-use_4","title":"When to Use","text":"<ul> <li>Agent maintains conversation history</li> <li>Multi-turn interactions</li> <li>Want to learn from entire conversation</li> </ul>"},{"location":"INTEGRATION_GUIDE/#pattern_4","title":"Pattern","text":"<pre><code>class ACEChatAgent:\n    \"\"\"Wraps chat agent with per-conversation learning.\"\"\"\n\n    def __init__(self, chat_agent, ace_model: str = \"gpt-4o-mini\"):\n        self.agent = chat_agent\n        self.skillbook = Skillbook()\n        self.conversation_history = []\n\n        llm = LiteLLMClient(model=ace_model, max_tokens=2048)\n        self.reflector = Reflector(llm)\n        self.skill_manager = SkillManager(llm)\n\n    def chat(self, message: str) -&gt; str:\n        \"\"\"Single chat turn with context injection.\"\"\"\n        # Inject skillbook on first message\n        if len(self.conversation_history) == 0 and self.skillbook.skills():\n            system_context = wrap_skillbook_context(self.skillbook)\n            self.agent.add_system_message(system_context)\n\n        # Chat\n        response = self.agent.chat(message)\n\n        # Track conversation\n        self.conversation_history.append({\"user\": message, \"assistant\": response})\n\n        return response\n\n    def end_conversation(self, success: bool = True, feedback: str = \"\"):\n        \"\"\"Learn from entire conversation at the end.\"\"\"\n        if not self.conversation_history:\n            return\n\n        # Build conversation summary\n        conversation = \"\\n\".join(\n            f\"User: {turn['user']}\\nAssistant: {turn['assistant']}\"\n            for turn in self.conversation_history\n        )\n\n        # Learn from full conversation\n        agent_output = AgentOutput(\n            reasoning=conversation,\n            final_answer=self.conversation_history[-1][\"assistant\"],\n            skill_ids=[],\n            raw={\"turns\": len(self.conversation_history)}\n        )\n\n        feedback_text = (\n            f\"Conversation {'succeeded' if success else 'failed'} \"\n            f\"over {len(self.conversation_history)} turns. {feedback}\"\n        )\n\n        reflection = self.reflector.reflect(\n            question=self.conversation_history[0][\"user\"],\n            agent_output=agent_output,\n            skillbook=self.skillbook,\n            feedback=feedback_text\n        )\n\n        skill_manager_output = self.skill_manager.update_skills(\n            reflection=reflection,\n            skillbook=self.skillbook,\n            question_context=f\"Multi-turn conversation ({len(self.conversation_history)} turns)\",\n            progress=\"Conversation completed\"\n        )\n\n        self.skillbook.apply_update(skill_manager_output.update)\n\n        # Reset for next conversation\n        self.conversation_history = []\n\n# Usage\nchat_agent = MyChatAgent()\nace_agent = ACEChatAgent(chat_agent)\n\n# Multi-turn conversation\nace_agent.chat(\"Hello, I need help with X\")\nace_agent.chat(\"Can you clarify Y?\")\nace_agent.chat(\"Thanks, that works!\")\n\n# Learn from entire conversation\nace_agent.end_conversation(success=True, feedback=\"User satisfied\")\nace_agent.skillbook.save_to_file(\"chat_agent_learned.json\")\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#key-considerations_4","title":"Key Considerations","text":"<ul> <li>Learn from complete conversation (not individual turns)</li> <li>Inject skillbook context at conversation start</li> <li>Allow manual feedback at conversation end</li> </ul>"},{"location":"INTEGRATION_GUIDE/#batch-processing-agents","title":"Batch Processing Agents","text":""},{"location":"INTEGRATION_GUIDE/#when-to-use_5","title":"When to Use","text":"<ul> <li>Processing large batches of similar tasks</li> <li>Want to amortize learning costs</li> <li>Need high throughput</li> </ul>"},{"location":"INTEGRATION_GUIDE/#pattern_5","title":"Pattern","text":"<pre><code>class ACEBatchAgent:\n    \"\"\"Wraps agent with batched learning.\"\"\"\n\n    def __init__(self, agent, ace_model: str = \"gpt-4o-mini\", learn_every: int = 10):\n        self.agent = agent\n        self.skillbook = Skillbook()\n        self.learn_every = learn_every\n        self.pending_results = []\n\n        llm = LiteLLMClient(model=ace_model, max_tokens=2048)\n        self.reflector = Reflector(llm)\n        self.skill_manager = SkillManager(llm)\n\n    def process(self, task: str):\n        \"\"\"Process single task (learn in batches).\"\"\"\n        # Inject context\n        if self.skillbook.skills():\n            task = f\"{task}\\n\\n{wrap_skillbook_context(self.skillbook)}\"\n\n        # Execute\n        result = self.agent.execute(task)\n\n        # Add to pending\n        self.pending_results.append((task, result))\n\n        # Learn when batch is full\n        if len(self.pending_results) &gt;= self.learn_every:\n            self._learn_from_batch()\n\n        return result\n\n    def _learn_from_batch(self):\n        \"\"\"Learn from accumulated results.\"\"\"\n        if not self.pending_results:\n            return\n\n        # Aggregate feedback\n        successes = sum(1 for _, r in self.pending_results if r[\"success\"])\n        failures = len(self.pending_results) - successes\n\n        # Learn from batch summary\n        feedback = (\n            f\"Batch of {len(self.pending_results)} tasks: \"\n            f\"{successes} succeeded, {failures} failed\"\n        )\n\n        # Use first task as representative\n        task, result = self.pending_results[0]\n\n        agent_output = AgentOutput(\n            reasoning=f\"Batch processing: {feedback}\",\n            final_answer=result[\"output\"],\n            skill_ids=[],\n            raw={\"batch_size\": len(self.pending_results), \"success_rate\": successes / len(self.pending_results)}\n        )\n\n        reflection = self.reflector.reflect(\n            question=task,\n            agent_output=agent_output,\n            skillbook=self.skillbook,\n            feedback=feedback\n        )\n\n        skill_manager_output = self.skill_manager.update_skills(\n            reflection=reflection,\n            skillbook=self.skillbook,\n            question_context=f\"Batch processing ({len(self.pending_results)} items)\",\n            progress=\"Batch completed\"\n        )\n\n        self.skillbook.apply_update(skill_manager_output.update)\n\n        # Clear pending\n        self.pending_results = []\n\n    def flush(self):\n        \"\"\"Force learning from remaining pending results.\"\"\"\n        self._learn_from_batch()\n\n# Usage\nagent = MyBatchAgent()\nace_agent = ACEBatchAgent(agent, learn_every=10)\n\n# Process many tasks\nfor task in tasks:\n    ace_agent.process(task)\n\n# Learn from remainder\nace_agent.flush()\nace_agent.skillbook.save_to_file(\"batch_learned.json\")\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#key-considerations_5","title":"Key Considerations","text":"<ul> <li>Balance learning frequency vs cost (learn_every parameter)</li> <li>Call <code>flush()</code> at end to learn from remaining items</li> <li>Consider success rate in batch feedback</li> </ul>"},{"location":"INTEGRATION_GUIDE/#streaming-agents","title":"Streaming Agents","text":""},{"location":"INTEGRATION_GUIDE/#when-to-use_6","title":"When to Use","text":"<ul> <li>Agent streams responses token-by-token</li> <li>Want to maintain streaming interface</li> <li>Learn after complete stream</li> </ul>"},{"location":"INTEGRATION_GUIDE/#pattern_6","title":"Pattern","text":"<pre><code>class ACEStreamingAgent:\n    \"\"\"Wraps streaming agent with post-stream learning.\"\"\"\n\n    def __init__(self, streaming_agent, ace_model: str = \"gpt-4o-mini\"):\n        self.agent = streaming_agent\n        self.skillbook = Skillbook()\n\n        llm = LiteLLMClient(model=ace_model, max_tokens=2048)\n        self.reflector = Reflector(llm)\n        self.skill_manager = SkillManager(llm)\n\n    def stream(self, task: str):\n        \"\"\"Stream response with learning after completion.\"\"\"\n        # Inject context\n        if self.skillbook.skills():\n            task = f\"{task}\\n\\n{wrap_skillbook_context(self.skillbook)}\"\n\n        # Collect full response while streaming\n        full_response = []\n\n        for chunk in self.agent.stream(task):\n            full_response.append(chunk)\n            yield chunk  # Stream to caller\n\n        # Learn after stream completes\n        complete_response = \"\".join(full_response)\n        self._learn(task, complete_response)\n\n    def _learn(self, task: str, response: str):\n        \"\"\"Learn from complete streamed response.\"\"\"\n        agent_output = AgentOutput(\n            reasoning=f\"Streamed response for: {task}\",\n            final_answer=response,\n            skill_ids=[],\n            raw={\"response_length\": len(response)}\n        )\n\n        feedback = f\"Streamed {len(response)} characters\"\n\n        reflection = self.reflector.reflect(\n            question=task,\n            agent_output=agent_output,\n            skillbook=self.skillbook,\n            feedback=feedback\n        )\n\n        skill_manager_output = self.skill_manager.update_skills(\n            reflection=reflection,\n            skillbook=self.skillbook,\n            question_context=task,\n            progress=\"Streaming completed\"\n        )\n\n        self.skillbook.apply_update(skill_manager_output.update)\n\n# Usage\nstreaming_agent = MyStreamingAgent()\nace_agent = ACEStreamingAgent(streaming_agent)\n\nfor chunk in ace_agent.stream(\"Generate report\"):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#key-considerations_6","title":"Key Considerations","text":"<ul> <li>Collect full response before learning</li> <li>Don't block streaming (learn after completion)</li> <li>Maintain streaming interface for caller</li> </ul>"},{"location":"INTEGRATION_GUIDE/#error-prone-agents","title":"Error-Prone Agents","text":""},{"location":"INTEGRATION_GUIDE/#when-to-use_7","title":"When to Use","text":"<ul> <li>Agent frequently fails or throws exceptions</li> <li>Want to learn from failures</li> <li>Need robust error handling</li> </ul>"},{"location":"INTEGRATION_GUIDE/#pattern_7","title":"Pattern","text":"<pre><code>class ACERobustAgent:\n    \"\"\"Wraps agent with error handling and failure learning.\"\"\"\n\n    def __init__(self, agent, ace_model: str = \"gpt-4o-mini\", max_retries: int = 3):\n        self.agent = agent\n        self.skillbook = Skillbook()\n        self.max_retries = max_retries\n\n        llm = LiteLLMClient(model=ace_model, max_tokens=2048)\n        self.reflector = Reflector(llm)\n        self.skill_manager = SkillManager(llm)\n\n    def run(self, task: str):\n        \"\"\"Execute with retries and error learning.\"\"\"\n        # Inject context\n        if self.skillbook.skills():\n            task = f\"{task}\\n\\n{wrap_skillbook_context(self.skillbook)}\"\n\n        last_error = None\n        for attempt in range(self.max_retries):\n            try:\n                result = self.agent.execute(task)\n                # Success - learn from it\n                self._learn(task, result, success=True)\n                return result\n\n            except Exception as e:\n                last_error = str(e)\n                if attempt &lt; self.max_retries - 1:\n                    # Retry\n                    continue\n                else:\n                    # Final failure - learn from it\n                    self._learn(task, None, success=False, error=last_error)\n                    raise\n\n    def _learn(self, task: str, result, success: bool, error: str = None):\n        \"\"\"Learn from both successes and failures.\"\"\"\n        try:\n            # Build feedback\n            if success:\n                feedback = f\"Task succeeded. Output: {result['output']}\"\n                final_answer = result[\"output\"]\n            else:\n                feedback = f\"Task failed after {self.max_retries} attempts. Error: {error}\"\n                final_answer = \"\"\n\n            # Adapter\n            agent_output = AgentOutput(\n                reasoning=f\"Task: {task}. {feedback}\",\n                final_answer=final_answer,\n                skill_ids=[],\n                raw={\"success\": success, \"error\": error}\n            )\n\n            # Reflect + Update skills\n            reflection = self.reflector.reflect(\n                question=task,\n                agent_output=agent_output,\n                skillbook=self.skillbook,\n                feedback=feedback\n            )\n\n            skill_manager_output = self.skill_manager.update_skills(\n                reflection=reflection,\n                skillbook=self.skillbook,\n                question_context=f\"Task ({'success' if success else 'failure'}): {task}\",\n                progress=\"Execution completed\"\n            )\n\n            self.skillbook.apply_update(skill_manager_output.update)\n\n        except Exception as learning_error:\n            # Never crash due to learning failures\n            print(f\"Learning failed: {learning_error}\")\n\n# Usage\nerror_prone_agent = MyUnreliableAgent()\nace_agent = ACERobustAgent(error_prone_agent, max_retries=3)\n\ntry:\n    result = ace_agent.run(\"Risky task\")\nexcept Exception as e:\n    print(f\"Task failed: {e}\")\n    # But skillbook learned from the failure!\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#key-considerations_7","title":"Key Considerations","text":"<ul> <li>Learn from both successes AND failures</li> <li>Wrap learning in try/except (never crash from learning)</li> <li>Include error details in feedback for failure pattern learning</li> </ul>"},{"location":"INTEGRATION_GUIDE/#advanced-topics","title":"Advanced Topics","text":""},{"location":"INTEGRATION_GUIDE/#rich-feedback-extraction","title":"Rich Feedback Extraction","text":"<p>The quality of ACE learning depends on the feedback you provide. The more detailed, the better.</p> <p>Basic Feedback (Minimal): <pre><code>feedback = f\"Task {'succeeded' if success else 'failed'}\"\n</code></pre></p> <p>Good Feedback (Contextual): <pre><code>feedback = f\"\"\"\nTask {'succeeded' if success else 'failed'} in {steps} steps.\nDuration: {duration}s\nFinal output: {output[:200]}...\n\"\"\"\n</code></pre></p> <p>Rich Feedback (Detailed Trace): <pre><code># For agents with step-by-step execution\nfeedback_parts = []\nfeedback_parts.append(f\"Task {status} in {steps} steps\")\n\n# Add execution trace\nfor i, step in enumerate(execution_steps, 1):\n    feedback_parts.append(f\"\\nStep {i}:\")\n    feedback_parts.append(f\"  Thought: {step.thought}\")\n    feedback_parts.append(f\"  Action: {step.action}\")\n    feedback_parts.append(f\"  Result: {step.result}\")\n\nfeedback = \"\\n\".join(feedback_parts)\n</code></pre></p> <p>Benefits of Rich Feedback: - Learns action sequencing patterns - Understands timing requirements - Recognizes error patterns - Captures domain-specific knowledge</p>"},{"location":"INTEGRATION_GUIDE/#citation-based-strategy-tracking","title":"Citation-Based Strategy Tracking","text":"<p>ACE uses citations to track which strategies were used:</p> <p>How It Works: 1. Strategies are formatted with IDs: <code>[section-00001]</code> 2. Agent cites them in reasoning: <code>\"Following [navigation-00042], I will...\"</code> 3. ACE extracts citations automatically</p> <p>Extracting Citations: <pre><code>from ace.roles import extract_cited_skill_ids\n\n# Agent's reasoning with citations\nreasoning = \"\"\"\nStep 1: Following [navigation-00042], navigate to main page.\nStep 2: Using [extraction-00003], extract title element.\n\"\"\"\n\n# Extract citations\ncited_ids = extract_cited_skill_ids(reasoning)\n# Returns: ['navigation-00042', 'extraction-00003']\n\n# Pass to AgentOutput\nagent_output = AgentOutput(\n    reasoning=reasoning,\n    final_answer=result,\n    skill_ids=cited_ids,\n    raw={}\n)\n</code></pre></p> <p>For External Agents: <pre><code># Extract from agent's thought process\nif hasattr(history, 'model_thoughts'):\n    thoughts = history.model_thoughts()\n    thoughts_text = \"\\n\".join(t.thinking for t in thoughts)\n    cited_ids = extract_cited_skill_ids(thoughts_text)\n</code></pre></p>"},{"location":"INTEGRATION_GUIDE/#handling-async-agents","title":"Handling Async Agents","text":"<p>If your agent is async, wrap the learning in a sync function:</p> <pre><code>async def run(self, task: str):\n    # Inject context\n    enhanced_task = self._inject_context(task)\n\n    # Execute (async)\n    result = await self.agent.execute(enhanced_task)\n\n    # Learn (sync Reflector/SkillManager)\n    if self.is_learning:\n        await asyncio.to_thread(self._learn, task, result)\n\n    return result\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#error-handling","title":"Error Handling","text":"<p>Always wrap learning in try/except to prevent crashes:</p> <pre><code>def _learn(self, task: str, result):\n    try:\n        # Reflection\n        reflection = self.reflector.reflect(...)\n\n        # Update skills\n        skill_manager_output = self.skill_manager.update_skills(...)\n\n        # Update\n        self.skillbook.apply_update(skill_manager_output.update)\n\n    except Exception as e:\n        logger.error(f\"ACE learning failed: {e}\")\n        # Continue without learning - don't crash!\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#token-limits","title":"Token Limits","text":"<p>ACE learning components need sufficient tokens:</p> <pre><code># Reflector: 400-800 tokens typical\n# SkillManager: 300-1000 tokens typical\nllm = LiteLLMClient(model=\"gpt-4o-mini\", max_tokens=2048)  # Recommended\n\n# For complex tasks with long traces:\nllm = LiteLLMClient(model=\"gpt-4o-mini\", max_tokens=4096)\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#troubleshooting","title":"Troubleshooting","text":""},{"location":"INTEGRATION_GUIDE/#problem-json-parsing-errors-from-skillmanager","title":"Problem: JSON Parsing Errors from SkillManager","text":"<p>Cause: Insufficient <code>max_tokens</code> for structured output</p> <p>Solution: <pre><code>llm = LiteLLMClient(model=\"gpt-4o-mini\", max_tokens=2048)  # or higher\n</code></pre></p>"},{"location":"INTEGRATION_GUIDE/#problem-not-learning-anything","title":"Problem: Not Learning Anything","text":"<p>Checks: 1. Is <code>is_learning=True</code>? 2. Is SkillManager output non-empty? <code>print(skill_manager_output.update)</code> 3. Is skillbook being saved? <code>skillbook.save_to_file(...)</code></p>"},{"location":"INTEGRATION_GUIDE/#problem-too-many-skills","title":"Problem: Too Many Skills","text":"<p>Solution: SkillManager automatically manages skills via TAG operations. Review with: <pre><code>skills = skillbook.skills()\nprint(f\"Total: {len(skills)}\")\nfor s in skills[:10]:\n    print(f\"[{s.id}] +{s.helpful}/-{s.harmful}: {s.content}\")\n</code></pre></p>"},{"location":"INTEGRATION_GUIDE/#problem-high-api-costs","title":"Problem: High API Costs","text":"<p>Solutions: - Use cheaper model: <code>ace_model=\"gpt-4o-mini\"</code> - Disable learning for simple tasks: <code>is_learning=False</code> - Batch learning: Learn only every N tasks</p>"},{"location":"INTEGRATION_GUIDE/#problem-agent-ignores-skillbook-strategies","title":"Problem: Agent Ignores Skillbook Strategies","text":"<p>Checks: 1. Are you actually injecting context? <code>print(enhanced_task)</code> 2. Does skillbook have skills? <code>print(len(skillbook.skills()))</code> 3. Is context clear enough for your agent?</p>"},{"location":"INTEGRATION_GUIDE/#next-steps","title":"Next Steps","text":"<ol> <li>Start Simple: Use the wrapper class template above</li> <li>Adapt <code>_learn()</code>: Customize for your agent's output format</li> <li>Test Without Learning: Set <code>is_learning=False</code> first</li> <li>Enable Learning: Turn on and monitor skillbook growth</li> <li>Iterate: Improve feedback extraction for better learning</li> </ol>"},{"location":"INTEGRATION_GUIDE/#see-also","title":"See Also","text":"<ul> <li>Out-of-box integrations: ACELiteLLM, ACEAgent (browser-use), ACELangChain</li> <li>Full ACE guide: COMPLETE_GUIDE_TO_ACE.md</li> <li>API reference: API_REFERENCE.md</li> </ul> <p>Questions? Join our Discord</p>"},{"location":"OPIK/","title":"Opik Integration Guide","text":"<p>This guide covers how to use Opik for tracing, monitoring, and cost tracking with the ACE framework.</p>"},{"location":"OPIK/#overview","title":"Overview","text":"<p>ACE framework includes built-in Opik integration for: - Tracing: Track Agent, Reflector, and SkillManager interactions - Cost Tracking: Automatic token usage and LLM cost monitoring - Performance Analysis: Visualize learning loops and skill evolution - Debugging: Inspect detailed traces of the ACE pipeline</p>"},{"location":"OPIK/#installation","title":"Installation","text":"<pre><code># Install ACE with observability features\npip install ace-framework[observability]\n\n# Or for development (includes Opik in optional deps)\nuv sync\n</code></pre>"},{"location":"OPIK/#starting-opik","title":"Starting Opik","text":""},{"location":"OPIK/#local-development-recommended","title":"Local Development (Recommended)","text":"<p>Run the local Opik server using Docker:</p> <pre><code># Start Opik server\ndocker run -d -p 5173:5173 --name opik ghcr.io/comet-ml/opik:latest\n\n# View traces at http://localhost:5173\n</code></pre>"},{"location":"OPIK/#comet-cloud","title":"Comet Cloud","text":"<p>For production, use Comet's hosted Opik:</p> <pre><code># Set your Comet API key\nexport COMET_API_KEY=\"your-api-key\"\n</code></pre>"},{"location":"OPIK/#quick-start","title":"Quick Start","text":""},{"location":"OPIK/#script-initialization","title":"Script Initialization","text":"<p>Add this at the start of your <code>main()</code> function:</p> <pre><code>import os\nfrom ace.observability.opik_integration import configure_opik\n\ndef main():\n    # Initialize Opik with project name\n    project_name = os.environ.get(\"OPIK_PROJECT_NAME\", \"ace-default\")\n    opik_integration = configure_opik(project_name=project_name)\n\n    if opik_integration.is_available():\n        # Register LiteLLM callback for automatic token tracking\n        opik_integration.setup_litellm_callback()\n        print(f\"Opik tracing enabled for project: {project_name}\")\n\n    # Your ACE code here...\n</code></pre>"},{"location":"OPIK/#running-with-tracing","title":"Running with Tracing","text":"<pre><code># Run with custom project name\nOPIK_PROJECT_NAME=\"my-experiment\" uv run python my_script.py\n\n# View traces at http://localhost:5173\n</code></pre>"},{"location":"OPIK/#environment-variables","title":"Environment Variables","text":"Variable Description Default <code>OPIK_PROJECT_NAME</code> Project name for organizing traces <code>ace-framework</code> <code>OPIK_DISABLED=true</code> Disable all Opik tracing Not set <code>OPIK_ENABLED=false</code> Alternative way to disable tracing Not set <code>OPIK_URL_OVERRIDE</code> Custom Opik server URL <code>http://localhost:5173/api</code> <code>OPIK_WORKSPACE</code> Opik workspace name <code>default</code>"},{"location":"OPIK/#component-tracing","title":"Component Tracing","text":""},{"location":"OPIK/#automatically-traced-components","title":"Automatically Traced Components","text":"<p>The following ACE components have built-in tracing:</p> Component Trace Name Tags <code>Agent.generate()</code> <code>agent_generate</code> <code>[\"agent\"]</code> <code>Reflector.reflect()</code> <code>reflector_reflect</code> <code>[\"reflector\"]</code> <code>SkillManager.update_skills()</code> <code>skill_manager_update_skills</code> <code>[\"skill_manager\"]</code> <code>RecursiveReflector.reflect()</code> <code>recursive_reflector</code> <code>[\"reflector\", \"recursive\"]</code> <code>ReplayAgent.generate()</code> <code>replay_agent_generate</code> <code>[\"agent\", \"replay\"]</code>"},{"location":"OPIK/#adding-tracing-to-custom-components","title":"Adding Tracing to Custom Components","text":"<p>Use the <code>@maybe_track</code> decorator to add tracing to your own components:</p> <pre><code>from ace.observability.tracers import maybe_track\n\nclass MyCustomComponent:\n    @maybe_track(name=\"my_component_process\", tags=[\"custom\", \"processing\"])\n    def process(self, input_data):\n        # Your processing logic here\n        result = self._do_work(input_data)\n        return result\n</code></pre> <p>The <code>@maybe_track</code> decorator: - Only applies tracing when Opik is installed and enabled - Gracefully degrades to no-op when Opik is unavailable - Respects <code>OPIK_DISABLED</code> environment variable</p>"},{"location":"OPIK/#automatic-token-cost-tracking","title":"Automatic Token &amp; Cost Tracking","text":"<p>When you call <code>setup_litellm_callback()</code>, all LiteLLM calls are automatically tracked with:</p> <ul> <li>Input/output tokens</li> <li>Model used</li> <li>Cost per call</li> <li>Latency</li> </ul> <pre><code>from ace.observability.opik_integration import configure_opik\nfrom ace.llm_providers.litellm_client import LiteLLMClient\n\n# Initialize Opik\nopik_integration = configure_opik(project_name=\"cost-tracking-demo\")\nif opik_integration.is_available():\n    opik_integration.setup_litellm_callback()\n\n# All LLM calls are now tracked\nclient = LiteLLMClient(model=\"gpt-4\")\nresponse = client.complete(\"What is 2+2?\")\n# Token usage and cost automatically logged to Opik\n</code></pre>"},{"location":"OPIK/#viewing-traces","title":"Viewing Traces","text":""},{"location":"OPIK/#local-opik-ui","title":"Local Opik UI","text":"<ol> <li>Start the Opik server (see above)</li> <li>Open http://localhost:5173 in your browser</li> <li>Select your project from the dropdown</li> <li>Browse traces, spans, and metrics</li> </ol>"},{"location":"OPIK/#trace-hierarchy","title":"Trace Hierarchy","text":"<p>ACE traces are organized hierarchically:</p> <pre><code>[Project: my-experiment]\n  \u2514\u2500\u2500 [Trace: script_run_12345]\n        \u251c\u2500\u2500 [Span: agent_generate]\n        \u2502     \u2514\u2500\u2500 [LLM Call: gpt-4]\n        \u251c\u2500\u2500 [Span: reflector_reflect]\n        \u2502     \u2514\u2500\u2500 [LLM Call: gpt-4]\n        \u2514\u2500\u2500 [Span: skill_manager_update_skills]\n              \u2514\u2500\u2500 [LLM Call: gpt-4]\n</code></pre>"},{"location":"OPIK/#advanced-usage","title":"Advanced Usage","text":""},{"location":"OPIK/#logging-custom-metrics","title":"Logging Custom Metrics","text":"<p>Use the <code>OpikIntegration</code> class to log custom metrics:</p> <pre><code>from ace.observability.opik_integration import get_integration\n\nopik = get_integration()\n\n# Log skill evolution\nopik.log_skill_evolution(\n    skill_id=\"skill_001\",\n    skill_content=\"Always validate input...\",\n    helpful_count=5,\n    harmful_count=1,\n    neutral_count=2,\n    section=\"error_handling\"\n)\n\n# Log adaptation metrics\nopik.log_adaptation_metrics(\n    epoch=2,\n    step=15,\n    performance_score=0.85,\n    skill_count=12,\n    successful_predictions=85,\n    total_predictions=100\n)\n</code></pre>"},{"location":"OPIK/#disabling-tracing-for-tests","title":"Disabling Tracing for Tests","text":"<pre><code># Disable in CI/tests\nOPIK_DISABLED=true pytest tests/\n\n# Or in code\nimport os\nos.environ[\"OPIK_DISABLED\"] = \"true\"\n</code></pre>"},{"location":"OPIK/#using-with-async-code","title":"Using with Async Code","text":"<p>The <code>@maybe_track</code> decorator works with both sync and async functions:</p> <pre><code>from ace.observability.tracers import maybe_track\n\nclass AsyncComponent:\n    @maybe_track(name=\"async_process\", tags=[\"async\"])\n    async def process(self, data):\n        result = await self._async_work(data)\n        return result\n</code></pre>"},{"location":"OPIK/#troubleshooting","title":"Troubleshooting","text":""},{"location":"OPIK/#opik-not-available","title":"\"Opik not available\"","text":"<p>Install the observability extras: <pre><code>pip install ace-framework[observability]\n# or\nuv add opik\n</code></pre></p>"},{"location":"OPIK/#traces-not-appearing","title":"Traces not appearing","text":"<ol> <li>Check Opik server is running: <code>curl http://localhost:5173/api/health</code></li> <li>Verify <code>OPIK_DISABLED</code> is not set</li> <li>Ensure <code>configure_opik()</code> is called before any traced functions</li> </ol>"},{"location":"OPIK/#async-warnings","title":"Async warnings","text":"<p>Warnings like \"coroutine was never awaited\" in synchronous contexts are harmless and can be ignored. They occur when Opik's async internals interact with sync code.</p>"},{"location":"OPIK/#high-memory-usage","title":"High memory usage","text":"<p>For long-running scripts, traces accumulate in memory. Consider: - Running in batches with separate processes - Flushing traces periodically (if supported by Opik)</p>"},{"location":"OPIK/#example-full-ace-pipeline-with-tracing","title":"Example: Full ACE Pipeline with Tracing","text":"<pre><code>import os\nfrom ace import Skillbook, Agent, Reflector, SkillManager, OfflineACE\nfrom ace.llm_providers.litellm_client import LiteLLMClient\nfrom ace.observability.opik_integration import configure_opik\n\ndef main():\n    # 1. Initialize Opik\n    project_name = os.environ.get(\"OPIK_PROJECT_NAME\", \"ace-training\")\n    opik_integration = configure_opik(project_name=project_name)\n    if opik_integration.is_available():\n        opik_integration.setup_litellm_callback()\n        print(f\"Tracing enabled: {project_name}\")\n\n    # 2. Create ACE components (all LLM calls will be traced)\n    llm = LiteLLMClient(model=\"gpt-4\")\n    skillbook = Skillbook()\n    agent = Agent(llm)\n    reflector = Reflector(llm)\n    skill_manager = SkillManager(llm)\n\n    # 3. Run adaptation (traces appear in Opik)\n    adapter = OfflineACE(\n        skillbook=skillbook,\n        agent=agent,\n        reflector=reflector,\n        skill_manager=skill_manager\n    )\n\n    results = adapter.run(samples, environment, epochs=3)\n\n    # 4. View traces at http://localhost:5173\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Run with: <pre><code>OPIK_PROJECT_NAME=\"training-run-001\" uv run python train.py\n</code></pre></p>"},{"location":"OPIK/#related-documentation","title":"Related Documentation","text":"<ul> <li>Quick Start Guide</li> <li>Integration Guide</li> <li>API Reference</li> </ul>"},{"location":"PROMPTS/","title":"ACE Framework - Prompt Template Guide","text":"<p>This guide explains the different prompt versions available in ACE, their use cases, and how to migrate between versions.</p>"},{"location":"PROMPTS/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Prompt Versions</li> <li>Template Variables</li> <li>Version Comparison</li> <li>Migration Guide</li> <li>Custom Prompts</li> </ul>"},{"location":"PROMPTS/#prompt-versions","title":"Prompt Versions","text":""},{"location":"PROMPTS/#v10-simple-acepromptspy","title":"v1.0 (Simple) - <code>ace/prompts.py</code>","text":"<p>Status: \u2705 Maintained Use Case: Quick starts, tutorials, minimal examples Lines of Code: 149</p> <p>Characteristics: - Simple, straightforward templates - Minimal formatting and structure - Best for understanding ACE fundamentals - Lower token usage - Suitable for weaker models (GPT-3.5, etc.)</p> <p>Example: <pre><code>from ace.prompts import AGENT_PROMPT\nfrom ace import Agent, LiteLLMClient\n\nllm = LiteLLMClient(model=\"gpt-3.5-turbo\")\nagent = Agent(llm, prompt_template=AGENT_PROMPT)\n</code></pre></p>"},{"location":"PROMPTS/#v20-advanced-aceprompts_v2py","title":"v2.0 (Advanced) - <code>ace/prompts_v2.py</code>","text":"<p>Status: \u26a0\ufe0f DEPRECATED (use v2.1 instead) Use Case: None - superseded by v2.1 Lines of Code: 984</p> <p>Deprecation Notice: <pre><code># Will emit DeprecationWarning\nfrom ace.prompts_v2 import AGENT_V2_PROMPT\n</code></pre></p> <p>Why Deprecated: v2.1 includes all v2.0 features plus MCP enhancements and better error handling. There's no reason to use v2.0 over v2.1.</p>"},{"location":"PROMPTS/#v21-recommended-aceprompts_v2_1py","title":"v2.1 (Recommended) - <code>ace/prompts_v2_1.py</code>","text":"<p>Status: \u2705 Recommended for production Use Case: Production systems, advanced applications, best performance Lines of Code: 1469</p> <p>Characteristics: - State-of-the-art prompt engineering - MCP (Model Context Protocol) techniques - Identity headers with metadata - Hierarchical organization - Meta-cognitive instructions - Enhanced error handling - Optimized for Claude 3.5, GPT-4, and similar models</p> <p>Example: <pre><code>from ace.prompts_v2_1 import PromptManager\n\n# Use the recommended prompts\nprompt_mgr = PromptManager()\nagent = Agent(llm, prompt_template=prompt_mgr.get_agent_prompt())\nreflector = Reflector(llm, prompt_template=prompt_mgr.get_reflector_prompt())\nskill_manager = SkillManager(llm, prompt_template=prompt_mgr.get_skill_manager_prompt())\n</code></pre></p>"},{"location":"PROMPTS/#version-comparison","title":"Version Comparison","text":"<p>Quick guide to choosing the right prompt version:</p> Feature v1.0 v2.0 (Deprecated) v2.1 (Recommended) Status Stable Deprecated Recommended Performance Baseline +12% vs v1 +17% vs v1 Lines of Code 146 969 1,469 Use Case Tutorials, simple tasks N/A (use v2.1) Production systems MCP Support \u274c No \u274c No \u2705 Yes Error Handling Basic Enhanced Advanced Meta-Cognition None Basic Advanced Examples Included \u274c No \u2705 Yes \u2705 Yes Identity Headers \u274c No \u2705 Yes \u2705 Enhanced Validation Basic Strict Strict + Recovery Best For Learning ACE basics Don't use All production use <p>Recommendation: Use v2.1 for all new projects. Only use v1 for educational/tutorial purposes where simplicity is more important than performance.</p> <p>Migration: Switching from v1 \u2192 v2.1 is straightforward: <pre><code># Before (v1)\nfrom ace.prompts import AGENT_PROMPT\nagent = Agent(llm, prompt_template=AGENT_PROMPT)\n\n# After (v2.1)\nfrom ace.prompts_v2_1 import PromptManager\nmgr = PromptManager()\nagent = Agent(llm, prompt_template=mgr.get_agent_prompt())\n</code></pre></p>"},{"location":"PROMPTS/#template-variables","title":"Template Variables","text":"<p>All ACE prompts use Python's <code>.format()</code> syntax with these variables:</p>"},{"location":"PROMPTS/#agent-prompts","title":"Agent Prompts","text":"Variable Type Description Required <code>{skillbook}</code> str Formatted skillbook skills \u2705 Yes <code>{question}</code> str The question to answer \u2705 Yes <code>{context}</code> str Additional context (optional) \u274c No <code>{reflection}</code> str Prior reflection (optional) \u274c No <p>Output Format (JSON): <pre><code>{\n  \"reasoning\": \"Step-by-step thinking process\",\n  \"final_answer\": \"The actual answer\",\n  \"skill_ids\": [\"skill1\", \"skill2\"]\n}\n</code></pre></p>"},{"location":"PROMPTS/#reflector-prompts","title":"Reflector Prompts","text":"Variable Type Description Required <code>{skillbook}</code> str Formatted skillbook skills \u2705 Yes <code>{question}</code> str Original question \u2705 Yes <code>{context}</code> str Additional context \u274c No <code>{agent_output}</code> str Agent's JSON output \u2705 Yes <code>{feedback}</code> str Environment feedback \u2705 Yes <code>{ground_truth}</code> str Expected answer (optional) \u274c No <p>Output Format (JSON): <pre><code>{\n  \"analysis\": \"Analysis of what went wrong/right\",\n  \"skill_tags\": [\n    {\"id\": \"skill1\", \"tag\": \"helpful\"},\n    {\"id\": \"skill2\", \"tag\": \"harmful\"}\n  ]\n}\n</code></pre></p>"},{"location":"PROMPTS/#skillmanager-prompts","title":"SkillManager Prompts","text":"Variable Type Description Required <code>{skillbook}</code> str Current skillbook state \u2705 Yes <code>{reflection}</code> str Reflector's analysis \u2705 Yes <code>{recent_reflections}</code> str Past N reflections \u274c No <p>Output Format (JSON): <pre><code>{\n  \"updates\": [\n    {\"operation\": \"ADD\", \"section\": \"Math\", \"content\": \"Always check units\"},\n    {\"operation\": \"UPDATE\", \"skill_id\": \"s1\", \"content\": \"Revised strategy\"},\n    {\"operation\": \"TAG\", \"skill_id\": \"s2\", \"tag\": \"helpful\", \"increment\": 1},\n    {\"operation\": \"REMOVE\", \"skill_id\": \"s3\"}\n  ]\n}\n</code></pre></p>"},{"location":"PROMPTS/#version-comparison_1","title":"Version Comparison","text":"Feature v1.0 v2.0 v2.1 Token Usage Low High High Complexity Simple Complex Complex Performance Good Better Best Error Handling Basic Good Excellent MCP Techniques \u274c \u274c \u2705 Meta-Cognitive \u274c \u2705 \u2705 Production Ready \u2705 \u26a0\ufe0f \u2705 Status Maintained Deprecated Recommended"},{"location":"PROMPTS/#performance-benchmarks","title":"Performance Benchmarks","text":"<p>Based on internal testing (200 samples, Claude Sonnet 4.5):</p> Metric v1.0 v2.1 Improvement Success Rate 72% 89% +17% JSON Parse Errors 8% 1% -7% Avg Tokens/Call 850 1200 +41% Quality Score 7.2/10 9.1/10 +26% <p>Recommendation: Use v2.1 for production. The token increase is worth the quality gain.</p>"},{"location":"PROMPTS/#migration-guide","title":"Migration Guide","text":""},{"location":"PROMPTS/#v10-v21","title":"v1.0 \u2192 v2.1","text":"<p>Step 1: Update imports <pre><code># Before\nfrom ace.prompts import AGENT_PROMPT, REFLECTOR_PROMPT, SKILL_MANAGER_PROMPT\n\n# After\nfrom ace.prompts_v2_1 import PromptManager\nprompt_mgr = PromptManager()\n</code></pre></p> <p>Step 2: Update role initialization <pre><code># Before\nagent = Agent(llm)  # Uses v1.0 default\n\n# After\nagent = Agent(llm, prompt_template=prompt_mgr.get_agent_prompt())\nreflector = Reflector(llm, prompt_template=prompt_mgr.get_reflector_prompt())\nskill_manager = SkillManager(llm, prompt_template=prompt_mgr.get_skill_manager_prompt())\n</code></pre></p> <p>Step 3: Test and validate - Run your test suite - Monitor JSON parse success rates - Check output quality - Adjust <code>max_retries</code> if needed (v2.1 is more reliable)</p>"},{"location":"PROMPTS/#v20-v21","title":"v2.0 \u2192 v2.1","text":"<p>Minimal changes required:</p> <pre><code># Before (emits DeprecationWarning)\nfrom ace.prompts_v2 import AGENT_V2_PROMPT, REFLECTOR_V2_PROMPT, SKILL_MANAGER_V2_PROMPT\n\n# After\nfrom ace.prompts_v2_1 import PromptManager\nprompt_mgr = PromptManager()\n\nagent = Agent(llm, prompt_template=prompt_mgr.get_agent_prompt())\nreflector = Reflector(llm, prompt_template=prompt_mgr.get_reflector_prompt())\nskill_manager = SkillManager(llm, prompt_template=prompt_mgr.get_skill_manager_prompt())\n</code></pre> <p>Benefits of Upgrading: - +12% fewer JSON parse errors - Better handling of edge cases - MCP-enhanced reasoning - No deprecation warnings</p>"},{"location":"PROMPTS/#custom-prompts","title":"Custom Prompts","text":""},{"location":"PROMPTS/#creating-custom-prompts","title":"Creating Custom Prompts","text":"<p>You can create domain-specific prompts while maintaining ACE compatibility:</p> <pre><code>CUSTOM_AGENT_PROMPT = \"\"\"\nYou are a medical diagnosis assistant using ACE strategies.\n\n# Available Strategies\n{skillbook}\n\n# Patient Question\n{question}\n\n# Medical Context\n{context}\n\n# Previous Reflection\n{reflection}\n\nIMPORTANT: Return JSON with:\n- \"reasoning\": Your diagnostic reasoning\n- \"final_answer\": Your diagnosis and recommendations\n- \"skill_ids\": Strategy IDs you used (e.g., [\"med_01\", \"diag_03\"])\n\nRespond ONLY with valid JSON, no other text.\n\"\"\"\n\n# Use it\nagent = Agent(llm, prompt_template=CUSTOM_AGENT_PROMPT)\n</code></pre>"},{"location":"PROMPTS/#template-requirements","title":"Template Requirements","text":"<p>\u2705 Required: - Include all 4 variables: <code>{skillbook}</code>, <code>{question}</code>, <code>{context}</code>, <code>{reflection}</code> - Specify JSON output format clearly - List required fields: <code>reasoning</code>, <code>final_answer</code>, <code>skill_ids</code></p> <p>\u274c Avoid: - Hardcoding language-specific instructions in prompts - Overly complex nested instructions - Ambiguous output format requirements</p>"},{"location":"PROMPTS/#testing-custom-prompts","title":"Testing Custom Prompts","text":"<pre><code>import json\nfrom ace import Agent, Skillbook, Sample\n\n# Create test setup\nllm = LiteLLMClient(model=\"claude-sonnet-4-5-20250929\")\nskillbook = Skillbook()\nskillbook.add_skill(section=\"Testing\", content=\"Always validate output format\")\n\nagent = Agent(llm, prompt_template=CUSTOM_AGENT_PROMPT)\n\n# Test\noutput = agent.generate(\n    question=\"Test question\",\n    context=\"Test context\",\n    skillbook=skillbook,\n    reflection=None\n)\n\n# Validate\nassert output.reasoning\nassert output.final_answer\nassert isinstance(output.skill_ids, list)\nprint(\"\u2713 Custom prompt works!\")\n</code></pre>"},{"location":"PROMPTS/#advanced-topics","title":"Advanced Topics","text":""},{"location":"PROMPTS/#domain-specific-sections","title":"Domain-Specific Sections","text":"<p>Organize skillbook skills by domain:</p> <pre><code>skillbook.add_skill(\n    section=\"Medical/Diagnosis\",\n    content=\"Check for common symptoms first\"\n)\n\nskillbook.add_skill(\n    section=\"Medical/Treatment\",\n    content=\"Consider contraindications\"\n)\n\nskillbook.add_skill(\n    section=\"Legal/Compliance\",\n    content=\"Verify HIPAA requirements\"\n)\n</code></pre>"},{"location":"PROMPTS/#multi-language-support","title":"Multi-Language Support","text":"<p>v2.1 prompts work with non-English content:</p> <pre><code># Question and context can be in any language\noutput = agent.generate(\n    question=\"\u00bfCu\u00e1l es la capital de Francia?\",\n    context=\"Responde en espa\u00f1ol\",\n    skillbook=skillbook\n)\n# Output will be in Spanish\n</code></pre>"},{"location":"PROMPTS/#troubleshooting","title":"Troubleshooting","text":""},{"location":"PROMPTS/#high-json-parse-failure-rate","title":"High JSON Parse Failure Rate","text":"<p>Symptom: Frequent <code>RuntimeError: Agent failed to produce valid JSON</code></p> <p>Solutions: 1. Upgrade to v2.1 prompts 2. Increase <code>max_retries</code>: <code>Agent(llm, max_retries=5)</code> 3. Use a more capable model (Claude 3.5 Sonnet, GPT-4 Turbo) 4. Add custom retry prompt if using non-English</p>"},{"location":"PROMPTS/#empty-skill-ids","title":"Empty Skill IDs","text":"<p>Symptom: <code>skill_ids</code> is always <code>[]</code></p> <p>Cause: Skillbook is empty or skills not referenced</p> <p>Solution: <pre><code># Ensure skillbook has skills\nprint(f\"Skillbook has {len(skillbook.skills())} skills\")\n\n# Check skillbook format\nprint(skillbook.as_prompt())\n</code></pre></p>"},{"location":"PROMPTS/#poor-quality-answers","title":"Poor Quality Answers","text":"<p>Symptom: Agent produces generic/unhelpful answers</p> <p>Solutions: 1. Add more specific skills to skillbook 2. Provide richer context 3. Upgrade to v2.1 for better reasoning 4. Increase model temperature for creativity: <code>LiteLLMClient(model=\"...\", temperature=0.7)</code></p>"},{"location":"PROMPTS/#best-practices","title":"Best Practices","text":"<p>\u2705 Do: - Use v2.1 for production systems - Provide rich context in <code>context</code> parameter - Test prompts with your specific domain - Monitor JSON parse success rates</p> <p>\u274c Don't: - Use v2.0 (deprecated) - Hardcode language-specific instructions in templates - Skip testing custom prompts thoroughly - Ignore JSON parse errors silently</p>"},{"location":"PROMPTS/#references","title":"References","text":"<ul> <li>Research Paper: Agentic Context Engineering</li> <li>API Documentation: See <code>ace/roles.py</code> docstrings</li> <li>Examples: <code>examples/</code> directory</li> <li>Changelog: See <code>CHANGELOG.md</code></li> </ul> <p>Questions or Issues?</p> <ul> <li>GitHub Issues: https://github.com/kayba-ai/agentic-context-engine/issues</li> <li>Documentation: https://github.com/kayba-ai/agentic-context-engine#readme</li> </ul>"},{"location":"PROMPT_ENGINEERING/","title":"ACE Prompt Engineering Guide","text":""},{"location":"PROMPT_ENGINEERING/#state-of-the-art-prompt-design-principles","title":"State-of-the-Art Prompt Design Principles","text":"<p>This guide documents best practices for creating and optimizing ACE prompts, based on analysis of 80+ production prompts from leading AI systems (GPT-5, Claude 3.5, and others).</p>"},{"location":"PROMPT_ENGINEERING/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Core Principles</li> <li>Prompt Structure</li> <li>Writing Effective Instructions</li> <li>Domain-Specific Optimizations</li> <li>Common Anti-Patterns</li> <li>Testing and Iteration</li> <li>Migration from v1 to v2</li> </ol>"},{"location":"PROMPT_ENGINEERING/#core-principles","title":"Core Principles","text":""},{"location":"PROMPT_ENGINEERING/#1-identity-headers-are-essential","title":"1. Identity Headers Are Essential","text":"<p>Always start prompts with clear identity and metadata:</p> <pre><code># GOOD - Clear identity with version and capabilities\n\"\"\"\n# Identity and Metadata\nYou are ACE Agent v2.0, an expert problem-solving agent.\nPrompt Version: 2.0.0\nCurrent Date: {current_date}\nMode: Strategic Problem Solving\nConfidence Threshold: 0.7\n\"\"\"\n\n# BAD - Vague introduction\n\"\"\"\nYou are an assistant that helps solve problems.\n\"\"\"\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#2-hierarchical-organization","title":"2. Hierarchical Organization","text":"<p>Structure prompts with clear sections and subsections:</p> <pre><code># GOOD - Hierarchical structure\n\"\"\"\n## Core Responsibilities\n1. Analyze questions using skillbook\n2. Apply relevant strategies\n\n### Strategy Selection Protocol\n- ONLY use skills with confidence &gt; 0.7\n- NEVER apply conflicting strategies\n\n### Output Requirements\nReturn JSON with exact schema...\n\"\"\"\n\n# BAD - Flat structure\n\"\"\"\nAnalyze questions and apply strategies. Use skills with high confidence.\nReturn JSON with your answer.\n\"\"\"\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#3-concrete-examples-over-abstract-principles","title":"3. Concrete Examples Over Abstract Principles","text":"<p>Show exactly what you want with good/bad examples:</p> <pre><code># GOOD - Concrete examples\n\"\"\"\n### Good Example:\n{\n  \"reasoning\": \"1. Breaking down 15 \u00d7 24: This is multiplication. 2. Using decomposition: 15 \u00d7 (20 + 4)...\",\n  \"skill_ids\": [\"skill_023\"],\n  \"final_answer\": \"360\"\n}\n\n### Bad Example (DO NOT DO THIS):\n{\n  \"reasoning\": \"Using the skillbook strategies, the answer is clear.\",\n  \"final_answer\": \"360\"\n}\n\"\"\"\n\n# BAD - Abstract guidance\n\"\"\"\nProvide clear reasoning and cite relevant skills.\n\"\"\"\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#4-emphatic-capitalization-for-critical-requirements","title":"4. Emphatic Capitalization for Critical Requirements","text":"<p>Use MUST/NEVER/ALWAYS only for frequently violated rules:</p> <pre><code># GOOD - Emphatic for critical requirements\n\"\"\"\n**MUST** include step-by-step reasoning\n**NEVER** skip intermediate calculations\n**ALWAYS** cite specific skill IDs\n\"\"\"\n\n# BAD - Overuse dilutes effectiveness\n\"\"\"\nALWAYS be helpful. NEVER make mistakes. MUST think carefully.\n\"\"\"\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#prompt-structure","title":"Prompt Structure","text":""},{"location":"PROMPT_ENGINEERING/#standard-v2-template","title":"Standard v2 Template","text":"<pre><code>TEMPLATE = \"\"\"\n# Identity and Metadata\n[Role, version, capabilities, date]\n\n## Core Mission\n[One-sentence primary objective]\n\n## Input Context\n[Structured presentation of inputs]\n\n## Processing Protocol\n[Numbered steps or decision tree]\n\n## Critical Requirements\n**MUST**: [Absolute requirements]\n**NEVER**: [Explicit prohibitions]\n\n## Output Format\n[Exact schema with examples]\n\n## Error Recovery\n[Fallback procedures]\n\"\"\"\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#decision-trees-for-complex-logic","title":"Decision Trees for Complex Logic","text":"<pre><code># GOOD - Clear conditional logic\n\"\"\"\nExecute in order - use FIRST that applies:\n\n### 1. SUCCESS_CASE_DETECTED\nIF prediction matches ground truth:\n   \u2192 Tag strategies as helpful\n   \u2192 Extract reusable patterns\n\n### 2. CALCULATION_ERROR_DETECTED\nIF mathematical error in reasoning:\n   \u2192 Pinpoint error location\n   \u2192 Identify root cause\n\"\"\"\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#writing-effective-instructions","title":"Writing Effective Instructions","text":""},{"location":"PROMPT_ENGINEERING/#1-specify-anti-patterns-explicitly","title":"1. Specify Anti-Patterns Explicitly","text":"<pre><code># GOOD - Explicit anti-patterns\n\"\"\"\n**NEVER** use these phrases:\n- \"Based on the skillbook\"\n- \"The model was wrong\"\n- \"Should have known better\"\n- \"Obviously incorrect\"\n\"\"\"\n\n# BAD - Vague guidance\n\"\"\"\nAvoid unhelpful phrases.\n\"\"\"\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#2-include-meta-cognitive-instructions","title":"2. Include Meta-Cognitive Instructions","text":"<pre><code># GOOD - Self-assessment thresholds\n\"\"\"\nONLY use strategies if confidence &gt; 0.7\nIf uncertain about approach, state \"low_confidence\" in output\nVerify each reasoning step before proceeding\n\"\"\"\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#3-procedural-workflows","title":"3. Procedural Workflows","text":"<pre><code># GOOD - Numbered procedures\n\"\"\"\n## Solution Process\n1. Classify problem type (arithmetic/algebra/geometry)\n2. Select appropriate method based on classification\n3. Apply method with ALL intermediate steps shown\n4. Verify answer using alternative approach\n5. Format output according to schema\n\"\"\"\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#4-completeness-requirements","title":"4. Completeness Requirements","text":"<pre><code># GOOD - Prevent partial outputs\n\"\"\"\n**MUST** output COMPLETE code even if lengthy\n**NEVER** use \"...\" or \"rest remains the same\"\n**ALWAYS** include ALL import statements\n\"\"\"\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#domain-specific-optimizations","title":"Domain-Specific Optimizations","text":""},{"location":"PROMPT_ENGINEERING/#mathematics-prompts","title":"Mathematics Prompts","text":"<pre><code># Key additions for math domain:\n\"\"\"\n## Mathematical Protocols\n- ALWAYS show intermediate steps\n- VERIFY calculations twice\n- Use standard order of operations (PEMDAS)\n- State units in final answer\n- Round only final result, not intermediate values\n\"\"\"\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#code-generation-prompts","title":"Code Generation Prompts","text":"<pre><code># Key additions for code domain:\n\"\"\"\n## Code Requirements\n- Write COMPLETE, runnable code\n- Include error handling for edge cases\n- Follow language idioms and style guides\n- Add type hints where applicable\n- Include basic test cases\n\"\"\"\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#reasoninganalysis-prompts","title":"Reasoning/Analysis Prompts","text":"<pre><code># Key additions for reasoning:\n\"\"\"\n## Analytical Framework\n- Identify assumptions explicitly\n- Consider multiple perspectives\n- Acknowledge uncertainty ranges\n- Distinguish correlation from causation\n- Cite evidence for claims\n\"\"\"\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#common-anti-patterns","title":"Common Anti-Patterns","text":""},{"location":"PROMPT_ENGINEERING/#1-vague-instructions","title":"1. Vague Instructions","text":"<pre><code># BAD\n\"Be careful with your analysis\"\n\"Think step by step\"\n\"Consider all aspects\"\n\n# GOOD\n\"Verify arithmetic at each step using reverse operations\"\n\"Follow this 5-step analysis procedure: [specific steps]\"\n\"Address these specific aspects: [enumerated list]\"\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#2-over-reliance-on-training","title":"2. Over-Reliance on Training","text":"<pre><code># BAD\n\"Use your knowledge to solve this\"\n\"Apply appropriate methods\"\n\n# GOOD\n\"Use methods from the skillbook section 'algebra'\"\n\"Apply the quadratic formula: x = (-b \u00b1 \u221a(b\u00b2-4ac))/2a\"\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#3-missing-error-recovery","title":"3. Missing Error Recovery","text":"<pre><code># BAD\n\"Return JSON with the answer\"\n\n# GOOD\n\"\"\"\nReturn JSON with exact schema.\nIf JSON generation fails:\n1. Check all required fields present\n2. Escape special characters\n3. Validate number formats\nMaximum retries: 3\n\"\"\"\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#4-ambiguous-output-format","title":"4. Ambiguous Output Format","text":"<pre><code># BAD\n\"Respond with your analysis and conclusion\"\n\n# GOOD\n\"\"\"\nReturn ONLY valid JSON:\n{\n  \"analysis\": \"&lt;numbered points&gt;\",\n  \"conclusion\": \"&lt;one sentence&gt;\",\n  \"confidence\": 0.0-1.0\n}\n\"\"\"\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#testing-and-iteration","title":"Testing and Iteration","text":""},{"location":"PROMPT_ENGINEERING/#1-ab-testing-framework","title":"1. A/B Testing Framework","text":"<pre><code>from ace.prompts_v2_1 import PromptManager\n\n# Test different versions\nmanager = PromptManager()\n\n# Version A - Standard v2.1\nprompt_a = manager.get_agent_prompt(version=\"2.1\")\n\n# Version B - Custom variant\nprompt_b = custom_prompt_with_modifications\n\n# Track performance\nresults_a = run_tests_with_prompt(prompt_a)\nresults_b = run_tests_with_prompt(prompt_b)\n\n# Compare metrics\ncompare_accuracy(results_a, results_b)\ncompare_confidence_calibration(results_a, results_b)\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#2-validation-utilities","title":"2. Validation Utilities","text":"<pre><code>from ace.prompts_v2_1 import validate_prompt_output\n\n# Test output compliance\noutput = llm.generate(prompt)\nis_valid, errors = validate_prompt_output(output, role=\"agent\")\n\nif not is_valid:\n    print(f\"Output validation failed: {errors}\")\n    # Iterate on prompt to fix common failures\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#3-performance-metrics","title":"3. Performance Metrics","text":"<p>Track these metrics to evaluate prompt effectiveness:</p> <ul> <li>Accuracy: Correct answers / total</li> <li>Compliance: Valid JSON outputs / total</li> <li>Confidence Calibration: Correlation between confidence and accuracy</li> <li>Retry Rate: Failed attempts requiring retry</li> <li>Token Efficiency: Average tokens per response</li> </ul>"},{"location":"PROMPT_ENGINEERING/#4-iterative-refinement-process","title":"4. Iterative Refinement Process","text":"<ol> <li>Baseline: Start with v2.1 template</li> <li>Observe: Identify failure patterns</li> <li>Hypothesize: Form specific improvements</li> <li>Test: A/B test modifications</li> <li>Adopt: Integrate successful changes</li> <li>Document: Record what worked and why</li> </ol>"},{"location":"PROMPT_ENGINEERING/#migration-to-v21","title":"Migration to v2.1","text":""},{"location":"PROMPT_ENGINEERING/#quick-start","title":"Quick Start","text":"<pre><code># Old approach (v1)\nfrom ace.prompts import AGENT_PROMPT\nagent = Agent(llm, prompt_template=AGENT_PROMPT)\n\n# Recommended approach (v2.1) - +17% success rate vs v1\nfrom ace.prompts_v2_1 import PromptManager\nmanager = PromptManager(default_version=\"2.1\")\nagent = Agent(llm, prompt_template=manager.get_agent_prompt())\n</code></pre> <p>Note: v2.0 prompts are deprecated. Use v2.1 for best performance.</p>"},{"location":"PROMPT_ENGINEERING/#key-improvements","title":"Key Improvements","text":"Feature v1 v2.1 Performance Baseline +17% success rate Structure Basic sections Hierarchical with metadata Examples None Good/bad examples included Error Handling Basic JSON check Detailed recovery procedures Requirements General guidance MUST/NEVER with specifics Output Loose schema Strict schema with validation Domains One-size-fits-all Specialized variants Anti-patterns Not specified Explicitly prohibited Confidence Not tracked Built-in confidence scores MCP Support No Yes (v2.1 enhancement)"},{"location":"PROMPT_ENGINEERING/#gradual-migration-strategy","title":"Gradual Migration Strategy","text":"<ol> <li>Phase 1: Test v2 prompts with small sample</li> <li>Phase 2: A/B test v1 vs v2 on real tasks</li> <li>Phase 3: Migrate best-performing roles first</li> <li>Phase 4: Customize v2 based on your needs</li> <li>Phase 5: Fully migrate to v2 framework</li> </ol>"},{"location":"PROMPT_ENGINEERING/#best-practices-summary","title":"Best Practices Summary","text":""},{"location":"PROMPT_ENGINEERING/#do","title":"DO:","text":"<ul> <li>\u2705 Start with identity headers and metadata</li> <li>\u2705 Use hierarchical organization with clear sections</li> <li>\u2705 Provide concrete good/bad examples</li> <li>\u2705 Specify exact output schemas</li> <li>\u2705 Include error recovery procedures</li> <li>\u2705 Add domain-specific optimizations</li> <li>\u2705 List explicit anti-patterns to avoid</li> <li>\u2705 Use emphatic caps sparingly for critical rules</li> <li>\u2705 Include meta-cognitive assessment instructions</li> <li>\u2705 Test and iterate based on failure patterns</li> </ul>"},{"location":"PROMPT_ENGINEERING/#dont","title":"DON'T:","text":"<ul> <li>\u274c Write vague, abstract instructions</li> <li>\u274c Rely solely on model training</li> <li>\u274c Overuse emphatic capitalization</li> <li>\u274c Skip error handling</li> <li>\u274c Use ambiguous output formats</li> <li>\u274c Ignore domain-specific needs</li> <li>\u274c Assume one prompt fits all cases</li> <li>\u274c Deploy without validation testing</li> <li>\u274c Mix multiple concerns in one section</li> <li>\u274c Forget to version your prompts</li> </ul>"},{"location":"PROMPT_ENGINEERING/#advanced-techniques","title":"Advanced Techniques","text":""},{"location":"PROMPT_ENGINEERING/#1-prompt-chaining","title":"1. Prompt Chaining","text":"<pre><code># Break complex tasks into stages\nstage1_prompt = manager.get_agent_prompt(domain=\"analysis\")\nstage2_prompt = manager.get_agent_prompt(domain=\"synthesis\")\n\n# Chain outputs\nanalysis = agent_stage1.generate(prompt=stage1_prompt, ...)\nsynthesis = agent_stage2.generate(\n    prompt=stage2_prompt,\n    context=analysis.output,\n    ...\n)\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#2-dynamic-prompt-selection","title":"2. Dynamic Prompt Selection","text":"<pre><code>def select_prompt_by_difficulty(question: str) -&gt; str:\n    \"\"\"Select prompt variant based on problem complexity.\"\"\"\n    difficulty = assess_difficulty(question)\n\n    if difficulty &gt; 0.8:\n        return manager.get_agent_prompt(variant=\"expert\")\n    elif difficulty &gt; 0.5:\n        return manager.get_agent_prompt(variant=\"standard\")\n    else:\n        return manager.get_agent_prompt(variant=\"simple\")\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#3-prompt-versioning","title":"3. Prompt Versioning","text":"<pre><code>class VersionedPromptManager:\n    \"\"\"Track prompt performance across versions.\"\"\"\n\n    def __init__(self):\n        self.versions = {}\n        self.performance = {}\n\n    def register_version(self, version: str, prompt: str):\n        self.versions[version] = prompt\n        self.performance[version] = {\"uses\": 0, \"success\": 0}\n\n    def get_best_performing(self) -&gt; str:\n        \"\"\"Return prompt with highest success rate.\"\"\"\n        best = max(\n            self.performance.items(),\n            key=lambda x: x[1][\"success\"] / max(x[1][\"uses\"], 1)\n        )\n        return self.versions[best[0]]\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#resources","title":"Resources","text":"<ul> <li>Original ACE Paper</li> <li>Prompt Engineering Best Practices</li> <li>LiteLLM Documentation</li> <li>Example Implementations - See examples directory for prompt engineering patterns</li> </ul>"},{"location":"PROMPT_ENGINEERING/#contributing","title":"Contributing","text":"<p>When contributing new prompts:</p> <ol> <li>Follow the v2 template structure</li> <li>Include at least 2 good/bad examples</li> <li>Add domain-specific optimizations if applicable</li> <li>Test with validation utilities</li> <li>Document performance improvements</li> <li>Submit with A/B test results if available</li> </ol> <p>This guide is based on analysis of production prompts from GPT-5, Claude 3.5, Grok, and 80+ other systems. It will be updated as new patterns emerge.</p>"},{"location":"QUICK_START/","title":"ACE Framework Quick Start","text":"<p>Get your first self-learning AI agent running!</p>"},{"location":"QUICK_START/#installation","title":"Installation","text":"<pre><code>pip install ace-framework\n</code></pre> <p>Set your API key:</p> <pre><code>export OPENAI_API_KEY=\"your-key-here\"\n# Or: ANTHROPIC_API_KEY, GOOGLE_API_KEY, etc.\n</code></pre>"},{"location":"QUICK_START/#integration-examples","title":"Integration Examples","text":""},{"location":"QUICK_START/#acelitellm-simple-self-improving-agent","title":"ACELiteLLM - Simple Self-Improving Agent","text":"<pre><code>from ace import ACELiteLLM\n\n# Create self-improving agent\nagent = ACELiteLLM(model=\"gpt-4o-mini\")\n\n# Ask related questions - agent learns patterns\nanswer1 = agent.ask(\"If all cats are animals, is Felix (a cat) an animal?\")\nanswer2 = agent.ask(\"If all birds fly, can penguins (birds) fly?\")  # Learns to check assumptions!\nanswer3 = agent.ask(\"If all metals conduct electricity, does copper conduct electricity?\")\n\n# View learned strategies\nprint(f\"Learned {len(agent.skillbook.skills())} reasoning skills\")\n\n# Save for reuse\nagent.save_skillbook(\"my_agent.json\")\n\n# Load and continue\nagent2 = ACELiteLLM(model=\"gpt-4o-mini\", skillbook_path=\"my_agent.json\")\n</code></pre>"},{"location":"QUICK_START/#acelangchain-wrap-langchain-chainsagents","title":"ACELangChain - Wrap LangChain Chains/Agents","text":"<p>Best for multi-step workflows and tool-using agents.</p> <pre><code>from ace import ACELangChain\n\nace_chain = ACELangChain(runnable=your_langchain_chain)\nresult = ace_chain.invoke({\"question\": \"Your task\"})  # Learns automatically\n</code></pre>"},{"location":"QUICK_START/#aceagent-browser-automation-browser-use","title":"ACEAgent - Browser Automation (browser-use)","text":"<p>Drop-in replacement for <code>browser_use.Agent</code> with automatic learning.</p> <pre><code>pip install ace-framework[browser-use]\n</code></pre> <pre><code>from ace import ACEAgent\nfrom browser_use import ChatBrowserUse\n\n# Two LLMs: ChatBrowserUse for browser, gpt-4o-mini for ACE learning\nagent = ACEAgent(\n    llm=ChatBrowserUse(),      # Browser execution\n    ace_model=\"gpt-4o-mini\"    # ACE learning\n)\n\nawait agent.run(task=\"Find top Hacker News post\")\nagent.save_skillbook(\"hn_expert.json\")\n\n# Reuse learned knowledge\nagent = ACEAgent(llm=ChatBrowserUse(), skillbook_path=\"hn_expert.json\")\nawait agent.run(task=\"New task\")  # Starts smart!\n</code></pre> <p>\u2192 Browser Use Guide</p>"},{"location":"QUICK_START/#aceclaudecode-claude-code-cli","title":"ACEClaudeCode - Claude Code CLI","text":"<p>Self-improving coding agent using Claude Code.</p> <pre><code>from ace import ACEClaudeCode\n\nagent = ACEClaudeCode(\n    working_dir=\"./my_project\",\n    ace_model=\"claude-sonnet-4-5-20250929\"  # Any LiteLLM-supported model works\n)\n\n# Execute coding tasks - agent learns from each\nresult = agent.run(task=\"Add unit tests for utils.py\")\nagent.save_skillbook(\"coding_expert.json\")\n\n# Reuse learned knowledge\nagent = ACEClaudeCode(working_dir=\"./project\", skillbook_path=\"coding_expert.json\")\n</code></pre> <p>\u2192 Claude Code Loop Example</p>"},{"location":"QUICK_START/#advanced-tutorial-understanding-ace-internals","title":"Advanced Tutorial: Understanding ACE Internals","text":"<p>Want to understand how ACE works under the hood? This section shows the full architecture with Agent, Reflector, and SkillManager roles.</p>"},{"location":"QUICK_START/#full-pipeline-example","title":"Full Pipeline Example","text":"<pre><code>from ace import OfflineACE, Agent, Reflector, SkillManager\nfrom ace import LiteLLMClient, Sample, TaskEnvironment, EnvironmentResult\n\n\n# Simple environment that checks if answer contains the ground truth\nclass SimpleEnvironment(TaskEnvironment):\n    def evaluate(self, sample, agent_output):\n        correct = str(sample.ground_truth).lower() in str(agent_output.final_answer).lower()\n        return EnvironmentResult(\n            feedback=\"Correct!\" if correct else \"Incorrect\",\n            ground_truth=sample.ground_truth\n        )\n\n\n# Initialize LLM client\nclient = LiteLLMClient(model=\"gpt-4o-mini\")\n\n# Create ACE components (three roles)\nagent = Agent(client)              # Produces answers\nreflector = Reflector(client)      # Analyzes performance\nskill_manager = SkillManager(client)  # Updates skillbook\n\n# Create adapter to orchestrate everything\nadapter = OfflineACE(agent=agent, reflector=reflector, skill_manager=skill_manager)\n\n# Create training samples\nsamples = [\n    Sample(question=\"What is the capital of France?\", context=\"\", ground_truth=\"Paris\"),\n    Sample(question=\"What is 2 + 2?\", context=\"\", ground_truth=\"4\"),\n    Sample(question=\"Who wrote Romeo and Juliet?\", context=\"\", ground_truth=\"Shakespeare\")\n]\n\n# Train the agent\nprint(\"Training agent...\")\nresults = adapter.run(samples, SimpleEnvironment(), epochs=2)\n\n# Save learned strategies\nadapter.skillbook.save_to_file(\"my_agent.json\")\nprint(f\"\u2705 Agent trained! Learned {len(adapter.skillbook.skills())} strategies\")\n\n# Test with new question\ntest_output = agent.generate(\n    question=\"What is 5 + 3?\",\n    context=\"\",\n    skillbook=adapter.skillbook\n)\nprint(f\"\\nTest question: What is 5 + 3?\")\nprint(f\"Answer: {test_output.final_answer}\")\n</code></pre> <p>Expected output: <pre><code>Training agent...\n\u2705 Agent trained! Learned 3 strategies\n\nTest question: What is 5 + 3?\nAnswer: 8\n</code></pre></p>"},{"location":"QUICK_START/#understanding-the-architecture","title":"Understanding the Architecture","text":"<p>Three ACE Roles: 1. Agent - Executes tasks using skillbook strategies 2. Reflector - Analyzes what worked/didn't work 3. SkillManager - Updates skillbook with new strategies</p> <p>Two Adaptation Modes: - OfflineACE - Train on batch of samples (shown above) - OnlineACE - Learn from each task in real-time</p>"},{"location":"QUICK_START/#next-steps","title":"Next Steps","text":""},{"location":"QUICK_START/#load-saved-agent","title":"Load Saved Agent","text":"<pre><code>from ace import ACELiteLLM\n\n# Load previously trained agent\nagent = ACELiteLLM(model=\"gpt-4o-mini\", skillbook_path=\"my_agent.json\")\n\n# Use it immediately\nanswer = agent.ask(\"New question\")\n</code></pre> <p>Or with full pipeline:</p> <pre><code>from ace import Skillbook, Agent, LiteLLMClient\n\n# Load skillbook\nskillbook = Skillbook.load_from_file(\"my_agent.json\")\n\n# Use with agent\nclient = LiteLLMClient(model=\"gpt-4o-mini\")\nagent = Agent(client)\noutput = agent.generate(\n    question=\"New question\",\n    context=\"\",\n    skillbook=skillbook\n)\n</code></pre>"},{"location":"QUICK_START/#try-different-models","title":"Try Different Models","text":"<pre><code># Anthropic Claude\nagent = ACELiteLLM(model=\"claude-3-5-sonnet-20241022\")\n\n# Google Gemini\nagent = ACELiteLLM(model=\"gemini-pro\")\n\n# Local Ollama\nagent = ACELiteLLM(model=\"ollama/llama2\")\n</code></pre>"},{"location":"QUICK_START/#common-patterns","title":"Common Patterns","text":""},{"location":"QUICK_START/#online-learning-learn-while-running","title":"Online Learning (Learn While Running)","text":"<pre><code>from ace import OnlineACE\n\nadapter = OnlineACE(\n    skillbook=skillbook,\n    agent=agent,\n    reflector=reflector,\n    skill_manager=skill_manager\n)\n\n# Process tasks one by one, learning from each\nfor task in tasks:\n    result = adapter.process(task, environment)\n</code></pre>"},{"location":"QUICK_START/#custom-evaluation","title":"Custom Evaluation","text":"<pre><code>class MathEnvironment(TaskEnvironment):\n    def evaluate(self, sample, output):\n        try:\n            result = eval(output.final_answer)\n            correct = result == sample.ground_truth\n            return EnvironmentResult(\n                feedback=f\"Result: {result}. {'\u2713' if correct else '\u2717'}\",\n                ground_truth=sample.ground_truth\n            )\n        except:\n            return EnvironmentResult(\n                feedback=\"Invalid math expression\",\n                ground_truth=sample.ground_truth\n            )\n</code></pre>"},{"location":"QUICK_START/#learn-more","title":"Learn More","text":"<ul> <li>Integration Guide - Add ACE to existing agents</li> <li>Complete Guide - Deep dive into ACE concepts</li> <li>Examples - Real-world examples</li> <li>Browser Automation - Self-improving browser agents</li> <li>LangChain Integration - Wrap chains with learning</li> <li>Custom Integration - Any agent pattern</li> </ul>"},{"location":"QUICK_START/#troubleshooting","title":"Troubleshooting","text":"<p>Import errors? <pre><code>pip install --upgrade ace-framework\n</code></pre></p> <p>API key not working? - Verify key is correct: <code>echo $OPENAI_API_KEY</code> - Try different model: <code>ACELiteLLM(model=\"gpt-3.5-turbo\")</code></p> <p>Need help? - GitHub Issues - Discord Community</p> <p>Ready to build production agents? Check out the Integration Guide for browser automation, LangChain, and custom agent patterns.</p>"},{"location":"SETUP_GUIDE/","title":"\u2699\ufe0f ACE Framework Setup Guide","text":"<p>Quick setup and configuration guide for ACE Framework.</p>"},{"location":"SETUP_GUIDE/#requirements","title":"Requirements","text":"<ul> <li>Python 3.12</li> <li>API key for your LLM provider (OpenAI, Anthropic, Google, etc.)</li> </ul> <p>Check Python version: <pre><code>python --version  # Should show 3.12\n</code></pre></p>"},{"location":"SETUP_GUIDE/#installation","title":"Installation","text":""},{"location":"SETUP_GUIDE/#for-users","title":"For Users","text":"<pre><code># Basic installation\npip install ace-framework\n\n# With optional features\npip install ace-framework[observability]  # Opik monitoring + cost tracking\npip install ace-framework[browser-use]    # Browser automation\npip install ace-framework[langchain]      # LangChain integration\npip install ace-framework[all]            # All features\n</code></pre>"},{"location":"SETUP_GUIDE/#for-contributors","title":"For Contributors","text":"<pre><code>git clone https://github.com/kayba-ai/agentic-context-engine\ncd agentic-context-engine\nuv sync  # Installs everything automatically (10-100x faster than pip)\n</code></pre>"},{"location":"SETUP_GUIDE/#api-key-setup","title":"API Key Setup","text":""},{"location":"SETUP_GUIDE/#option-1-environment-variable-recommended","title":"Option 1: Environment Variable (Recommended)","text":"<pre><code># Set in your shell\nexport OPENAI_API_KEY=\"sk-...\"\n\n# Or create .env file\necho \"OPENAI_API_KEY=sk-...\" &gt; .env\n</code></pre> <p>Load in Python: <pre><code>from dotenv import load_dotenv\nload_dotenv()  # Loads from .env file\n</code></pre></p>"},{"location":"SETUP_GUIDE/#option-2-direct-in-code","title":"Option 2: Direct in Code","text":"<pre><code>from ace import LiteLLMClient\n\nclient = LiteLLMClient(\n    model=\"gpt-4o-mini\",\n    api_key=\"your-key-here\"  # Not recommended for production\n)\n</code></pre>"},{"location":"SETUP_GUIDE/#provider-examples","title":"Provider Examples","text":""},{"location":"SETUP_GUIDE/#openai","title":"OpenAI","text":"<ol> <li>Get API key: platform.openai.com</li> <li>Set key: <code>export OPENAI_API_KEY=\"sk-...\"</code></li> <li>Use it: <pre><code>from ace import LiteLLMClient\nclient = LiteLLMClient(model=\"gpt-4o-mini\")\n</code></pre></li> </ol>"},{"location":"SETUP_GUIDE/#anthropic-claude","title":"Anthropic Claude","text":"<ol> <li>Get API key: console.anthropic.com</li> <li>Set key: <code>export ANTHROPIC_API_KEY=\"sk-ant-...\"</code></li> <li>Use it: <pre><code>client = LiteLLMClient(model=\"claude-3-5-sonnet-20241022\")\n</code></pre></li> </ol>"},{"location":"SETUP_GUIDE/#google-gemini","title":"Google Gemini","text":"<ol> <li>Get API key: makersuite.google.com</li> <li>Set key: <code>export GOOGLE_API_KEY=\"AIza...\"</code></li> <li>Use it: <pre><code>client = LiteLLMClient(model=\"gemini-pro\")\n</code></pre></li> </ol>"},{"location":"SETUP_GUIDE/#local-models-ollama","title":"Local Models (Ollama)","text":"<ol> <li>Install Ollama: ollama.ai</li> <li>Pull model: <code>ollama pull llama2</code></li> <li>Use it: <pre><code>client = LiteLLMClient(model=\"ollama/llama2\")\n</code></pre></li> </ol> <p>Supported Providers: 100+ via LiteLLM (AWS Bedrock, Azure, Cohere, Hugging Face, etc.)</p>"},{"location":"SETUP_GUIDE/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"SETUP_GUIDE/#custom-llm-parameters","title":"Custom LLM Parameters","text":"<pre><code>from ace import LiteLLMClient\n\nclient = LiteLLMClient(\n    model=\"gpt-4o-mini\",\n    temperature=0.7,\n    max_tokens=2048,\n    timeout=60  # seconds\n)\n</code></pre>"},{"location":"SETUP_GUIDE/#production-monitoring-opik","title":"Production Monitoring (Opik)","text":"<pre><code>pip install ace-framework[observability]\n</code></pre> <p>Opik automatically tracks: - Token usage per LLM call - Cost per operation - Agent/Reflector/SkillManager performance - Skillbook evolution over time</p> <p>View dashboard: comet.com/opik</p>"},{"location":"SETUP_GUIDE/#skillbook-storage","title":"Skillbook Storage","text":"<pre><code>from ace import Skillbook\n\n# Save skillbook\nskillbook.save_to_file(\"my_skillbook.json\")\n\n# Load skillbook\nskillbook = Skillbook.load_from_file(\"my_skillbook.json\")\n\n# For production: Use database storage\n# PostgreSQL, SQLite, or vector stores supported\n</code></pre>"},{"location":"SETUP_GUIDE/#checkpoint-saving","title":"Checkpoint Saving","text":"<pre><code>from ace import OfflineACE\n\nadapter = OfflineACE(\n    skillbook=skillbook,\n    agent=agent,\n    reflector=reflector,\n    skill_manager=skill_manager\n)\n\n# Save skillbook every 10 samples during training\nresults = adapter.run(\n    samples,\n    environment,\n    checkpoint_interval=10,\n    checkpoint_dir=\"./checkpoints\"\n)\n</code></pre>"},{"location":"SETUP_GUIDE/#troubleshooting","title":"Troubleshooting","text":""},{"location":"SETUP_GUIDE/#import-errors","title":"Import Errors","text":"<pre><code># Upgrade to latest version\npip install --upgrade ace-framework\n\n# Check installation\npip show ace-framework\n</code></pre>"},{"location":"SETUP_GUIDE/#api-key-not-working","title":"API Key Not Working","text":"<pre><code># Verify key is set\necho $OPENAI_API_KEY\n\n# Test different model\nfrom ace import LiteLLMClient\nclient = LiteLLMClient(model=\"gpt-3.5-turbo\")  # Cheaper for testing\n</code></pre>"},{"location":"SETUP_GUIDE/#rate-limits","title":"Rate Limits","text":"<pre><code>from ace import LiteLLMClient\n\n# Add delays between calls\nimport time\ntime.sleep(1)  # 1 second between calls\n\n# Or use a cheaper/faster model\nclient = LiteLLMClient(model=\"gpt-3.5-turbo\")\n</code></pre>"},{"location":"SETUP_GUIDE/#json-parse-failures","title":"JSON Parse Failures","text":"<pre><code># Increase max_tokens for SkillManager/Reflector\nfrom ace import SkillManager, Reflector\n\nllm = LiteLLMClient(model=\"gpt-4o-mini\", max_tokens=2048)  # Higher limit\nskill_manager = SkillManager(llm)\nreflector = Reflector(llm)\n</code></pre>"},{"location":"SETUP_GUIDE/#need-more-help","title":"Need More Help?","text":"<ul> <li>GitHub Issues: github.com/kayba-ai/agentic-context-engine/issues</li> <li>Discord Community: discord.gg/mqCqH7sTyK</li> <li>Documentation: Complete Guide, Quick Start, Integration Guide</li> </ul> <p>Next Steps: Check out the Quick Start Guide to build your first self-learning agent!</p>"},{"location":"TESTING_GUIDE/","title":"\ud83e\uddea ACE Framework Testing Guide","text":"<p>Complete guide for testing ACE agents and validating performance.</p>"},{"location":"TESTING_GUIDE/#testing-philosophy","title":"Testing Philosophy","text":"<p>ACE testing focuses on three key areas: 1. Correctness: Does the agent produce accurate answers? 2. Learning: Does the skillbook improve over time? 3. Robustness: Does the system handle edge cases?</p>"},{"location":"TESTING_GUIDE/#running-tests","title":"Running Tests","text":""},{"location":"TESTING_GUIDE/#quick-start","title":"Quick Start","text":"<pre><code># Run all tests\nuv run pytest\n\n# Run with verbose output\nuv run pytest -v\n\n# Run specific test file\nuv run pytest tests/test_adaptation.py\n\n# Run integration tests\nuv run pytest tests/test_integration.py\n</code></pre>"},{"location":"TESTING_GUIDE/#using-unittest","title":"Using unittest","text":"<pre><code># Run all tests\npython -m unittest discover -s tests\n\n# Run specific test file\npython -m unittest tests.test_adaptation\n\n# Verbose output\npython -m unittest discover -s tests -v\n</code></pre>"},{"location":"TESTING_GUIDE/#unit-testing","title":"Unit Testing","text":""},{"location":"TESTING_GUIDE/#testing-skillbook-operations","title":"Testing Skillbook Operations","text":"<pre><code>import unittest\nfrom ace import Skillbook\n\nclass TestSkillbook(unittest.TestCase):\n    def test_add_and_retrieve_skill(self):\n        skillbook = Skillbook()\n\n        skill = skillbook.add_skill(\n            section=\"Test\",\n            content=\"Test strategy\"\n        )\n\n        retrieved = skillbook.get_skill(skill.id)\n        self.assertEqual(retrieved.content, \"Test strategy\")\n\n    def test_save_and_load(self):\n        skillbook = Skillbook()\n        skillbook.add_skill(\"Section\", \"Content\")\n\n        skillbook.save_to_file(\"test.json\")\n        loaded = Skillbook.load_from_file(\"test.json\")\n\n        self.assertEqual(len(loaded.skills()), 1)\n</code></pre>"},{"location":"TESTING_GUIDE/#testing-agent","title":"Testing Agent","text":"<pre><code>from ace import Agent, DummyLLMClient, Skillbook\n\nclass TestAgent(unittest.TestCase):\n    def setUp(self):\n        self.client = DummyLLMClient()\n        self.agent = Agent(self.client)\n        self.skillbook = Skillbook()\n\n    def test_generate_with_empty_skillbook(self):\n        output = self.agent.generate(\n            question=\"What is 2+2?\",\n            context=\"\",\n            skillbook=self.skillbook\n        )\n\n        self.assertIsNotNone(output.final_answer)\n        self.assertIsNotNone(output.reasoning)\n\n    def test_generate_uses_skillbook(self):\n        skill = self.skillbook.add_skill(\n            section=\"Math\",\n            content=\"Show step-by-step work\"\n        )\n\n        output = self.agent.generate(\n            question=\"What is 10*5?\",\n            context=\"\",\n            skillbook=self.skillbook\n        )\n\n        # Agent should cite the skill\n        self.assertIn(skill.id, output.skill_ids)\n</code></pre>"},{"location":"TESTING_GUIDE/#testing-reflector-skillmanager","title":"Testing Reflector &amp; SkillManager","text":"<pre><code>from ace import Reflector, SkillManager, AgentOutput\n\nclass TestReflectorSkillManager(unittest.TestCase):\n    def setUp(self):\n        self.client = DummyLLMClient()\n        self.reflector = Reflector(self.client)\n        self.skill_manager = SkillManager(self.client)\n        self.skillbook = Skillbook()\n\n    def test_reflection(self):\n        agent_output = AgentOutput(\n            reasoning=\"Solved math problem\",\n            final_answer=\"4\",\n            skill_ids=[],\n            raw={}\n        )\n\n        reflection = self.reflector.reflect(\n            question=\"What is 2+2?\",\n            agent_output=agent_output,\n            skillbook=self.skillbook,\n            feedback=\"Correct answer\"\n        )\n\n        self.assertIsNotNone(reflection.reasoning)\n\n    def test_update_skills(self):\n        reflection = self.reflector.reflect(...)\n\n        skill_manager_output = self.skill_manager.update_skills(\n            reflection=reflection,\n            skillbook=self.skillbook,\n            question_context=\"Math problem\",\n            progress=\"Task 1/10\"\n        )\n\n        self.assertIsNotNone(skill_manager_output.update)\n</code></pre>"},{"location":"TESTING_GUIDE/#integration-testing","title":"Integration Testing","text":""},{"location":"TESTING_GUIDE/#end-to-end-learning-cycle","title":"End-to-End Learning Cycle","text":"<pre><code>from ace import OfflineACE, Sample, TaskEnvironment, EnvironmentResult\n\nclass SimpleEnvironment(TaskEnvironment):\n    def evaluate(self, sample, output):\n        correct = sample.ground_truth in output.final_answer\n        return EnvironmentResult(\n            feedback=\"Correct\" if correct else \"Wrong\",\n            ground_truth=sample.ground_truth\n        )\n\nclass TestLearningCycle(unittest.TestCase):\n    def test_offline_adaptation(self):\n        # Setup\n        client = DummyLLMClient()\n        agent = Agent(client)\n        reflector = Reflector(client)\n        skill_manager = SkillManager(client)\n        adapter = OfflineACE(\n            agent=agent,\n            reflector=reflector,\n            skill_manager=skill_manager\n        )\n\n        # Training samples\n        samples = [\n            Sample(\"What is 2+2?\", \"\", \"4\"),\n            Sample(\"What is 3+3?\", \"\", \"6\")\n        ]\n\n        # Run adaptation\n        results = adapter.run(samples, SimpleEnvironment(), epochs=2)\n\n        # Verify learning occurred\n        self.assertGreater(len(adapter.skillbook.skills()), 0)\n</code></pre>"},{"location":"TESTING_GUIDE/#testing-checkpoints","title":"Testing Checkpoints","text":"<pre><code>def test_checkpoint_saving(self):\n    import tempfile\n    import os\n\n    with tempfile.TemporaryDirectory() as tmpdir:\n        adapter = OfflineACE(\n            agent=agent,\n            reflector=reflector,\n            skill_manager=skill_manager\n        )\n\n        results = adapter.run(\n            samples,\n            environment,\n            checkpoint_interval=2,\n            checkpoint_dir=tmpdir\n        )\n\n        # Verify checkpoints exist\n        checkpoints = os.listdir(tmpdir)\n        self.assertGreater(len(checkpoints), 0)\n        self.assertIn(\"ace_latest.json\", checkpoints)\n</code></pre>"},{"location":"TESTING_GUIDE/#testing-without-api-calls","title":"Testing Without API Calls","text":"<p>Use <code>DummyLLMClient</code> to avoid real API calls during tests:</p> <pre><code>from ace import DummyLLMClient\n\n# Returns predefined responses\nclient = DummyLLMClient()\n\n# Use in tests\nagent = Agent(client)\noutput = agent.generate(question=\"test question\", context=\"\", skillbook=skillbook)\n</code></pre> <p>Benefits: - No API costs - Deterministic test results - Fast execution - No rate limits</p>"},{"location":"TESTING_GUIDE/#performance-testing","title":"Performance Testing","text":""},{"location":"TESTING_GUIDE/#benchmark-learning-speed","title":"Benchmark Learning Speed","text":"<pre><code>import time\n\ndef test_learning_performance(self):\n    start = time.time()\n\n    results = adapter.run(samples, environment, epochs=3)\n\n    duration = time.time() - start\n    avg_per_sample = duration / len(samples)\n\n    print(f\"Processed {len(samples)} samples in {duration:.2f}s\")\n    print(f\"Average: {avg_per_sample:.2f}s per sample\")\n\n    # Assert reasonable performance\n    self.assertLess(avg_per_sample, 5.0)  # Less than 5s per sample\n</code></pre>"},{"location":"TESTING_GUIDE/#measure-skillbook-growth","title":"Measure Skillbook Growth","text":"<pre><code>def test_skillbook_growth(self):\n    initial_skills = len(adapter.skillbook.skills())\n\n    results = adapter.run(samples, environment)\n\n    final_skills = len(adapter.skillbook.skills())\n    growth = final_skills - initial_skills\n\n    print(f\"Skillbook grew by {growth} skills\")\n\n    # Verify learning occurred\n    self.assertGreater(growth, 0)\n</code></pre>"},{"location":"TESTING_GUIDE/#common-test-patterns","title":"Common Test Patterns","text":""},{"location":"TESTING_GUIDE/#fixture-setup","title":"Fixture Setup","text":"<pre><code>class TestACEComponents(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        \"\"\"Run once for all tests\"\"\"\n        cls.client = DummyLLMClient()\n\n    def setUp(self):\n        \"\"\"Run before each test\"\"\"\n        self.skillbook = Skillbook()\n        self.agent = Agent(self.client)\n        self.reflector = Reflector(self.client)\n        self.skill_manager = SkillManager(self.client)\n\n    def tearDown(self):\n        \"\"\"Run after each test\"\"\"\n        # Clean up temporary files\n        import os\n        if os.path.exists(\"test.json\"):\n            os.remove(\"test.json\")\n</code></pre>"},{"location":"TESTING_GUIDE/#mocking-external-services","title":"Mocking External Services","text":"<pre><code>from unittest.mock import Mock, patch\n\ndef test_with_mock_llm(self):\n    mock_client = Mock()\n    mock_client.complete.return_value = Mock(text='{\"answer\": \"42\"}')\n\n    agent = Agent(mock_client)\n    output = agent.generate(question=\"test\", context=\"\", skillbook=skillbook)\n\n    mock_client.complete.assert_called_once()\n</code></pre>"},{"location":"TESTING_GUIDE/#continuous-integration","title":"Continuous Integration","text":"<p>ACE includes pytest configuration and GitHub Actions workflow.</p>"},{"location":"TESTING_GUIDE/#local-ci-simulation","title":"Local CI Simulation","text":"<pre><code># Run full test suite\nuv run pytest\n\n# With coverage\nuv run pytest --cov=ace --cov-report=html\n\n# Type checking\nuv run mypy ace/\n\n# Code formatting check\nuv run black --check ace/ tests/\n</code></pre>"},{"location":"TESTING_GUIDE/#github-actions","title":"GitHub Actions","text":"<p>Tests run automatically on: - Push to main - Pull requests - Release tags</p> <p>See <code>.github/workflows/test.yml</code> for configuration.</p>"},{"location":"TESTING_GUIDE/#troubleshooting-tests","title":"Troubleshooting Tests","text":""},{"location":"TESTING_GUIDE/#tests-timeout","title":"Tests Timeout","text":"<pre><code># Reduce epochs/samples\nresults = adapter.run(samples[:5], environment, epochs=1)\n\n# Use DummyLLMClient\nclient = DummyLLMClient()  # Instead of real LLM\n</code></pre>"},{"location":"TESTING_GUIDE/#flaky-tests","title":"Flaky Tests","text":"<pre><code># Use deterministic data\nsamples = [Sample(\"2+2\", \"\", \"4\")]  # Not random\n\n# Set random seeds\nimport random\nrandom.seed(42)\n</code></pre>"},{"location":"TESTING_GUIDE/#import-errors","title":"Import Errors","text":"<pre><code># Install test dependencies\nuv sync  # Installs dev dependencies\n\n# Or manually\npip install pytest pytest-cov\n</code></pre>"},{"location":"TESTING_GUIDE/#best-practices","title":"Best Practices","text":"<ol> <li>Use DummyLLMClient for unit tests (fast, no API costs)</li> <li>Test one thing per test (easier to debug failures)</li> <li>Clean up temp files in tearDown()</li> <li>Mock external services (databases, APIs)</li> <li>Set timeouts for long-running tests</li> <li>Run tests frequently during development</li> </ol>"},{"location":"TESTING_GUIDE/#resources","title":"Resources","text":"<ul> <li>Test Suite: <code>tests/</code> directory</li> <li>Integration Tests: <code>tests/test_integration.py</code> (10 comprehensive tests)</li> <li>CI Configuration: <code>.github/workflows/test.yml</code></li> <li>Coverage Reports: Run <code>uv run pytest --cov=ace</code></li> </ul> <p>Need help with testing? Join our Discord or open a GitHub issue.</p>"},{"location":"api/","title":"API Reference","text":"<p>See also</p> <p>The full content for this page will be migrated from API_REFERENCE.md.</p>"},{"location":"getting-started/quick-start/","title":"ACE Framework Quick Start","text":"<p>Get your first self-learning AI agent running!</p>"},{"location":"getting-started/quick-start/#installation","title":"Installation","text":"<pre><code>pip install ace-framework\n</code></pre> <p>Set your API key:</p> <pre><code>export OPENAI_API_KEY=\"your-key-here\"\n# Or: ANTHROPIC_API_KEY, GOOGLE_API_KEY, etc.\n</code></pre>"},{"location":"getting-started/quick-start/#integration-examples","title":"Integration Examples","text":""},{"location":"getting-started/quick-start/#acelitellm-simple-self-improving-agent","title":"ACELiteLLM - Simple Self-Improving Agent","text":"<pre><code>from ace import ACELiteLLM\n\n# Create self-improving agent\nagent = ACELiteLLM(model=\"gpt-4o-mini\")\n\n# Ask related questions - agent learns patterns\nanswer1 = agent.ask(\"If all cats are animals, is Felix (a cat) an animal?\")\nanswer2 = agent.ask(\"If all birds fly, can penguins (birds) fly?\")  # Learns to check assumptions!\nanswer3 = agent.ask(\"If all metals conduct electricity, does copper conduct electricity?\")\n\n# View learned strategies\nprint(f\"Learned {len(agent.skillbook.skills())} reasoning skills\")\n\n# Save for reuse\nagent.save_skillbook(\"my_agent.json\")\n\n# Load and continue\nagent2 = ACELiteLLM(model=\"gpt-4o-mini\", skillbook_path=\"my_agent.json\")\n</code></pre>"},{"location":"getting-started/quick-start/#acelangchain-wrap-langchain-chainsagents","title":"ACELangChain - Wrap LangChain Chains/Agents","text":"<p>Best for multi-step workflows and tool-using agents.</p> <pre><code>from ace import ACELangChain\n\nace_chain = ACELangChain(runnable=your_langchain_chain)\nresult = ace_chain.invoke({\"question\": \"Your task\"})  # Learns automatically\n</code></pre>"},{"location":"getting-started/quick-start/#aceagent-browser-automation-browser-use","title":"ACEAgent - Browser Automation (browser-use)","text":"<p>Drop-in replacement for <code>browser_use.Agent</code> with automatic learning.</p> <pre><code>pip install ace-framework[browser-use]\n</code></pre> <pre><code>from ace import ACEAgent\nfrom browser_use import ChatBrowserUse\n\n# Two LLMs: ChatBrowserUse for browser, gpt-4o-mini for ACE learning\nagent = ACEAgent(\n    llm=ChatBrowserUse(),      # Browser execution\n    ace_model=\"gpt-4o-mini\"    # ACE learning\n)\n\nawait agent.run(task=\"Find top Hacker News post\")\nagent.save_skillbook(\"hn_expert.json\")\n\n# Reuse learned knowledge\nagent = ACEAgent(llm=ChatBrowserUse(), skillbook_path=\"hn_expert.json\")\nawait agent.run(task=\"New task\")  # Starts smart!\n</code></pre>"},{"location":"getting-started/quick-start/#aceclaudecode-claude-code-cli","title":"ACEClaudeCode - Claude Code CLI","text":"<p>Self-improving coding agent using Claude Code.</p> <pre><code>from ace import ACEClaudeCode\n\nagent = ACEClaudeCode(\n    working_dir=\"./my_project\",\n    ace_model=\"claude-sonnet-4-5-20250929\"  # Any LiteLLM-supported model works\n)\n\n# Execute coding tasks - agent learns from each\nresult = agent.run(task=\"Add unit tests for utils.py\")\nagent.save_skillbook(\"coding_expert.json\")\n\n# Reuse learned knowledge\nagent = ACEClaudeCode(working_dir=\"./project\", skillbook_path=\"coding_expert.json\")\n</code></pre>"},{"location":"getting-started/quick-start/#advanced-tutorial-understanding-ace-internals","title":"Advanced Tutorial: Understanding ACE Internals","text":"<p>Want to understand how ACE works under the hood? This section shows the full architecture with Agent, Reflector, and SkillManager roles.</p>"},{"location":"getting-started/quick-start/#full-pipeline-example","title":"Full Pipeline Example","text":"<pre><code>from ace import OfflineACE, Agent, Reflector, SkillManager\nfrom ace import LiteLLMClient, Sample, TaskEnvironment, EnvironmentResult\n\n\n# Simple environment that checks if answer contains the ground truth\nclass SimpleEnvironment(TaskEnvironment):\n    def evaluate(self, sample, agent_output):\n        correct = str(sample.ground_truth).lower() in str(agent_output.final_answer).lower()\n        return EnvironmentResult(\n            feedback=\"Correct!\" if correct else \"Incorrect\",\n            ground_truth=sample.ground_truth\n        )\n\n\n# Initialize LLM client\nclient = LiteLLMClient(model=\"gpt-4o-mini\")\n\n# Create ACE components (three roles)\nagent = Agent(client)              # Produces answers\nreflector = Reflector(client)      # Analyzes performance\nskill_manager = SkillManager(client)  # Updates skillbook\n\n# Create adapter to orchestrate everything\nadapter = OfflineACE(agent=agent, reflector=reflector, skill_manager=skill_manager)\n\n# Create training samples\nsamples = [\n    Sample(question=\"What is the capital of France?\", context=\"\", ground_truth=\"Paris\"),\n    Sample(question=\"What is 2 + 2?\", context=\"\", ground_truth=\"4\"),\n    Sample(question=\"Who wrote Romeo and Juliet?\", context=\"\", ground_truth=\"Shakespeare\")\n]\n\n# Train the agent\nprint(\"Training agent...\")\nresults = adapter.run(samples, SimpleEnvironment(), epochs=2)\n\n# Save learned strategies\nadapter.skillbook.save_to_file(\"my_agent.json\")\nprint(f\"\u2705 Agent trained! Learned {len(adapter.skillbook.skills())} strategies\")\n\n# Test with new question\ntest_output = agent.generate(\n    question=\"What is 5 + 3?\",\n    context=\"\",\n    skillbook=adapter.skillbook\n)\nprint(f\"\\nTest question: What is 5 + 3?\")\nprint(f\"Answer: {test_output.final_answer}\")\n</code></pre> <p>Expected output: <pre><code>Training agent...\n\u2705 Agent trained! Learned 3 strategies\n\nTest question: What is 5 + 3?\nAnswer: 8\n</code></pre></p>"},{"location":"getting-started/quick-start/#understanding-the-architecture","title":"Understanding the Architecture","text":"<p>Three ACE Roles: 1. Agent - Executes tasks using skillbook strategies 2. Reflector - Analyzes what worked/didn't work 3. SkillManager - Updates skillbook with new strategies</p> <p>Two Adaptation Modes: - OfflineACE - Train on batch of samples (shown above) - OnlineACE - Learn from each task in real-time</p>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":""},{"location":"getting-started/quick-start/#load-saved-agent","title":"Load Saved Agent","text":"<pre><code>from ace import ACELiteLLM\n\n# Load previously trained agent\nagent = ACELiteLLM(model=\"gpt-4o-mini\", skillbook_path=\"my_agent.json\")\n\n# Use it immediately\nanswer = agent.ask(\"New question\")\n</code></pre> <p>Or with full pipeline:</p> <pre><code>from ace import Skillbook, Agent, LiteLLMClient\n\n# Load skillbook\nskillbook = Skillbook.load_from_file(\"my_agent.json\")\n\n# Use with agent\nclient = LiteLLMClient(model=\"gpt-4o-mini\")\nagent = Agent(client)\noutput = agent.generate(\n    question=\"New question\",\n    context=\"\",\n    skillbook=skillbook\n)\n</code></pre>"},{"location":"getting-started/quick-start/#try-different-models","title":"Try Different Models","text":"<pre><code># Anthropic Claude\nagent = ACELiteLLM(model=\"claude-3-5-sonnet-20241022\")\n\n# Google Gemini\nagent = ACELiteLLM(model=\"gemini-pro\")\n\n# Local Ollama\nagent = ACELiteLLM(model=\"ollama/llama2\")\n</code></pre>"},{"location":"getting-started/quick-start/#common-patterns","title":"Common Patterns","text":""},{"location":"getting-started/quick-start/#online-learning-learn-while-running","title":"Online Learning (Learn While Running)","text":"<pre><code>from ace import OnlineACE\n\nadapter = OnlineACE(\n    skillbook=skillbook,\n    agent=agent,\n    reflector=reflector,\n    skill_manager=skill_manager\n)\n\n# Process tasks one by one, learning from each\nfor task in tasks:\n    result = adapter.process(task, environment)\n</code></pre>"},{"location":"getting-started/quick-start/#custom-evaluation","title":"Custom Evaluation","text":"<pre><code>class MathEnvironment(TaskEnvironment):\n    def evaluate(self, sample, output):\n        try:\n            result = eval(output.final_answer)\n            correct = result == sample.ground_truth\n            return EnvironmentResult(\n                feedback=f\"Result: {result}. {'\u2713' if correct else '\u2717'}\",\n                ground_truth=sample.ground_truth\n            )\n        except:\n            return EnvironmentResult(\n                feedback=\"Invalid math expression\",\n                ground_truth=sample.ground_truth\n            )\n</code></pre>"},{"location":"getting-started/quick-start/#learn-more","title":"Learn More","text":"<ul> <li>Integration Guide - Add ACE to existing agents</li> <li>Complete Guide - Deep dive into ACE concepts</li> <li>API Reference - Full class and method documentation</li> </ul>"},{"location":"getting-started/quick-start/#troubleshooting","title":"Troubleshooting","text":"<p>Import errors? <pre><code>pip install --upgrade ace-framework\n</code></pre></p> <p>API key not working? - Verify key is correct: <code>echo $OPENAI_API_KEY</code> - Try different model: <code>ACELiteLLM(model=\"gpt-3.5-turbo\")</code></p> <p>Need help? - GitHub Issues - Discord Community</p> <p>Ready to build production agents? Check out the Integration Guide for browser automation, LangChain, and custom agent patterns.</p>"},{"location":"getting-started/setup/","title":"ACE Framework Setup Guide","text":"<p>Quick setup and configuration guide for ACE Framework.</p>"},{"location":"getting-started/setup/#requirements","title":"Requirements","text":"<ul> <li>Python 3.12</li> <li>API key for your LLM provider (OpenAI, Anthropic, Google, etc.)</li> </ul> <p>Check Python version: <pre><code>python --version  # Should show 3.12\n</code></pre></p>"},{"location":"getting-started/setup/#installation","title":"Installation","text":""},{"location":"getting-started/setup/#for-users","title":"For Users","text":"<pre><code># Basic installation\npip install ace-framework\n\n# With optional features\npip install ace-framework[observability]  # Opik monitoring + cost tracking\npip install ace-framework[browser-use]    # Browser automation\npip install ace-framework[langchain]      # LangChain integration\npip install ace-framework[all]            # All features\n</code></pre>"},{"location":"getting-started/setup/#for-contributors","title":"For Contributors","text":"<pre><code>git clone https://github.com/kayba-ai/agentic-context-engine\ncd agentic-context-engine\nuv sync  # Installs everything automatically (10-100x faster than pip)\n</code></pre>"},{"location":"getting-started/setup/#api-key-setup","title":"API Key Setup","text":""},{"location":"getting-started/setup/#option-1-environment-variable-recommended","title":"Option 1: Environment Variable (Recommended)","text":"<pre><code># Set in your shell\nexport OPENAI_API_KEY=\"sk-...\"\n\n# Or create .env file\necho \"OPENAI_API_KEY=sk-...\" &gt; .env\n</code></pre> <p>Load in Python: <pre><code>from dotenv import load_dotenv\nload_dotenv()  # Loads from .env file\n</code></pre></p>"},{"location":"getting-started/setup/#option-2-direct-in-code","title":"Option 2: Direct in Code","text":"<pre><code>from ace import LiteLLMClient\n\nclient = LiteLLMClient(\n    model=\"gpt-4o-mini\",\n    api_key=\"your-key-here\"  # Not recommended for production\n)\n</code></pre>"},{"location":"getting-started/setup/#provider-examples","title":"Provider Examples","text":""},{"location":"getting-started/setup/#openai","title":"OpenAI","text":"<ol> <li>Get API key: platform.openai.com</li> <li>Set key: <code>export OPENAI_API_KEY=\"sk-...\"</code></li> <li>Use it: <pre><code>from ace import LiteLLMClient\nclient = LiteLLMClient(model=\"gpt-4o-mini\")\n</code></pre></li> </ol>"},{"location":"getting-started/setup/#anthropic-claude","title":"Anthropic Claude","text":"<ol> <li>Get API key: console.anthropic.com</li> <li>Set key: <code>export ANTHROPIC_API_KEY=\"sk-ant-...\"</code></li> <li>Use it: <pre><code>client = LiteLLMClient(model=\"claude-3-5-sonnet-20241022\")\n</code></pre></li> </ol>"},{"location":"getting-started/setup/#google-gemini","title":"Google Gemini","text":"<ol> <li>Get API key: makersuite.google.com</li> <li>Set key: <code>export GOOGLE_API_KEY=\"AIza...\"</code></li> <li>Use it: <pre><code>client = LiteLLMClient(model=\"gemini-pro\")\n</code></pre></li> </ol>"},{"location":"getting-started/setup/#local-models-ollama","title":"Local Models (Ollama)","text":"<ol> <li>Install Ollama: ollama.ai</li> <li>Pull model: <code>ollama pull llama2</code></li> <li>Use it: <pre><code>client = LiteLLMClient(model=\"ollama/llama2\")\n</code></pre></li> </ol> <p>Supported Providers: 100+ via LiteLLM (AWS Bedrock, Azure, Cohere, Hugging Face, etc.)</p>"},{"location":"getting-started/setup/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"getting-started/setup/#custom-llm-parameters","title":"Custom LLM Parameters","text":"<pre><code>from ace import LiteLLMClient\n\nclient = LiteLLMClient(\n    model=\"gpt-4o-mini\",\n    temperature=0.7,\n    max_tokens=2048,\n    timeout=60  # seconds\n)\n</code></pre>"},{"location":"getting-started/setup/#production-monitoring-opik","title":"Production Monitoring (Opik)","text":"<pre><code>pip install ace-framework[observability]\n</code></pre> <p>Opik automatically tracks: - Token usage per LLM call - Cost per operation - Agent/Reflector/SkillManager performance - Skillbook evolution over time</p> <p>View dashboard: comet.com/opik</p>"},{"location":"getting-started/setup/#skillbook-storage","title":"Skillbook Storage","text":"<pre><code>from ace import Skillbook\n\n# Save skillbook\nskillbook.save_to_file(\"my_skillbook.json\")\n\n# Load skillbook\nskillbook = Skillbook.load_from_file(\"my_skillbook.json\")\n\n# For production: Use database storage\n# PostgreSQL, SQLite, or vector stores supported\n</code></pre>"},{"location":"getting-started/setup/#checkpoint-saving","title":"Checkpoint Saving","text":"<pre><code>from ace import OfflineACE\n\nadapter = OfflineACE(\n    skillbook=skillbook,\n    agent=agent,\n    reflector=reflector,\n    skill_manager=skill_manager\n)\n\n# Save skillbook every 10 samples during training\nresults = adapter.run(\n    samples,\n    environment,\n    checkpoint_interval=10,\n    checkpoint_dir=\"./checkpoints\"\n)\n</code></pre>"},{"location":"getting-started/setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/setup/#import-errors","title":"Import Errors","text":"<pre><code># Upgrade to latest version\npip install --upgrade ace-framework\n\n# Check installation\npip show ace-framework\n</code></pre>"},{"location":"getting-started/setup/#api-key-not-working","title":"API Key Not Working","text":"<pre><code># Verify key is set\necho $OPENAI_API_KEY\n\n# Test different model\nfrom ace import LiteLLMClient\nclient = LiteLLMClient(model=\"gpt-3.5-turbo\")  # Cheaper for testing\n</code></pre>"},{"location":"getting-started/setup/#rate-limits","title":"Rate Limits","text":"<pre><code>from ace import LiteLLMClient\n\n# Add delays between calls\nimport time\ntime.sleep(1)  # 1 second between calls\n\n# Or use a cheaper/faster model\nclient = LiteLLMClient(model=\"gpt-3.5-turbo\")\n</code></pre>"},{"location":"getting-started/setup/#json-parse-failures","title":"JSON Parse Failures","text":"<pre><code># Increase max_tokens for SkillManager/Reflector\nfrom ace import SkillManager, Reflector\n\nllm = LiteLLMClient(model=\"gpt-4o-mini\", max_tokens=2048)  # Higher limit\nskill_manager = SkillManager(llm)\nreflector = Reflector(llm)\n</code></pre>"},{"location":"getting-started/setup/#need-more-help","title":"Need More Help?","text":"<ul> <li>GitHub Issues: github.com/kayba-ai/agentic-context-engine/issues</li> <li>Discord Community: discord.gg/mqCqH7sTyK</li> <li>Documentation: Complete Guide, Quick Start, Integration Guide</li> </ul> <p>Next Steps: Check out the Quick Start Guide to build your first self-learning agent!</p>"},{"location":"guides/complete-guide/","title":"Complete ACE Guide","text":"<p>See also</p> <p>The full content for this page will be migrated from COMPLETE_GUIDE_TO_ACE.md.</p>"},{"location":"guides/integration/","title":"Integration Guide","text":"<p>See also</p> <p>The full content for this page will be migrated from INTEGRATION_GUIDE.md.</p>"},{"location":"guides/prompts/","title":"Prompt Engineering","text":"<p>See also</p> <p>The full content for this page will be migrated from PROMPTS.md and PROMPT_ENGINEERING.md.</p>"},{"location":"guides/testing/","title":"Testing","text":"<p>See also</p> <p>The full content for this page will be migrated from TESTING_GUIDE.md.</p>"},{"location":"integrations/opik/","title":"Opik Observability","text":"<p>See also</p> <p>The full content for this page will be migrated from OPIK.md.</p>"}]}