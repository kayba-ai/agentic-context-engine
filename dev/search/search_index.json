{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"ACE Framework","text":"<p>Agentic Context Engineering \u2014 a framework for self-improving language model agents.</p> <p>ACE enables AI agents to learn from their own execution feedback through three collaborative roles: Agent, Reflector, and SkillManager. Learned strategies accumulate in a Skillbook that makes every subsequent call smarter.</p>"},{"location":"#the-learning-loop","title":"The Learning Loop","text":"<pre><code>graph LR\n    S[Sample] --&gt; A[Agent]\n    A --&gt; E[Environment]\n    E --&gt;|feedback| R[Reflector]\n    R --&gt;|analyzes| SM[SkillManager]\n    SM --&gt;|updates| SK[Skillbook]\n    SK -.-&gt;|context| A</code></pre> <p>Each pass through the loop discovers new strategies, reinforces what works, and prunes what doesn't.</p>"},{"location":"#get-started-in-30-seconds","title":"Get Started in 30 Seconds","text":"<pre><code>from ace_next import ACELiteLLM\n\nagent = ACELiteLLM.from_model(\"gpt-4o-mini\")\n\nanswer = agent.ask(\"If all cats are animals, is Felix (a cat) an animal?\")\n\nagent.save(\"learned.json\")\n</code></pre>"},{"location":"#install","title":"Install","text":"<pre><code>pip install ace-framework\n</code></pre>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li> <p>Getting Started</p> <p>Install the framework and run your first self-improving agent.</p> <p> Installation  Quick Start</p> </li> <li> <p>Concepts</p> <p>Understand the Skillbook, Roles, Insight Levels, and Update Operations.</p> <p> Overview</p> </li> <li> <p>Guides</p> <p>Build full pipelines, integrate with existing agents, tune prompts.</p> <p> Full Pipeline  Integration Guide</p> </li> <li> <p>Integrations</p> <p>Ready-made runners for LiteLLM, LangChain, browser-use, and Claude Code.</p> <p> Integrations Overview</p> </li> </ul>"},{"location":"#available-runners","title":"Available Runners","text":"Runner Framework Use Case <code>ACELiteLLM</code> LiteLLM (100+ providers) Simple self-improving agent <code>LangChain</code> LangChain Runnables Wrap chains/agents with learning <code>BrowserUse</code> browser-use Browser automation with learning <code>ClaudeCode</code> Claude Code CLI Coding tasks with learning <code>ACE</code> Full pipeline Agent + Reflector + SkillManager"},{"location":"#paper","title":"Paper","text":"<p>This framework implements the method from:</p> <p>Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models arXiv:2510.04618</p>"},{"location":"ACE2_PIPELINE%28old%29/","title":"ACE2 Pipeline Architecture","text":"<p>ACE2 reimplements the ACE core on top of the generic pipeline engine (<code>pipeline/</code>). This document records the design choices, file layout, and migration path.</p>"},{"location":"ACE2_PIPELINE%28old%29/#why-ace2","title":"Why ACE2?","text":"<p>The original ACE implementation (<code>ace/adaptation.py</code>) hard-codes the four-step loop (Agent \u2192 Evaluate \u2192 Reflect \u2192 Update) inside monolithic <code>OfflineACE</code> / <code>OnlineACE</code> classes. This makes it difficult to:</p> <ul> <li>Swap, reorder, or skip steps</li> <li>Run steps in parallel or in background threads</li> <li>Share the pipeline engine with non-ACE workloads</li> <li>Test individual steps in isolation</li> </ul> <p>ACE2 decomposes the same logic into discrete steps that plug into the generic pipeline engine, giving us all of the above for free.</p>"},{"location":"ACE2_PIPELINE%28old%29/#file-layout","title":"File Layout","text":"<pre><code>ace2/\n\u251c\u2500\u2500 __init__.py\n\u251c\u2500\u2500 steps/\n\u2502   \u251c\u2500\u2500 __init__.py          # re-exports all steps\n\u2502   \u251c\u2500\u2500 agent.py             # AgentStep\n\u2502   \u251c\u2500\u2500 evaluate.py          # EvaluateStep\n\u2502   \u251c\u2500\u2500 reflect.py           # ReflectStep\n\u2502   \u2514\u2500\u2500 update.py            # UpdateStep\n\u2514\u2500\u2500 pipelines/\n    \u251c\u2500\u2500 __init__.py           # ace_pipeline() factory + re-exports\n    \u251c\u2500\u2500 offline.py            # OfflineACE runner\n    \u2514\u2500\u2500 online.py             # OnlineACE runner\n</code></pre>"},{"location":"ACE2_PIPELINE%28old%29/#the-four-steps","title":"The Four Steps","text":"<p>Each step implements the <code>StepProtocol</code> (requires/provides/<code>__call__</code>). The pipeline engine validates that every step's <code>requires</code> are satisfied by earlier steps or by the initial context seeded by the runner.</p> Step Requires Provides Notes AgentStep <code>sample</code>, <code>skillbook</code> <code>agent_output</code> Wraps <code>Agent.generate()</code>. Reads <code>recent_reflections</code> from context but does NOT declare it as a requirement (see below). EvaluateStep <code>sample</code>, <code>agent_output</code>, <code>environment</code> <code>environment_result</code> Stateless \u2014 no constructor args. Calls <code>environment.evaluate()</code>. ReflectStep <code>sample</code>, <code>agent_output</code>, <code>environment_result</code>, <code>skillbook</code> <code>reflection</code>, <code>recent_reflections</code> <code>async_boundary=True</code>, <code>max_workers=3</code>. Tags skills and maintains rolling reflection window. UpdateStep <code>reflection</code>, <code>skillbook</code>, <code>sample</code>, <code>environment_result</code>, <code>agent_output</code> <code>skill_manager_output</code> <code>max_workers=1</code> to serialise skillbook writes. Attaches insight-source provenance."},{"location":"ACE2_PIPELINE%28old%29/#pipeline-wiring","title":"Pipeline Wiring","text":"<pre><code>Pipeline()\n    .then(AgentStep(agent))\n    .then(EvaluateStep())\n    .then(ReflectStep(reflector, reflection_window=3))\n    .then(UpdateStep(skill_manager))\n</code></pre>"},{"location":"ACE2_PIPELINE%28old%29/#design-decisions","title":"Design Decisions","text":""},{"location":"ACE2_PIPELINE%28old%29/#composition-over-inheritance","title":"Composition over Inheritance","text":"<p>The runners (<code>OfflineACE</code>, <code>OnlineACE</code>) hold a <code>Pipeline</code> by composition \u2014 they are not subclasses of <code>Pipeline</code>. This matches the spec and keeps the runner logic (epoch loops, checkpointing, error capture) separate from the pipeline engine's step-chaining logic.</p>"},{"location":"ACE2_PIPELINE%28old%29/#recent_reflections-is-cross-sample-state","title":"<code>recent_reflections</code> is Cross-Sample State","text":"<p><code>recent_reflections</code> is a rolling window of serialised reflections that carries across samples. It is:</p> <ul> <li>Seeded by the runner (starts as empty tuple <code>()</code>)</li> <li>Grown by <code>ReflectStep</code> (appends new reflection, trims to window size)</li> <li>Propagated by the runner between samples (<code>recent_reflections = out_ctx.recent_reflections</code>)</li> <li>Read by <code>AgentStep</code> to build the reflection context string</li> </ul> <p>Critically, <code>AgentStep</code> does not declare <code>recent_reflections</code> in its <code>requires</code> set. If it did, the pipeline validator would see a circular dependency (AgentStep requires something that ReflectStep provides, but ReflectStep comes after AgentStep). Instead, the runner guarantees it is always present in the initial <code>StepContext</code>.</p>"},{"location":"ACE2_PIPELINE%28old%29/#async-boundaries-and-worker-limits","title":"Async Boundaries and Worker Limits","text":"<ul> <li><code>ReflectStep</code> sets <code>async_boundary = True</code> \u2014 the pipeline engine can run it   (and everything after it) in a background thread pool, so the Agent + Evaluate   foreground path returns quickly.</li> <li><code>ReflectStep</code> uses <code>max_workers = 3</code> \u2014 multiple reflections can run concurrently.</li> <li><code>UpdateStep</code> uses <code>max_workers = 1</code> \u2014 skillbook mutations are serialised to   avoid race conditions, even when multiple ReflectSteps finish in parallel.</li> </ul>"},{"location":"ACE2_PIPELINE%28old%29/#insight-source-provenance","title":"Insight Source Provenance","text":"<p><code>UpdateStep</code> calls <code>build_insight_source()</code> before applying updates to the skillbook. This attaches provenance metadata (sample question, epoch, step, error identification) to each operation so we can trace where a skill came from.</p>"},{"location":"ACE2_PIPELINE%28old%29/#deduplication-wiring","title":"Deduplication Wiring","text":"<p>Both runners accept an optional <code>dedup_config</code> parameter in <code>from_roles()</code> / <code>from_client()</code>. If provided and the <code>SkillManager</code> doesn't already have a <code>DeduplicationManager</code>, one is created and attached. This matches the old <code>ACEBase</code> behaviour.</p>"},{"location":"ACE2_PIPELINE%28old%29/#api-surface","title":"API Surface","text":""},{"location":"ACE2_PIPELINE%28old%29/#quick-start-from_client","title":"Quick Start (from_client)","text":"<p>The simplest way \u2014 a single LLM client, all roles created internally:</p> <pre><code>from ace.llm_providers import LiteLLMClient\nfrom ace2.pipelines import OfflineACE, OnlineACE\n\nclient = LiteLLMClient(model=\"gpt-4o-mini\")\n\n# Offline: multi-epoch training\nace = OfflineACE.from_client(client)\nresults = ace.run(train_samples, environment, epochs=3)\n\n# Online: single-pass streaming\nace = OnlineACE.from_client(client)\nresults = ace.run(test_samples, environment)\n</code></pre>"},{"location":"ACE2_PIPELINE%28old%29/#custom-roles-from_roles","title":"Custom Roles (from_roles)","text":"<p>Full control over role configuration:</p> <pre><code>from ace.roles import Agent, Reflector, SkillManager\nfrom ace.prompts_v2_1 import PromptManager\n\npm = PromptManager()\nace = OfflineACE.from_roles(\n    agent=Agent(client, prompt_template=pm.get_agent_prompt()),\n    reflector=Reflector(client),\n    skill_manager=SkillManager(client),\n    skillbook=skillbook,\n    reflection_window=5,\n    dedup_config=dedup_config,\n)\n</code></pre>"},{"location":"ACE2_PIPELINE%28old%29/#manual-pipeline","title":"Manual Pipeline","text":"<p>Build a custom pipeline with different steps or ordering:</p> <pre><code>from pipeline import Pipeline\nfrom ace2.steps import AgentStep, EvaluateStep, ReflectStep, UpdateStep\n\npipe = (\n    Pipeline()\n    .then(AgentStep(agent))\n    .then(EvaluateStep())\n    .then(ReflectStep(reflector, reflection_window=5))\n    .then(UpdateStep(skill_manager))\n)\n\n# Use with a runner or call directly\nace = OfflineACE(pipe, skillbook)\n</code></pre>"},{"location":"ACE2_PIPELINE%28old%29/#runner-behaviour","title":"Runner Behaviour","text":""},{"location":"ACE2_PIPELINE%28old%29/#offlineace","title":"OfflineACE","text":"<ul> <li>Loops <code>epochs \u00d7 samples</code>, building a <code>StepContext</code> per sample</li> <li>Propagates <code>recent_reflections</code> across samples within an epoch</li> <li>Supports checkpoint saving (<code>checkpoint_interval</code> + <code>checkpoint_dir</code>)</li> <li>Captures exceptions per-sample as <code>SampleResult.error</code></li> </ul>"},{"location":"ACE2_PIPELINE%28old%29/#onlineace","title":"OnlineACE","text":"<ul> <li>Single pass over an arbitrary iterable (list or generator)</li> <li>Same <code>StepContext</code> + error capture pattern</li> <li>No epoch concept \u2014 <code>epoch=1, total_epochs=1</code> always</li> </ul>"},{"location":"ACE2_PIPELINE%28old%29/#testing","title":"Testing","text":"<pre><code>python -m pytest tests/test_ace2_pipeline.py -v\n</code></pre> <p>The test suite covers: - Each step in isolation (6 tests) - Pipeline wiring validation (1 test) - OfflineACE end-to-end: single epoch, multi epoch, checkpointing, error capture (4 tests) - OnlineACE end-to-end: single sample, streaming generator (2 tests) - <code>from_client</code> shorthand for both runners (3 tests)</p>"},{"location":"ACE2_PIPELINE%28old%29/#migration-path","title":"Migration Path","text":"<p>ACE2 is currently a parallel implementation. The cut-over plan (TODO Part 5):</p> <ol> <li>Move <code>ace2/steps/</code> and <code>ace2/pipelines/</code> into <code>ace/pipeline/</code></li> <li>Update <code>ace/__init__.py</code> to import from the new location</li> <li>Deprecate <code>ace/adaptation.py</code> (<code>OfflineACE</code>, <code>OnlineACE</code>, <code>ACEBase</code>)</li> <li>Update all examples and benchmarks to use the new imports</li> </ol>"},{"location":"ACE_DESIGN/","title":"ACE Architecture Design","text":"<p>Specification for rewriting the legacy <code>ace/</code> module to use the pipeline engine.</p>"},{"location":"ACE_DESIGN/#implementation-status","title":"Implementation Status","text":"<p>Implemented in <code>ace_next/</code> (parallel to <code>ace/</code> for easy rollback). The package is fully self-contained \u2014 all types are copied locally, zero imports from <code>ace/</code>.</p> Component Status Location Core types (<code>ACEStepContext</code>, <code>SkillbookView</code>, <code>ACESample</code>) Done <code>ace_next/context.py</code> Data types (<code>Skill</code>, <code>Skillbook</code>, <code>UpdateBatch</code>, outputs) Done <code>ace_next/skill.py</code>, <code>skillbook.py</code>, <code>updates.py</code>, <code>outputs.py</code> Environments (<code>Sample</code>, <code>TaskEnvironment</code>, etc.) Done <code>ace_next/environments.py</code> Protocols (<code>AgentLike</code>, <code>ReflectorLike</code>, etc.) Done <code>ace_next/protocols/</code> Steps (all 10) Done <code>ace_next/steps/</code> <code>learning_tail()</code> helper Done <code>ace_next/steps/__init__.py</code> <code>ACERunner</code> base class Done <code>ace_next/runners/base.py</code> <code>TraceAnalyser</code> Done <code>ace_next/runners/trace_analyser.py</code> <code>ACE</code> runner Done <code>ace_next/runners/ace.py</code> Implementations (<code>Agent</code>, <code>Reflector</code>, <code>SkillManager</code>) Done <code>ace_next/implementations/</code> Deduplication (<code>DeduplicationManager</code>, <code>SimilarityDetector</code>) Done <code>ace_next/deduplication/</code> Integration steps (<code>BrowserExecuteStep</code>, <code>LangChainExecuteStep</code>, <code>ClaudeCodeExecuteStep</code>) Done <code>ace_next/integrations/</code> Integration runners (<code>BrowserUse</code>, <code>LangChain</code>, <code>ClaudeCode</code>) Done <code>ace_next/runners/</code> Convenience <code>from_model()</code> on integration runners Done <code>ace_next/runners/browser_use.py</code>, <code>langchain.py</code>, <code>claude_code.py</code> <code>ACELiteLLM</code> convenience wrapper Done <code>ace_next/runners/litellm.py</code> LLM providers (<code>LiteLLMClient</code>, <code>InstructorClient</code>, <code>LangChainLiteLLMClient</code>, <code>ClaudeCodeLLMClient</code>) Done <code>ace_next/providers/</code> Recursive Reflector Done <code>ace_next/rr/</code> (SubRunner base in <code>ace_next/core/</code>)"},{"location":"ACE_DESIGN/#goals","title":"Goals","text":"<ol> <li>Replace the monolithic <code>ACEBase</code> / <code>OfflineACE</code> / <code>OnlineACE</code> in <code>adaptation.py</code> with pipeline-based classes.</li> <li>Rename to match what each class actually does:</li> <li>TraceAnalyser: takes pre-recorded traces, outputs a skillbook. Replaces the concept of \"offline\" learning.</li> <li>ACE: the live adaptive pipeline. Replaces the concept of \"online\" learning. Also supports multi-epoch batch runs.</li> <li>Clean OOP: shared base class, composition over inheritance, pluggable steps.</li> <li>Unify the integration pattern \u2014 external frameworks produce raw trace objects (any type), TraceAnalyser passes them to the Reflector as-is.</li> <li>Maximise step granularity \u2014 each step does one thing so concerns are separated and each step is independently testable.</li> </ol>"},{"location":"ACE_DESIGN/#naming-changes","title":"Naming Changes","text":"Legacy New What it does <code>OfflineACE</code> <code>TraceAnalyser</code> Analyse pre-recorded traces \u2192 evolve a skillbook <code>OnlineACE</code> <code>ACE</code> Live execution \u2192 feedback \u2192 learning loop <code>ACEBase</code> <code>ACERunner</code> Shared runner infrastructure (composition, not inheritance from Pipeline) <code>ACEStepResult</code> Removed \u2014 use <code>SampleResult</code> from the pipeline engine Unified result type"},{"location":"ACE_DESIGN/#core-types","title":"Core Types","text":""},{"location":"ACE_DESIGN/#sample-unchanged","title":"Sample (unchanged)","text":"<p>The existing <code>Sample</code> dataclass stays as-is. ACE uses it.</p> <pre><code>@dataclass\nclass Sample:\n    question: str\n    context: str = \"\"\n    ground_truth: str | None = None\n    metadata: dict = field(default_factory=dict)\n    id: str | None = None\n</code></pre>"},{"location":"ACE_DESIGN/#acesample-protocol-for-step-access","title":"ACESample \u2014 protocol for step access","text":"<p>Steps access <code>ctx.sample.question</code> uniformly. A <code>Protocol</code> makes this duck typing explicit and type-safe:</p> <pre><code>class ACESample(Protocol):\n    \"\"\"Minimal interface that Sample satisfies.\"\"\"\n\n    @property\n    def question(self) -&gt; str: ...\n\n    @property\n    def context(self) -&gt; str: ...\n\n    @property\n    def ground_truth(self) -&gt; str | None: ...\n\n    @property\n    def metadata(self) -&gt; dict: ...\n</code></pre> <p><code>ACEStepContext.sample</code> is typed as <code>ACESample</code>. <code>Sample</code> satisfies it structurally. Mypy validates both sides: producers must provide the attributes, consumers can rely on them.</p>"},{"location":"ACE_DESIGN/#skillbookview-read-only-projection","title":"SkillbookView \u2014 read-only projection","text":"<p>The <code>Skillbook</code> is mutable \u2014 steps add, tag, and remove skills. Putting it directly on a <code>frozen=True</code> context would allow mutation through the reference (<code>ctx.skillbook.tag_skill(...)</code> succeeds even though <code>ctx.skillbook = other</code> fails). That breaks the immutability guarantee.</p> <p><code>SkillbookView</code> solves this. It wraps a <code>Skillbook</code> and exposes only read methods. Write methods don't exist on the class \u2014 calling them raises <code>AttributeError</code> at runtime and a type error at check time.</p> <pre><code>class SkillbookView:\n    \"\"\"Read-only projection of a Skillbook. Safe on a frozen context.\"\"\"\n\n    __slots__ = (\"_sb\",)\n\n    def __init__(self, skillbook: Skillbook) -&gt; None:\n        self._sb = skillbook\n\n    def as_prompt(self) -&gt; str:\n        return self._sb.as_prompt()\n\n    def get_skill(self, skill_id: str) -&gt; Skill | None:\n        return self._sb.get_skill(skill_id)\n\n    def skills(self, include_invalid: bool = False) -&gt; list[Skill]:\n        return self._sb.skills(include_invalid=include_invalid)\n\n    def stats(self) -&gt; dict[str, object]:\n        return self._sb.stats()\n\n    def __len__(self) -&gt; int:\n        return len(self._sb.skills())\n\n    def __iter__(self):\n        return iter(self._sb.skills())\n\n    def __repr__(self) -&gt; str:\n        return f\"SkillbookView({len(self)} skills)\"\n</code></pre> <p>Enforcement: - Type checker \u2014 mypy/pyright flags <code>ctx.skillbook.add_skill(...)</code> because <code>SkillbookView</code> has no such method. - Runtime \u2014 <code>AttributeError</code> if someone calls a write method anyway. - Convention \u2014 the underlying <code>_sb</code> is underscore-prefixed. Accessing it is a deliberate violation, not an accident.</p> <p>Steps that only read the skillbook (AgentStep, ReflectStep, UpdateStep, OpikStep) access <code>ctx.skillbook</code> \u2014 the view. Steps that write the skillbook (TagStep, ApplyStep, DeduplicateStep, CheckpointStep) receive the real <code>Skillbook</code> via constructor injection and use <code>self.skillbook</code>.</p>"},{"location":"ACE_DESIGN/#acestepcontext","title":"ACEStepContext","text":"<p>Subclass of the pipeline engine's <code>StepContext</code>. Carries all step-to-step data for the ACE pipeline. The pipeline engine only knows about <code>sample</code> and <code>metadata</code>; all ACE-specific fields live here.</p> <pre><code>@dataclass(frozen=True)\nclass ACEStepContext(StepContext):\n    \"\"\"Immutable context for the ACE pipeline.\n\n    The skillbook field is a SkillbookView (read-only). Steps that need to\n    write to the skillbook receive the real Skillbook via constructor injection.\n    \"\"\"\n\n    sample: ACESample | None = None\n    skillbook: SkillbookView | None = None\n    trace: object | None = None\n    agent_output: AgentOutput | None = None\n    reflection: ReflectorOutput | None = None\n    skill_manager_output: UpdateBatch | None = None\n    epoch: int = 1\n    total_epochs: int = 1\n    step_index: int = 0\n    total_steps: int | None = None\n    global_sample_index: int = 0\n</code></pre> <p>The <code>trace</code> field holds the raw execution record from any external system \u2014 a browser-use <code>AgentHistoryList</code>, a LangChain result dict, a Claude Code transcript, or any arbitrary Python object. It has no enforced schema. The Reflector receives the raw trace and is responsible for making sense of it \u2014 this gives maximum flexibility for analysis without constraining trace format. Extraction helpers can be added later as an optional layer if needed.</p> <p>What goes on the context vs what gets injected:</p> On the context Injected via constructor Nature Step-to-step data + read-only dependencies Mutable shared state Lifetime Per-sample (born in <code>_build_context</code>, dies after pipeline) Per-runner (created once, shared across samples) Immutable? Yes \u2014 frozen fields, read-only views No \u2014 mutable by design Examples <code>agent_output</code>, <code>reflection</code>, <code>skillbook</code> (view) <code>skillbook</code> (real), <code>environment</code>, <code>dedup_manager</code> Validated by engine? Yes \u2014 <code>requires</code>/<code>provides</code> No \u2014 runtime error if missing"},{"location":"ACE_DESIGN/#protocols","title":"Protocols","text":"<p>Steps depend on protocols, not concrete classes. Each protocol defines the minimal interface a step needs. Concrete implementations satisfy them structurally \u2014 no inheritance required.</p> <p>All protocols live in <code>ace_next/protocols/</code> (one file per protocol, re-exported from <code>__init__.py</code>).</p> Protocol Method Used by Satisfied by <code>AgentLike</code> <code>generate(question, context, skillbook, reflection, **kwargs) \u2192 AgentOutput</code> <code>AgentStep</code> <code>Agent</code> <code>ReflectorLike</code> <code>reflect(question, agent_output, skillbook, ground_truth, feedback, **kwargs) \u2192 ReflectorOutput</code> <code>ReflectStep</code> <code>Reflector</code> <code>SkillManagerLike</code> <code>update_skills(reflection, skillbook, question_context, progress, **kwargs) \u2192 SkillManagerOutput</code> <code>UpdateStep</code> <code>SkillManager</code> <code>DeduplicationManagerLike</code> <code>get_similarity_report(skillbook) \u2192 str \\| None</code> <code>DeduplicateStep</code> <code>DeduplicationManager</code> <code>LLMClientLike</code> <code>complete(prompt, **kwargs) \u2192 Any</code> + <code>complete_structured(prompt, response_model, **kwargs) \u2192 T</code> <code>Agent</code>, <code>Reflector</code>, <code>SkillManager</code> Any LLM client with both methods"},{"location":"ACE_DESIGN/#llmclientlike","title":"LLMClientLike","text":"<p>The implementations (<code>Agent</code>, <code>Reflector</code>, <code>SkillManager</code>) all depend on <code>LLMClientLike</code> \u2014 a protocol requiring two methods:</p> <pre><code>@runtime_checkable\nclass LLMClientLike(Protocol):\n    def complete(self, prompt: str, **kwargs: Any) -&gt; Any: ...\n    def complete_structured(self, prompt: str, response_model: type[T], **kwargs: Any) -&gt; T: ...\n</code></pre> <p><code>complete_structured</code> returns a validated Pydantic model instance. This is the key capability \u2014 implementations call <code>llm.complete_structured(prompt, AgentOutput)</code> and get back a typed, validated object. Any LLM client that provides both methods satisfies the protocol: <code>LiteLLMClient</code> wrapped with Instructor, a custom OpenAI wrapper, or a mock for testing.</p> <p>Design decision: The old <code>ace/roles.py</code> auto-wrapped LLM clients with Instructor if <code>complete_structured</code> was missing. In <code>ace_next</code>, this auto-wrapping is removed \u2014 callers must pass a pre-wrapped client (e.g. <code>wrap_with_instructor(LiteLLMClient(...))</code> from <code>ace_next.providers</code>). This makes the requirement explicit.</p>"},{"location":"ACE_DESIGN/#why-protocols-not-abc","title":"Why protocols, not ABC","text":"<p>Protocols use structural typing (duck typing checked by mypy). A class satisfies a protocol if it has the right methods \u2014 no <code>class Agent(AgentLike)</code> inheritance needed. This means: - Users can pass any object with a matching method, not just subclasses. - Mocks satisfy protocols without ceremony. - Steps are decoupled from implementations at the type level, not just by convention.</p>"},{"location":"ACE_DESIGN/#class-hierarchy","title":"Class Hierarchy","text":"<pre><code>ACERunner (shared infrastructure: epoch loop, delegates to Pipeline.run())\n\u251c\u2500\u2500 TraceAnalyser       \u2014 [Reflect \u2192 Tag \u2192 Update \u2192 Apply]; input = any trace object\n\u251c\u2500\u2500 ACE                 \u2014 [Agent \u2192 Evaluate \u2192 Reflect \u2192 Tag \u2192 Update \u2192 Apply]; input = Sample + Environment\n\u251c\u2500\u2500 BrowserUse          \u2014 [BrowserExecute \u2192 BrowserToTrace \u2192 Reflect \u2192 Tag \u2192 Update \u2192 Apply]; input = task strings\n\u251c\u2500\u2500 LangChain           \u2014 [LangChainExecute \u2192 LangChainToTrace \u2192 Reflect \u2192 Tag \u2192 Update \u2192 Apply]; input = chain inputs\n\u2514\u2500\u2500 ClaudeCode          \u2014 [ClaudeCodeExecute \u2192 ClaudeCodeToTrace \u2192 Reflect \u2192 Tag \u2192 Update \u2192 Apply]; input = task strings\n\nACELiteLLM (standalone convenience wrapper \u2014 not an ACERunner subclass)\n\u251c\u2500\u2500 ask()               \u2014 direct Agent call, no pipeline\n\u251c\u2500\u2500 learn()             \u2014 delegates to lazy-init ACE runner\n\u251c\u2500\u2500 learn_from_traces() \u2014 delegates to lazy-init TraceAnalyser\n\u2514\u2500\u2500 learn_from_feedback()\u2014 manual single-shot learning from last ask()\n\nRRStep (SubRunner \u2014 composable iterative step)\n\u251c\u2500\u2500 __call__()          \u2014 StepProtocol entry; can be placed in any runner's pipeline\n\u251c\u2500\u2500 reflect()           \u2014 ReflectorLike entry; standalone use\n\u2514\u2500\u2500 run_loop()          \u2014 SubRunner loop driver; inner Pipeline([LLMCall, ExtractCode, SandboxExec, CheckResult])\n</code></pre> <p>All runners compose a <code>Pipeline</code> rather than extending it. <code>RRStep</code> extends <code>SubRunner</code> (from <code>ace_next/core/sub_runner.py</code>) and can be used as a step in any runner's pipeline \u2014 it is a black box that satisfies <code>StepProtocol</code>. The pipeline is an implementation detail, not part of the public interface. Each subclass only overrides <code>run()</code> (public signature) and <code>_build_context()</code> (input mapping).</p> <p>Integration runners (<code>BrowserUse</code>, <code>LangChain</code>, <code>ClaudeCode</code>) each provide two construction paths: <code>from_roles()</code> for pre-built role instances, and <code>from_model()</code> for auto-building roles from a model string. <code>ACELiteLLM</code> is a standalone class (not an <code>ACERunner</code> subclass) because it wraps two different runners and exposes a different API (<code>ask</code>, <code>learn</code>, <code>learn_from_traces</code>).</p> <p>Each integration runner uses two steps before the learning tail: (1) an execute step that produces an integration-specific result type (e.g. <code>BrowserResult</code>), and (2) a ToTrace step that converts that result into the standardised trace dict the learning tail expects. This separation keeps framework-specific logic in the execute step and trace formatting in the converter \u2014 each is independently testable.</p>"},{"location":"ACE_DESIGN/#acerunner-shared-base","title":"ACERunner \u2014 shared base","text":"<p>Encapsulates everything that TraceAnalyser, ACE, and integration runners have in common. The runner's only job is the epoch loop and Iterable validation. Per-sample iteration, error handling, background execution, and checkpoints are all delegated to <code>Pipeline.run()</code>.</p> <p>Subclasses only override <code>run()</code> (public signature) and <code>_build_context()</code> (input mapping).</p> <pre><code>class ACERunner:\n    \"\"\"Shared runner infrastructure for all ACE runners.\"\"\"\n\n    def __init__(\n        self,\n        pipeline: Pipeline,\n        skillbook: Skillbook,\n    ) -&gt; None:\n        self.pipeline = pipeline\n        self.skillbook = skillbook\n\n    def save(self, path: str) -&gt; None:\n        \"\"\"Save the current skillbook to disk.\"\"\"\n        self.skillbook.save_to_file(path)\n\n    def wait_for_background(self, timeout: float | None = None) -&gt; None:\n        \"\"\"Block until all background learning tasks complete.\n\n        Delegates to Pipeline.wait_for_background(). Call this after run(wait=False)\n        before saving the skillbook or reading final results.\n        \"\"\"\n        self.pipeline.wait_for_background(timeout)\n\n    @property\n    def learning_stats(self) -&gt; dict:\n        \"\"\"Return background learning progress.\n\n        Useful after run(wait=False) to monitor learning without blocking.\n        Delegates to Pipeline.background_stats() to avoid reaching into\n        pipeline internals.\n        \"\"\"\n        return self.pipeline.background_stats()\n</code></pre>"},{"location":"ACE_DESIGN/#responsibilities","title":"Responsibilities","text":"Concern Owner Epoch loop + Iterable validation <code>ACERunner._run()</code> Per-sample iteration + error isolation <code>Pipeline.run()</code> Foreground/background split <code>Pipeline.run()</code> (via <code>async_boundary</code>) Concurrent workers <code>Pipeline.run(workers=N)</code> Checkpoints <code>CheckpointStep</code> (in the pipeline, configured at construction) Background drain <code>ACERunner.wait_for_background()</code> \u2192 <code>Pipeline.wait_for_background()</code> Background monitoring <code>ACERunner.learning_stats</code> Skillbook I/O <code>save(path)</code> on the runner (delegates to <code>skillbook.save_to_file()</code>) <p>Each sample is independent \u2014 no state persists across samples. The skillbook is the only cross-sample coupling: read-only steps see it via <code>ctx.skillbook</code> (a <code>SkillbookView</code>), write steps mutate it via <code>self.skillbook</code> (the real <code>Skillbook</code>, injected at construction).</p> <p>Eventual consistency: <code>SkillbookView</code> is a thin delegation wrapper, not a snapshot \u2014 it reads from the live <code>Skillbook</code> at call time. When background learning is active (<code>async_boundary</code> on <code>ReflectStep</code>), concurrent samples may observe partially-updated skillbook state. For example, Sample 2's <code>ReflectStep</code> might read the skillbook mid-mutation by Sample 1's <code>ApplyStep</code>. This is by design: steps see a best-effort view of the current skillbook rather than a point-in-time snapshot. The trade-off is acceptable because (1) the Reflector and SkillManager use the skillbook as LLM prompt context, where a few missing or extra skills have negligible impact on output quality, (2) serialising all skillbook reads would eliminate the concurrency benefit of <code>max_workers &gt; 1</code> on <code>ReflectStep</code>, and (3) write steps (<code>TagStep</code>, <code>ApplyStep</code>) already run with <code>max_workers = 1</code>, so writes are serialised \u2014 only reads interleave with writes. If stricter isolation is ever needed, <code>SkillbookView</code> can be changed to snapshot on construction (deep copy) without altering step code.</p>"},{"location":"ACE_DESIGN/#generic-run-loop","title":"Generic run loop","text":"<p>Every subclass delegates to <code>_run()</code>. The only thing that varies per subclass is (1) the public <code>run()</code> signature and (2) the <code>_build_context()</code> method that maps input items to <code>ACEStepContext</code>.</p> <pre><code>def _run(\n    self,\n    items: Sequence | Iterable,\n    *,\n    epochs: int,\n    wait: bool = True,\n    **kwargs,\n) -&gt; list[SampleResult]:\n    if epochs &gt; 1 and not isinstance(items, Sequence):\n        raise ValueError(\"Multi-epoch requires a Sequence, not a consumed Iterable.\")\n\n    results: list[SampleResult] = []\n    n = len(items) if isinstance(items, Sequence) else None\n\n    for epoch in range(1, epochs + 1):\n        contexts = [\n            self._build_context(item, epoch=epoch, total_epochs=epochs,\n                                index=idx, total=n,\n                                global_sample_index=(epoch - 1) * n + idx if n is not None else idx,\n                                **kwargs)\n            for idx, item in enumerate(items, start=1)\n        ]\n        epoch_results = self.pipeline.run(contexts)\n        results.extend(epoch_results)\n\n    if wait:\n        self.pipeline.wait_for_background()\n    return results\n</code></pre> <p><code>wait</code> parameter: When <code>wait=True</code> (default), <code>_run()</code> blocks until all background learning completes before returning \u2014 results are fully populated. When <code>wait=False</code>, <code>_run()</code> returns immediately after the foreground steps finish. Background learning continues asynchronously. Use <code>wait_for_background()</code> to drain later, or <code>learning_stats</code> to monitor progress.</p> <p>The runner builds fully-initialized <code>ACEStepContext</code> objects (epoch counters, pre-filled outputs for traces, etc.) and hands them to <code>Pipeline.run(contexts)</code>. Construction IS initialization \u2014 from that point on contexts are frozen and the pipeline processes what it receives without wrapping or guessing. <code>Pipeline.run()</code> handles iteration, error isolation, foreground/background split, and concurrent workers. The runner only owns the epoch loop.</p>"},{"location":"ACE_DESIGN/#traceanalyser","title":"TraceAnalyser","text":"<p>Analyses pre-recorded traces without executing an agent. Runs the learning tail only. Accepts raw trace objects of any type \u2014 the raw trace is placed directly on <code>ctx.trace</code> and the Reflector is responsible for making sense of it.</p>"},{"location":"ACE_DESIGN/#when-to-use","title":"When to use","text":"<ul> <li>You have execution logs from an external system (browser-use, LangChain, custom agent, human sessions).</li> <li>You want to build or refine a skillbook from historical data.</li> <li>You want to re-analyse the same data multiple times (multi-epoch) to extract deeper patterns.</li> </ul>"},{"location":"ACE_DESIGN/#pipeline","title":"Pipeline","text":"<pre><code>[ReflectStep] \u2192 [TagStep] \u2192 [UpdateStep] \u2192 [ApplyStep]\n</code></pre> <p>No AgentStep, no EvaluateStep. The trace already contains the agent's output and the evaluation feedback.</p>"},{"location":"ACE_DESIGN/#context-building","title":"Context building","text":"<p>TraceAnalyser places the raw trace directly on <code>ctx.trace</code>. No extraction, no conversion \u2014 the Reflector receives the trace as-is and has full freedom to analyze it however it sees fit.</p> <pre><code>def _build_context(self, raw_trace, *, epoch, total_epochs, index, total, global_sample_index) -&gt; ACEStepContext:\n    return ACEStepContext(\n        skillbook=SkillbookView(self.skillbook),\n        trace=raw_trace,                         # raw object, no enforced schema\n        epoch=epoch,\n        total_epochs=total_epochs,\n        step_index=index,\n        total_steps=total,\n        global_sample_index=global_sample_index,\n    )\n</code></pre> <p>The <code>skillbook</code> field is a <code>SkillbookView</code> \u2014 read-only steps access it from the context. Write steps (TagStep, ApplyStep) receive the real <code>Skillbook</code> via constructor injection. Each sample is independent \u2014 no state carries over from previous samples.</p>"},{"location":"ACE_DESIGN/#interface","title":"Interface","text":"<pre><code>class TraceAnalyser(ACERunner):\n    \"\"\"Analyse pre-recorded traces to build a skillbook.\"\"\"\n\n    @classmethod\n    def from_roles(cls, *, reflector, skill_manager, skillbook=None, **kwargs) -&gt; \"TraceAnalyser\": ...\n\n    def run(\n        self,\n        traces: Sequence[Any],\n        epochs: int = 1,\n    ) -&gt; list[SampleResult]: ...\n</code></pre> <p>Note: no <code>environment</code> parameter, no converter. The raw trace goes straight onto the context. The Reflector is responsible for making sense of it \u2014 this gives maximum flexibility for analysis without constraining trace format. Extraction into structured fields can be added later as an optional step if needed. No checkpoint parameters \u2014 checkpoints are configured at construction time via the factory methods.</p>"},{"location":"ACE_DESIGN/#multi-epoch-semantics","title":"Multi-epoch semantics","text":"<p>Each epoch re-processes all traces with the current (evolving) skillbook. Early epochs extract obvious patterns; later epochs refine and consolidate.</p> <pre><code>Epoch 1:  trace\u2081 \u2192 trace\u2082 \u2192 ... \u2192 trace\u2099   (skillbook grows)\nEpoch 2:  trace\u2081 \u2192 trace\u2082 \u2192 ... \u2192 trace\u2099   (skillbook refines)\nEpoch 3:  trace\u2081 \u2192 trace\u2082 \u2192 ... \u2192 trace\u2099   (diminishing returns)\n</code></pre> <p>Each sample is independent. The only thing that evolves across samples (and epochs) is the skillbook itself \u2014 visible as a read-only <code>SkillbookView</code> on the context, mutated by write steps via the real <code>Skillbook</code> (constructor-injected).</p>"},{"location":"ACE_DESIGN/#run-delegates-to-_run","title":"run() \u2014 delegates to _run()","text":"<pre><code>def run(self, traces, epochs=1, *, wait=True):\n    return self._run(traces, epochs=epochs, wait=wait)\n</code></pre> <p>No epoch loop, no per-sample iteration \u2014 <code>_run()</code> handles all of that.</p>"},{"location":"ACE_DESIGN/#ace","title":"ACE","text":"<p>The full live adaptive pipeline. An agent executes, the reflector analyses, the skill manager updates. Optionally evaluates against a <code>TaskEnvironment</code> for feedback-driven learning.</p>"},{"location":"ACE_DESIGN/#when-to-use_1","title":"When to use","text":"<ul> <li>You are building a new agent from scratch.</li> <li>You want closed-loop learning where the agent improves in real time.</li> <li>Optionally: you have a <code>TaskEnvironment</code> that can evaluate outputs (provides richer feedback for the Reflector).</li> </ul>"},{"location":"ACE_DESIGN/#pipeline_1","title":"Pipeline","text":"<pre><code>[AgentStep] \u2192 [EvaluateStep] \u2192 [ReflectStep] \u2192 [TagStep] \u2192 [UpdateStep] \u2192 [ApplyStep]\n</code></pre>"},{"location":"ACE_DESIGN/#context-building_1","title":"Context building","text":"<pre><code>def _build_context(self, sample, *, epoch, total_epochs, index, total, global_sample_index, **_) -&gt; ACEStepContext:\n    return ACEStepContext(\n        sample=sample,\n        skillbook=SkillbookView(self.skillbook),\n        epoch=epoch,\n        total_epochs=total_epochs,\n        step_index=index,\n        total_steps=total,\n        global_sample_index=global_sample_index,\n    )\n</code></pre> <p>Each sample is independent. The <code>skillbook</code> field is a <code>SkillbookView</code> (read-only). Write steps receive the real <code>Skillbook</code> via constructor injection. The environment (if any) is injected into <code>EvaluateStep</code> at construction time \u2014 it does not appear on the context.</p>"},{"location":"ACE_DESIGN/#interface_1","title":"Interface","text":"<pre><code>class ACE(ACERunner):\n    \"\"\"Live adaptive pipeline: Agent \u2192 Evaluate \u2192 Reflect \u2192 Tag \u2192 Update \u2192 Apply.\"\"\"\n\n    @classmethod\n    def from_roles(cls, *, agent, reflector, skill_manager, environment=None, skillbook=None, **kwargs) -&gt; \"ACE\": ...\n\n    def run(\n        self,\n        samples: Sequence[Sample] | Iterable[Sample],\n        epochs: int = 1,\n        *,\n        wait: bool = True,\n    ) -&gt; list[SampleResult]: ...\n</code></pre> <p>The <code>environment</code> is optional and provided at construction time, not at <code>run()</code> time. When provided, <code>EvaluateStep</code> uses it to generate feedback that enriches the trace. When omitted, the trace still contains the agent's output, question, context, and ground truth \u2014 the Reflector can learn from ground-truth comparison or from the agent's reasoning alone.</p>"},{"location":"ACE_DESIGN/#single-pass-vs-multi-epoch","title":"Single-pass vs multi-epoch","text":"<p>A single class handles both use cases. <code>epochs=1</code> gives single-pass behaviour. <code>epochs &gt; 1</code> gives multi-epoch batch training.</p> <pre><code># Single pass (was OnlineACE)\nresults = ace.run(samples, epochs=1)\n\n# Multi-epoch batch (was OfflineACE)\nresults = ace.run(training_set, epochs=3)\n\n# Fire-and-forget \u2014 agent results returned fast, learning continues in background\nresults = ace.run(samples, wait=False)\n</code></pre> <p>When <code>samples</code> is an <code>Iterable</code> (not <code>Sequence</code>), <code>epochs</code> must be <code>1</code> \u2014 you cannot replay a consumed iterable. <code>_run()</code> raises <code>ValueError</code> if <code>epochs &gt; 1</code> and <code>samples</code> is not a <code>Sequence</code>. Note: <code>_run()</code> materializes the full iterable into a list of contexts before passing them to <code>Pipeline.run()</code>. This is a deliberate simplification \u2014 see Potential Improvements.</p>"},{"location":"ACE_DESIGN/#run-delegates-to-_run_1","title":"run() \u2014 delegates to _run()","text":"<pre><code>def run(self, samples, epochs=1, *, wait=True):\n    return self._run(samples, epochs=epochs, wait=wait)\n</code></pre> <p>No instance state is modified \u2014 the runner stays reentrant.</p>"},{"location":"ACE_DESIGN/#factory-methods","title":"Factory Methods","text":"<p>All runners provide a <code>from_roles</code> factory that takes pre-built role instances. Integration runners (<code>BrowserUse</code>, <code>LangChain</code>, <code>ClaudeCode</code>) also provide a <code>from_model()</code> factory that auto-builds roles from a model string (see High-Level Convenience API).</p>"},{"location":"ACE_DESIGN/#from_roles-explicit-construction","title":"<code>from_roles</code> \u2014 explicit construction","text":"<pre><code># TraceAnalyser: bring your own roles\nanalyser = TraceAnalyser.from_roles(\n    reflector=Reflector(llm, prompt_template=custom_prompt),\n    skill_manager=SkillManager(llm),\n    skillbook=existing_skillbook,\n)\n\n# ACE: bring your own roles\nace = ACE.from_roles(\n    agent=Agent(llm),\n    reflector=Reflector(llm),\n    skill_manager=SkillManager(llm),\n    skillbook=existing_skillbook,\n    dedup_manager=DeduplicationManager(DeduplicationConfig(similarity_threshold=0.85)),\n)\n</code></pre>"},{"location":"ACE_DESIGN/#common-parameters-on-from_roles","title":"Common parameters on <code>from_roles</code>","text":"Parameter Default Description <code>skillbook</code> <code>Skillbook()</code> Starting skillbook (empty if not provided) <code>dedup_manager</code> <code>None</code> Appends a <code>DeduplicateStep</code> to the pipeline <code>dedup_interval</code> <code>10</code> Deduplication frequency (samples between runs) <code>checkpoint_dir</code> <code>None</code> Appends a <code>CheckpointStep</code> to the pipeline <code>checkpoint_interval</code> <code>10</code> Checkpoint frequency (samples between saves) <code>extra_steps</code> <code>None</code> Additional steps appended after the learning tail (e.g. <code>OpikStep</code>) <p>Checkpoint and deduplication are configured at construction time. The factory conditionally appends the corresponding steps to the pipeline tail. <code>extra_steps</code> are appended last \u2014 after dedup and checkpoint. Both classes follow the same pattern \u2014 ACE prepends its execute steps:</p> <pre><code># TraceAnalyser \u2014 learning tail only\n@classmethod\ndef from_roles(cls, *, reflector, skill_manager, skillbook=None,\n               dedup_manager=None, dedup_interval=10,\n               checkpoint_dir=None, checkpoint_interval=10,\n               extra_steps=None):\n    skillbook = skillbook or Skillbook()\n    steps = learning_tail(\n        reflector, skill_manager, skillbook,\n        dedup_manager=dedup_manager, dedup_interval=dedup_interval,\n        checkpoint_dir=checkpoint_dir, checkpoint_interval=checkpoint_interval,\n    )\n    if extra_steps:\n        steps.extend(extra_steps)\n    return cls(pipeline=Pipeline(steps), skillbook=skillbook)\n\n# ACE \u2014 execute head + learning tail\n@classmethod\ndef from_roles(cls, *, agent, reflector, skill_manager, environment=None,\n               skillbook=None, dedup_manager=None, dedup_interval=10,\n               checkpoint_dir=None, checkpoint_interval=10,\n               extra_steps=None):\n    skillbook = skillbook or Skillbook()\n    steps = [\n        AgentStep(agent),\n        EvaluateStep(environment),\n        *learning_tail(\n            reflector, skill_manager, skillbook,\n            dedup_manager=dedup_manager, dedup_interval=dedup_interval,\n            checkpoint_dir=checkpoint_dir, checkpoint_interval=checkpoint_interval,\n        ),\n    ]\n    if extra_steps:\n        steps.extend(extra_steps)\n    return cls(pipeline=Pipeline(steps), skillbook=skillbook)\n</code></pre> <p>Read-only steps (ReflectStep, UpdateStep) access the skillbook via <code>ctx.skillbook</code> (a <code>SkillbookView</code>). Write steps (TagStep, ApplyStep, DeduplicateStep, CheckpointStep) receive the real <code>Skillbook</code> via constructor.</p>"},{"location":"ACE_DESIGN/#steps","title":"Steps","text":"<p>Reusable step implementations live in <code>ace_next/steps/</code>. Each is a single class in a single file. All satisfy the <code>StepProtocol</code> from the pipeline engine. Each step does exactly one thing.</p> <p>Design principle: steps are stateless. A step's <code>__call__</code> is a pure function of its constructor arguments and the incoming <code>ACEStepContext</code>. No internal counters, no accumulated state between invocations. If a step needs run-scoped information (like a global sample index for interval logic), the runner computes it and places it on the context. This keeps steps predictable across multiple <code>run()</code> calls \u2014 behaviour depends only on what's in the context, not on invocation history.</p>"},{"location":"ACE_DESIGN/#step-summary","title":"Step Summary","text":"Step Requires (context) Injected (constructor) Provides Side effects <code>max_workers</code> AgentStep <code>sample</code>, <code>skillbook</code> <code>agent</code> <code>agent_output</code> None default (1) EvaluateStep <code>sample</code>, <code>agent_output</code> <code>environment</code> (optional) <code>trace</code> None default (1) ReflectStep <code>trace</code>, <code>skillbook</code> <code>reflector</code> <code>reflection</code> None (pure) 3; <code>async_boundary = True</code> TagStep <code>reflection</code> <code>skillbook</code> (real) \u2014 Tags skills on skillbook 1 UpdateStep <code>reflection</code>, <code>skillbook</code> <code>skill_manager</code> <code>skill_manager_output</code> None (pure) 1 ApplyStep <code>skill_manager_output</code> <code>skillbook</code> (real) \u2014 Applies update batch to skillbook 1 DeduplicateStep <code>global_sample_index</code> <code>manager</code> (DeduplicationManagerLike), <code>skillbook</code> (real) \u2014 Consolidates similar skills 1 CheckpointStep <code>global_sample_index</code> <code>skillbook</code> (real) \u2014 Saves skillbook to disk 1 OpikStep <code>skillbook</code> <code>project_name</code>, <code>tags</code> \u2014 Logs pipeline traces to Opik 1 PersistStep <code>skillbook</code> <code>target_path</code> \u2014 Writes skillbook to CLAUDE.md or similar 1 <p>Requires vs Injected: <code>Requires</code> lists context fields read by the step \u2014 validated by the pipeline engine at construction time. The <code>skillbook</code> field on the context is a <code>SkillbookView</code> (read-only). Steps that write to the skillbook (TagStep, ApplyStep, DeduplicateStep, CheckpointStep) receive the real <code>Skillbook</code> via constructor injection \u2014 marked as \"(real)\" in the table. These injected dependencies are not tracked by <code>requires</code>/<code>provides</code>.</p> <p><code>trace</code> as the universal learning input: The learning tail's entry point (ReflectStep) requires only <code>trace</code> and <code>skillbook</code> from the context \u2014 subsequent steps in the tail chain off ReflectStep's output (<code>reflection</code>). In the standard ACE pipeline, <code>EvaluateStep</code> bundles the structured fields (<code>sample</code>, <code>agent_output</code>, and optionally environment feedback) into a <code>trace</code> dict. In TraceAnalyser, <code>_build_context</code> places the raw trace directly. In integrations, the execute step provides <code>trace</code> from its framework's native output. This means the learning tail is agnostic to trace format \u2014 <code>ReflectStep</code> passes <code>ctx.trace</code> to the Reflector, which is responsible for making sense of whatever it receives.</p> <p>Steps with <code>provides = \u2014</code> are pure side-effect steps (<code>provides = frozenset()</code>). They mutate shared state (skillbook) or write to external systems (disk, Opik) but add no new fields to the context. <code>OpikStep</code> is not included in <code>learning_tail()</code> \u2014 users append it explicitly to keep observability decoupled from core learning.</p>"},{"location":"ACE_DESIGN/#agentstep","title":"AgentStep","text":"<pre><code>class AgentStep:\n    requires = frozenset({\"sample\", \"skillbook\"})\n    provides = frozenset({\"agent_output\"})\n\n    def __init__(self, agent: AgentLike) -&gt; None:\n        self.agent = agent\n\n    def __call__(self, ctx: ACEStepContext) -&gt; ACEStepContext:\n        agent_output = self.agent.generate(\n            question=ctx.sample.question,\n            context=ctx.sample.context,\n            skillbook=ctx.skillbook,       # SkillbookView (read-only)\n            sample=ctx.sample,\n        )\n        return ctx.replace(agent_output=agent_output)\n</code></pre> <p>Reads the skillbook via <code>ctx.skillbook</code> (a <code>SkillbookView</code>). No constructor injection needed \u2014 read-only access is sufficient.</p>"},{"location":"ACE_DESIGN/#evaluatestep","title":"EvaluateStep","text":"<pre><code>class EvaluateStep:\n    requires = frozenset({\"sample\", \"agent_output\"})\n    provides = frozenset({\"trace\"})\n\n    def __init__(self, environment: TaskEnvironment | None = None) -&gt; None:\n        self.environment = environment\n\n    def __call__(self, ctx: ACEStepContext) -&gt; ACEStepContext:\n        trace = {\n            \"question\": ctx.sample.question,\n            \"context\": ctx.sample.context,\n            \"ground_truth\": ctx.sample.ground_truth,\n            \"reasoning\": ctx.agent_output.reasoning,\n            \"answer\": ctx.agent_output.final_answer,\n            \"skill_ids\": ctx.agent_output.skill_ids,\n        }\n        if self.environment:\n            result = self.environment.evaluate(\n                sample=ctx.sample, agent_output=ctx.agent_output,\n            )\n            trace[\"feedback\"] = result.feedback\n        return ctx.replace(trace=trace)\n</code></pre> <p>Bridges the execute head (typed ACE objects) to the learning tail (raw traces). Always bundles the structured fields into a <code>trace</code> dict. Optionally evaluates the agent output against a <code>TaskEnvironment</code> \u2014 when provided via constructor, the environment's feedback is included in the trace. When no environment is provided, the trace still contains the agent's output, question, context, and ground truth \u2014 the Reflector can learn from these directly. The <code>TaskEnvironment</code> is injected at construction time (not on the context) to keep the context free of per-runner dependencies. This also means different <code>ACE</code> instances can use different environments without changing the pipeline shape.</p>"},{"location":"ACE_DESIGN/#reflectstep","title":"ReflectStep","text":"<pre><code>class ReflectStep:\n    requires = frozenset({\"trace\", \"skillbook\"})\n    provides = frozenset({\"reflection\"})\n\n    async_boundary = True\n    max_workers = 3\n\n    def __init__(self, reflector: ReflectorLike) -&gt; None:\n        self.reflector = reflector\n\n    def __call__(self, ctx: ACEStepContext) -&gt; ACEStepContext:\n        trace = ctx.trace\n\n        if isinstance(trace, dict):\n            # Structured trace from EvaluateStep \u2014 extract known fields\n            agent_output = AgentOutput(\n                reasoning=trace.get(\"reasoning\", \"\"),\n                final_answer=trace.get(\"answer\", \"\"),\n                skill_ids=trace.get(\"skill_ids\", []),\n            )\n            reflection = self.reflector.reflect(\n                question=trace.get(\"question\", \"\"),\n                agent_output=agent_output,\n                skillbook=ctx.skillbook,\n                ground_truth=trace.get(\"ground_truth\"),\n                feedback=trace.get(\"feedback\"),\n            )\n        else:\n            # Raw trace from TraceAnalyser or integration \u2014 pass as-is\n            reflection = self.reflector.reflect(\n                question=\"\",\n                agent_output=AgentOutput(reasoning=\"\", final_answer=\"\"),\n                skillbook=ctx.skillbook,\n                trace=trace,\n            )\n\n        return ctx.replace(reflection=reflection)\n</code></pre> <p>Pure \u2014 produces a reflection object, no side effects. Handles two trace formats: (1) when <code>ctx.trace</code> is a dict (from EvaluateStep), it extracts known fields and calls the Reflector's existing API with typed arguments; (2) when <code>ctx.trace</code> is any other object (from TraceAnalyser or integrations), it passes the raw trace via <code>**kwargs</code> for the Reflector to handle directly. Declares <code>async_boundary = True</code> \u2014 everything from here onward runs in a background thread pool. This lets the execute head return fast while learning continues.</p>"},{"location":"ACE_DESIGN/#tagstep","title":"TagStep","text":"<pre><code>class TagStep:\n    requires = frozenset({\"reflection\"})\n    provides = frozenset()\n\n    max_workers = 1\n\n    def __init__(self, skillbook: Skillbook) -&gt; None:\n        self.skillbook = skillbook\n\n    def __call__(self, ctx: ACEStepContext) -&gt; ACEStepContext:\n        for tag in ctx.reflection.skill_tags:\n            try:\n                self.skillbook.tag_skill(tag.id, tag.tag)\n            except ValueError:\n                logger.warning(\"TagStep: skill_id %r not found, skipping tag %r\", tag.id, tag.tag)\n        return ctx\n</code></pre> <p>Side-effect step \u2014 tags skills on <code>self.skillbook</code> (the real <code>Skillbook</code>, injected via constructor \u2014 not the <code>SkillbookView</code> on the context). <code>max_workers = 1</code> serialises skillbook writes. Hallucinated skill IDs from the Reflector are logged at <code>WARNING</code> level rather than silently swallowed \u2014 this provides a diagnostic signal without aborting the pipeline.</p> <p>Separated from ReflectStep so that: - ReflectStep is a pure function (LLM call \u2192 reflection object) and can be tested without a skillbook. - TagStep can be tested with a mock reflection without an LLM.</p>"},{"location":"ACE_DESIGN/#updatestep","title":"UpdateStep","text":"<pre><code>class UpdateStep:\n    requires = frozenset({\"reflection\", \"skillbook\"})\n    provides = frozenset({\"skill_manager_output\"})\n\n    max_workers = 1\n\n    def __init__(self, skill_manager: SkillManagerLike) -&gt; None:\n        self.skill_manager = skill_manager\n\n    def __call__(self, ctx: ACEStepContext) -&gt; ACEStepContext:\n        output = self.skill_manager.update(\n            reflection=ctx.reflection,     # ReflectorOutput\n            skillbook=ctx.skillbook,       # SkillbookView (read-only)\n        )\n        return ctx.replace(skill_manager_output=output)\n</code></pre> <p>Pure \u2014 generates update operations from the <code>ReflectorOutput</code> and the current skillbook state. The Reflector has already done the heavy lifting of analysing the trace; the SkillManager turns those insights into concrete skillbook operations (ADD, UPDATE, TAG, REMOVE). Does not mutate the skillbook. <code>max_workers = 1</code> because the skill manager reads the current skillbook state and concurrent calls would see stale data.</p>"},{"location":"ACE_DESIGN/#applystep","title":"ApplyStep","text":"<pre><code>class ApplyStep:\n    requires = frozenset({\"skill_manager_output\"})\n    provides = frozenset()\n\n    max_workers = 1\n\n    def __init__(self, skillbook: Skillbook) -&gt; None:\n        self.skillbook = skillbook\n\n    def __call__(self, ctx: ACEStepContext) -&gt; ACEStepContext:\n        self.skillbook.apply_update(ctx.skill_manager_output)\n        return ctx\n</code></pre> <p>Side-effect step \u2014 applies the update batch to <code>self.skillbook</code> (the real <code>Skillbook</code>, injected via constructor). Separated from UpdateStep so that: - UpdateStep can be tested without mutating a skillbook (check that correct operations are generated). - ApplyStep can be tested with a mock update batch (check that operations are applied correctly).</p>"},{"location":"ACE_DESIGN/#deduplicatestep","title":"DeduplicateStep","text":"<pre><code>class DeduplicateStep:\n    requires = frozenset({\"global_sample_index\"})\n    provides = frozenset()\n\n    max_workers = 1\n\n    def __init__(self, manager: DeduplicationManagerLike, skillbook: Skillbook, *, interval: int = 10) -&gt; None:\n        self.manager = manager\n        self.skillbook = skillbook\n        self.interval = interval\n\n    def __call__(self, ctx: ACEStepContext) -&gt; ACEStepContext:\n        if ctx.global_sample_index % self.interval != 0:\n            return ctx\n        report = self.manager.get_similarity_report(self.skillbook)\n        if report:\n            logger.info(\"DeduplicateStep: similarity report at sample %d:\\n%s\",\n                        ctx.global_sample_index, report)\n        return ctx\n</code></pre> <p>Optional side-effect step \u2014 consolidates similar skills in <code>self.skillbook</code> (injected). Appended to the pipeline by factory methods when a <code>DeduplicationManagerLike</code> is provided. The step takes a protocol, not the concrete <code>DeduplicationManager</code> \u2014 this keeps <code>ace_next</code> decoupled from the deduplication implementation in <code>ace/</code>. Stateless \u2014 uses <code>ctx.global_sample_index</code> (computed by the runner) with a configurable <code>interval</code> (default 10) to skip most invocations. Deduplication involves O(n\u00b2) similarity comparisons across all skills, so running it on every sample would be expensive as the skillbook grows.</p>"},{"location":"ACE_DESIGN/#checkpointstep","title":"CheckpointStep","text":"<p>Optional tail step that periodically saves the skillbook to disk. Stateless \u2014 derives the checkpoint decision from context fields.</p> <pre><code>class CheckpointStep:\n    requires = frozenset({\"global_sample_index\"})\n    provides = frozenset()\n\n    def __init__(self, directory: str | Path, skillbook: Skillbook, *, interval: int = 10) -&gt; None:\n        self.directory = Path(directory)\n        self.skillbook = skillbook\n        self.interval = interval\n\n    def __call__(self, ctx: ACEStepContext) -&gt; ACEStepContext:\n        if ctx.global_sample_index % self.interval != 0:\n            return ctx\n\n        self.directory.mkdir(parents=True, exist_ok=True)\n        self.skillbook.save_to_file(str(self.directory / f\"checkpoint_{ctx.global_sample_index}.json\"))\n        self.skillbook.save_to_file(str(self.directory / \"latest.json\"))\n        return ctx\n</code></pre> <p>Key points: - Stateless. Uses <code>ctx.global_sample_index</code> (computed by the runner) for interval logic. No internal counter, no reset needed. - <code>provides</code> is empty \u2014 it only writes to disk, does not modify the context. - Skillbook via constructor \u2014 saves <code>self.skillbook</code>, not a context field. - Placement: Appended after ApplyStep by the factory when <code>checkpoint_dir</code> is provided. When <code>async_boundary</code> is set, checkpoints happen in the background tail. - <code>max_workers</code> not set \u2014 inherits default of 1 from the pipeline engine, which is correct (disk writes should be serialised).</p>"},{"location":"ACE_DESIGN/#opikstep","title":"OpikStep","text":"<pre><code>class OpikStep:\n    requires = frozenset({\"skillbook\"})\n    provides = frozenset()\n\n    def __init__(\n        self,\n        project_name: str = \"ace-framework\",\n        tags: list[str] | None = None,\n    ) -&gt; None: ...\n</code></pre> <p>Explicit, opt-in observability step \u2014 creates an Opik trace per sample with pipeline metadata, agent output, reflection insights, and skill manager operations. Does NOT register the LiteLLM callback \u2014 call <code>register_opik_litellm_callback()</code> separately if you also want per-LLM-call token/cost tracking. Two independent tracing modes: (1) pipeline step (this class) \u2014 client-agnostic, reads <code>ACEStepContext</code> fields; (2) LiteLLM callback (<code>register_opik_litellm_callback</code>) \u2014 LiteLLM-specific, registers <code>OpikLogger</code> on <code>litellm.callbacks</code>.</p> <p>Only requires <code>skillbook</code> (always present). Reads other context fields (<code>reflection</code>, <code>skill_manager_output</code>, <code>trace</code>, <code>agent_output</code>) with guards \u2014 they may or may not be populated depending on pipeline shape. When used directly, gracefully degrades to a no-op when Opik is not installed or <code>OPIK_DISABLED=true</code>. When used via <code>ACELiteLLM(opik=True)</code>, fails loudly \u2014 raises <code>ImportError</code> if the package is missing, <code>RuntimeError</code> if client init fails.</p> <p>Passes <code>OPIK_API_KEY</code>, <code>OPIK_WORKSPACE</code>, and <code>OPIK_URL_OVERRIDE</code> explicitly from environment variables \u2014 does not depend on the global <code>~/.opik.config</code> file.</p> <p>Call <code>flush()</code> after the pipeline finishes to drain buffered traces before the process exits (the Opik client batches sends asynchronously).</p> <p>Not wired into <code>learning_tail()</code>. Users append it via <code>extra_steps</code> on <code>from_roles()</code>, or manually after calling <code>learning_tail()</code>:</p> <pre><code>from ace_next.steps import OpikStep, learning_tail\n\n# Append to a custom pipeline\nsteps = [\n    MyExecuteStep(agent),\n    MyToTrace(),\n    *learning_tail(reflector, skill_manager, skillbook),\n    OpikStep(project_name=\"my-project\"),\n]\n\n# Via from_roles() extra_steps parameter\nace = ACE.from_roles(agent=a, reflector=r, skill_manager=sm,\n                     extra_steps=[OpikStep(project_name=\"my-project\")])\n\n# Via ACELiteLLM (explicit opt-in \u2014 enables both pipeline + LiteLLM tracing)\nace = ACELiteLLM.from_model(\"gpt-4o-mini\", opik=True, opik_project=\"my-project\")\n\n# LLM-level token tracking only (no pipeline traces)\nfrom ace_next import register_opik_litellm_callback\nregister_opik_litellm_callback()\n</code></pre>"},{"location":"ACE_DESIGN/#persiststep","title":"PersistStep","text":"<pre><code>class PersistStep:\n    requires = frozenset({\"skillbook\"})\n    provides = frozenset()\n\n    def __init__(self, target_path: str | Path, skillbook: Skillbook) -&gt; None:\n        self.target_path = Path(target_path)\n        self.skillbook = skillbook\n\n    def __call__(self, ctx: ACEStepContext) -&gt; ACEStepContext:\n        self.skillbook.save_to_file(str(self.target_path))\n        return ctx\n</code></pre> <p>Integration-specific side-effect step \u2014 writes the current skillbook to an external file (e.g., <code>CLAUDE.md</code> for Claude Code). Used by <code>ClaudeCode</code> runner to persist learned strategies into the project's instruction file after each learning cycle. Unlike <code>CheckpointStep</code> (which saves the full skillbook JSON at intervals), <code>PersistStep</code> runs on every sample and writes in the target format expected by the integration. Receives the real <code>Skillbook</code> via constructor injection.</p>"},{"location":"ACE_DESIGN/#implementations","title":"Implementations","text":"<p>Concrete LLM-based implementations of the role protocols. Live in <code>ace_next/implementations/</code> \u2014 fully self-contained with no imports from <code>ace/</code>.</p>"},{"location":"ACE_DESIGN/#overview","title":"Overview","text":"Class Protocol Method Location <code>Agent</code> <code>AgentLike</code> <code>generate()</code> <code>implementations/agent.py</code> <code>Reflector</code> <code>ReflectorLike</code> <code>reflect()</code> <code>implementations/reflector.py</code> <code>SkillManager</code> <code>SkillManagerLike</code> <code>update_skills()</code> <code>implementations/skill_manager.py</code> <p>All three share the same constructor pattern:</p> <pre><code>def __init__(self, llm: LLMClientLike, prompt_template: str = DEFAULT_PROMPT, *, max_retries: int = 3) -&gt; None:\n</code></pre> <p>The <code>llm</code> parameter must satisfy <code>LLMClientLike</code> \u2014 it must have both <code>complete()</code> and <code>complete_structured()</code>. No auto-wrapping with Instructor; callers pass pre-wrapped clients.</p>"},{"location":"ACE_DESIGN/#agent","title":"Agent","text":"<p>Produces answers using the current skillbook of strategies. Formats the prompt with the skillbook, reflection, question, and context, then calls <code>llm.complete_structured(prompt, AgentOutput)</code>. After the LLM call, extracts cited skill IDs from the reasoning using <code>extract_cited_skill_ids()</code> (regex matching <code>[section-00001]</code> patterns).</p> <pre><code>agent = Agent(llm)\noutput = agent.generate(\n    question=\"What is the capital of France?\",\n    context=\"Answer concisely\",\n    skillbook=skillbook,\n)\n# output.final_answer == \"Paris\"\n# output.skill_ids == [\"geography-00001\"]  (extracted from reasoning)\n</code></pre>"},{"location":"ACE_DESIGN/#reflector","title":"Reflector","text":"<p>Analyzes agent outputs to extract lessons and improve strategies. Builds a skillbook excerpt from the agent's cited skill IDs (via <code>make_skillbook_excerpt()</code>), formats the prompt, and calls <code>llm.complete_structured(prompt, ReflectorOutput)</code>.</p> <p>SIMPLE mode only \u2014 single-pass reflection. Recursive mode (where the Reflector iterates multiple times to deepen analysis) is an advanced feature deferred to a later version.</p> <pre><code>reflector = Reflector(llm)\nreflection = reflector.reflect(\n    question=\"What is 2+2?\",\n    agent_output=agent_output,\n    skillbook=skillbook,\n    ground_truth=\"4\",\n    feedback=\"Correct!\",\n)\n# reflection.key_insight, reflection.skill_tags, reflection.extracted_learnings\n</code></pre>"},{"location":"ACE_DESIGN/#skillmanager","title":"SkillManager","text":"<p>Transforms reflections into actionable skillbook updates. Serializes the <code>ReflectorOutput</code> into a JSON dict, formats the prompt with progress and skillbook stats, and calls <code>llm.complete_structured(prompt, SkillManagerOutput)</code>.</p> <p>No dedup integration \u2014 in <code>ace_next</code>, deduplication is handled by a separate <code>DeduplicateStep</code> in the pipeline. The SkillManager only produces <code>SkillManagerOutput</code>; it does not call a dedup manager itself.</p> <pre><code>sm = SkillManager(llm)\noutput = sm.update_skills(\n    reflection=reflection_output,\n    skillbook=skillbook,\n    question_context=\"Math problem solving\",\n    progress=\"5/10 correct\",\n)\nskillbook.apply_update(output.update)\n</code></pre>"},{"location":"ACE_DESIGN/#shared-helpers-implementationshelperspy","title":"Shared Helpers (<code>implementations/helpers.py</code>)","text":"Function Purpose <code>extract_cited_skill_ids(text)</code> Regex <code>[section-00001]</code> \u2192 deduplicated list of IDs <code>format_optional(value)</code> Returns <code>\"(none)\"</code> for falsy values <code>make_skillbook_excerpt(skillbook, skill_ids)</code> Builds <code>[id] content</code> lines for cited skills"},{"location":"ACE_DESIGN/#prompt-templates-implementationspromptspy","title":"Prompt Templates (<code>implementations/prompts.py</code>)","text":"<p>Self-contained copy of the v2.1 prompts from <code>ace/prompts_v2_1.py</code>. The <code>{current_date}</code> placeholder is filled at import time via <code>datetime.now().strftime(...)</code>.</p> Constant Role <code>AGENT_PROMPT</code> Agent prompt with strategic problem-solving protocol <code>REFLECTOR_PROMPT</code> Reflector prompt with diagnostic analysis protocol <code>SKILL_MANAGER_PROMPT</code> SkillManager prompt with atomic strategy creation <code>SKILLBOOK_USAGE_INSTRUCTIONS</code> Shared text for skillbook usage guidance <p>Also exports <code>wrap_skillbook_for_external_agent(skillbook)</code> \u2014 the canonical function for injecting skillbook context into external agentic systems.</p>"},{"location":"ACE_DESIGN/#deduplication","title":"Deduplication","text":"<p>Skill deduplication subsystem. Lives in <code>ace_next/deduplication/</code> \u2014 fully self-contained with no imports from <code>ace/</code>.</p>"},{"location":"ACE_DESIGN/#overview_1","title":"Overview","text":"Class Role Location <code>SimilarityDetector</code> Computes embeddings, detects similar pairs <code>deduplication/detector.py</code> <code>DeduplicationManager</code> Coordinates detection and consolidation <code>deduplication/manager.py</code> <code>MergeOp</code>, <code>DeleteOp</code>, <code>KeepOp</code>, <code>UpdateOp</code> Consolidation operation types <code>deduplication/operations.py</code>"},{"location":"ACE_DESIGN/#similaritydetector","title":"SimilarityDetector","text":"<p>Computes embeddings and detects similar skill pairs using cosine similarity. Supports two embedding providers:</p> <ul> <li>LiteLLM \u2014 uses <code>litellm.embedding()</code> for remote embedding models</li> <li>sentence-transformers \u2014 uses a local <code>SentenceTransformer</code> model (lazy-loaded)</li> </ul> <p>Feature detection uses inline <code>importlib.import_module</code> checks instead of <code>ace.features</code>. Cosine similarity has a numpy implementation with a pure-Python fallback.</p> <p>Key methods: - <code>ensure_embeddings(skillbook)</code> \u2014 compute embeddings for all skills that lack one - <code>detect_similar_pairs(skillbook, threshold)</code> \u2014 find all pairs above the similarity threshold - Supports <code>within_section_only</code> mode (compare skills only within the same section) - Respects existing <code>KEEP</code> decisions via <code>skillbook.has_keep_decision()</code></p>"},{"location":"ACE_DESIGN/#deduplicationmanager","title":"DeduplicationManager","text":"<p>Satisfies <code>DeduplicationManagerLike</code> protocol. Coordinates the full dedup workflow:</p> <ol> <li><code>get_similarity_report(skillbook)</code> \u2014 ensures embeddings, detects similar pairs, generates a formatted report for the SkillManager prompt. Returns <code>None</code> if dedup is disabled or too few pairs found.</li> <li><code>parse_consolidation_operations(response_data)</code> \u2014 parses <code>consolidation_operations</code> from SkillManager response JSON into typed operation objects.</li> <li><code>apply_operations(operations, skillbook)</code> \u2014 applies consolidation operations to the skillbook.</li> </ol>"},{"location":"ACE_DESIGN/#consolidation-operations","title":"Consolidation Operations","text":"<p>Four operation types, all dataclasses:</p> Operation Effect <code>MergeOp</code> Combine skills \u2014 accumulate counters into <code>keep_id</code>, soft-delete others, update content <code>DeleteOp</code> Soft-delete a redundant skill <code>KeepOp</code> Store a <code>SimilarityDecision</code> so the pair is not flagged again <code>UpdateOp</code> Refine a skill's content to differentiate it, clear its embedding <p><code>apply_consolidation_operations(operations, skillbook)</code> dispatches each operation to the appropriate apply function.</p>"},{"location":"ACE_DESIGN/#pipeline-integration","title":"Pipeline Integration","text":"<p>Deduplication runs as a separate <code>DeduplicateStep</code> in the pipeline, not inside the SkillManager role. The step is appended by factory methods when a <code>DeduplicationManagerLike</code> is provided:</p> <pre><code>ace = ACE.from_roles(\n    agent=agent, reflector=reflector, skill_manager=skill_manager,\n    dedup_manager=DeduplicationManager(DeduplicationConfig(similarity_threshold=0.85)),\n    dedup_interval=10,\n)\n# Pipeline: Agent \u2192 Evaluate \u2192 Reflect \u2192 Tag \u2192 Update \u2192 Apply \u2192 Deduplicate\n</code></pre>"},{"location":"ACE_DESIGN/#integration-pattern","title":"Integration Pattern","text":"<p>External frameworks (browser-use, LangChain, Claude Code) integrate via composable pipeline steps in <code>ace_next/integrations/</code>. Each integration provides three things:</p> <ol> <li>Result type \u2014 an integration-specific dataclass (e.g. <code>BrowserResult</code>, <code>ClaudeCodeResult</code>)</li> <li>Execute step \u2014 INJECT skillbook context + EXECUTE the framework, writes the result to <code>ctx.trace</code></li> <li>ToTrace step \u2014 converts the integration-specific result into the standardised trace dict that <code>ReflectStep</code> expects</li> </ol> <p>Runners in <code>ace_next/runners/</code> compose these steps with <code>learning_tail()</code>.</p>"},{"location":"ACE_DESIGN/#core-idea-execute-convert-learn","title":"Core idea: execute \u2192 convert \u2192 learn","text":"<p>Each integration defines its own input/output format. A converter step acts as a compatibility layer between the integration-specific result and the learning tail's standardised trace dict.</p> <pre><code>Standard ACE:      [Agent \u2192 Evaluate]                          \u2192 [Reflect \u2192 Tag \u2192 Update \u2192 Apply]\n                    \u2570\u2500\u2500 execute (built-in) \u2500\u2500\u256f                    \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 learn (shared) \u2500\u2500\u2500\u2500\u2500\u2500\u256f\n                         provides: trace (dict) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba requires: trace\n\nBrowser-use:       [BrowserExecute] \u2192 [BrowserToTrace]         \u2192 [Reflect \u2192 Tag \u2192 Update \u2192 Apply]\n                    \u2570\u2500\u2500 execute \u2500\u2500\u2500\u2500\u256f   \u2570\u2500\u2500 convert \u2500\u2500\u256f           \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 learn (shared) \u2500\u2500\u2500\u2500\u2500\u2500\u256f\n                    provides: trace      rewrites trace             requires: trace\n                    (BrowserResult)      (BrowserResult \u2192 dict)\n\nTraceAnalyser:     [_build_context]                            \u2192 [Reflect \u2192 Tag \u2192 Update \u2192 Apply]\n                    \u2570\u2500\u2500 sets ctx.trace (raw object) \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u256f      \u2570\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 learn (shared) \u2500\u2500\u2500\u2500\u2500\u2500\u256f\n</code></pre> <p>The execute step writes an integration-specific result type to <code>ctx.trace</code>. The ToTrace step reads that result and rewrites <code>ctx.trace</code> with a standardised dict. The learning tail only ever sees the dict.</p>"},{"location":"ACE_DESIGN/#two-step-contract","title":"Two-step contract","text":"<p>Every integration provides two steps that chain together:</p> <p>Step 1 \u2014 Execute step (INJECT + EXECUTE):</p> <pre><code>class SomeExecuteStep:\n    requires = frozenset({\"sample\", \"skillbook\"})\n    provides = frozenset({\"trace\"})\n\n    def __init__(self, framework_client) -&gt; None:\n        self.framework_client = framework_client\n\n    def __call__(self, ctx: ACEStepContext) -&gt; ACEStepContext:\n        task = ctx.sample                               # raw input (string, dict, etc.)\n        enhanced = self._inject(task, ctx.skillbook)    # prepend skillbook context\n        result = self.framework_client.run(enhanced)    # framework-specific execution\n        return ctx.replace(trace=SomeResult(...))       # integration-specific result type\n</code></pre> <p>Step 2 \u2014 ToTrace step (convert to standardised dict):</p> <pre><code>class SomeToTrace:\n    requires = frozenset({\"trace\"})\n    provides = frozenset({\"trace\"})         # overwrites trace with standardised dict\n\n    def __call__(self, ctx: ACEStepContext) -&gt; ACEStepContext:\n        r: SomeResult = ctx.trace\n        trace = {\n            \"question\": r.task,\n            \"reasoning\": r.execution_trace,     # integration-specific formatting\n            \"answer\": r.output,\n            \"skill_ids\": r.cited_skill_ids,\n            \"feedback\": f\"Task {'succeeded' if r.success else 'failed'}\",\n            \"ground_truth\": None,\n        }\n        return ctx.replace(trace=trace)\n</code></pre> <p>The standardised trace dict keys match what <code>ReflectStep</code> expects: <code>question</code>, <code>reasoning</code>, <code>answer</code>, <code>skill_ids</code>, <code>feedback</code>, <code>ground_truth</code>.</p>"},{"location":"ACE_DESIGN/#why-two-steps-instead-of-one","title":"Why two steps instead of one","text":"<p>Splitting execute from trace conversion gives three benefits:</p> <ol> <li>Independent testability \u2014 test the execute step with a mock framework without worrying about trace format; test the ToTrace step with a fixture result without running a real framework.</li> <li>Reusability \u2014 the execute step can be used standalone (without learning) to get framework-specific results. The ToTrace step can be swapped for a custom converter.</li> <li>Separation of concerns \u2014 framework interaction logic stays in the execute step; trace formatting stays in the converter. Neither knows about the other's internals.</li> </ol>"},{"location":"ACE_DESIGN/#result-types","title":"Result types","text":"<p>Each integration defines its own result dataclass:</p> Integration Result type Key fields Browser-use <code>BrowserResult</code> <code>task</code>, <code>success</code>, <code>output</code>, <code>error</code>, <code>steps_count</code>, <code>duration_seconds</code>, <code>cited_skill_ids</code>, <code>chronological_steps</code>, <code>raw_history</code> Claude Code <code>ClaudeCodeResult</code> <code>task</code>, <code>success</code>, <code>output</code>, <code>execution_trace</code>, <code>returncode</code>, <code>error</code> LangChain <code>LangChainResult</code> <code>task</code>, <code>output</code>, <code>result_type</code> (simple/agent/langgraph/error), <code>success</code>, <code>error</code>, <code>intermediate_steps</code>, <code>messages</code>, <code>raw_result</code>"},{"location":"ACE_DESIGN/#example-browser-use-execute-step","title":"Example \u2014 browser-use execute step","text":"<pre><code>class BrowserExecuteStep:\n    requires = frozenset({\"sample\", \"skillbook\"})\n    provides = frozenset({\"trace\"})\n\n    def __init__(self, browser_llm, browser=None, **agent_kwargs) -&gt; None:\n        self.browser_llm = browser_llm\n        self.browser = browser\n        self.agent_kwargs = agent_kwargs\n\n    async def __call__(self, ctx: ACEStepContext) -&gt; ACEStepContext:\n        task: str = ctx.sample      # raw task string, not a Sample object\n\n        # INJECT \u2014 prepend skillbook context\n        enhanced_task = self._inject(task, ctx.skillbook)\n\n        # EXECUTE \u2014 run browser-use agent\n        agent = Agent(task=enhanced_task, llm=self.browser_llm, **self.agent_kwargs)\n        history = await agent.run()\n\n        # Build integration-specific result\n        result = BrowserResult(\n            task=task, success=True, output=history.final_result(),\n            steps_count=history.number_of_steps(),\n            chronological_steps=..., raw_history=history,\n        )\n        return ctx.replace(trace=result)\n</code></pre>"},{"location":"ACE_DESIGN/#composing-into-a-runner","title":"Composing into a runner","text":"<p>Runners compose execute step + ToTrace step + learning tail:</p> <pre><code>class BrowserUse(ACERunner):\n    \"\"\"Browser-use agent with ACE learning pipeline.\"\"\"\n\n    @classmethod\n    def from_roles(cls, *, browser_llm, reflector, skill_manager,\n                   skillbook=None, **kwargs):\n        skillbook = skillbook or Skillbook()\n        steps = [\n            BrowserExecuteStep(browser_llm),\n            BrowserToTrace(),\n            *learning_tail(reflector, skill_manager, skillbook, **kwargs),\n        ]\n        return cls(pipeline=Pipeline(steps), skillbook=skillbook)\n\n    def run(self, tasks, epochs=1, *, wait=True):\n        return self._run(tasks, epochs=epochs, wait=wait)\n\n    def _build_context(self, task, *, epoch, total_epochs, index, total,\n                       global_sample_index, **_):\n        return ACEStepContext(\n            sample=task,    # raw string \u2014 not wrapped in Sample\n            skillbook=SkillbookView(self.skillbook),\n            epoch=epoch, total_epochs=total_epochs,\n            step_index=index, total_steps=total,\n            global_sample_index=global_sample_index,\n        )\n</code></pre> <p>The pattern is the same for every integration: subclass <code>ACERunner</code>, compose <code>[ExecuteStep, ToTrace, *learning_tail()]</code> in the factory, accept raw inputs (strings, dicts) in <code>run()</code>, and map them to <code>ACEStepContext</code> in <code>_build_context()</code>. No <code>Sample</code> wrapping \u2014 the raw input goes directly on <code>ctx.sample</code>.</p>"},{"location":"ACE_DESIGN/#learning_tail-reusable-learning-steps","title":"<code>learning_tail()</code> \u2014 reusable learning steps","text":"<p>Every integration assembles the same <code>[Reflect \u2192 Tag \u2192 Update \u2192 Apply]</code> suffix with optional dedup and checkpoint steps. Rather than duplicating this wiring in every factory method, <code>learning_tail()</code> returns the standard step list:</p> <pre><code># ace_next/steps/__init__.py\n\ndef learning_tail(\n    reflector: ReflectorLike,\n    skill_manager: SkillManagerLike,\n    skillbook: Skillbook,\n    *,\n    dedup_manager: DeduplicationManagerLike | None = None,\n    dedup_interval: int = 10,\n    checkpoint_dir: str | Path | None = None,\n    checkpoint_interval: int = 10,\n) -&gt; list[StepProtocol]:\n    \"\"\"Return the standard ACE learning steps.\n\n    Use this when building custom integrations that provide their own\n    execute step(s) but want the standard learning pipeline.\n    \"\"\"\n    steps: list[StepProtocol] = [\n        ReflectStep(reflector),\n        TagStep(skillbook),\n        UpdateStep(skill_manager),\n        ApplyStep(skillbook),\n    ]\n    if dedup_manager:\n        steps.append(DeduplicateStep(dedup_manager, skillbook, interval=dedup_interval))\n    if checkpoint_dir:\n        steps.append(CheckpointStep(checkpoint_dir, skillbook, interval=checkpoint_interval))\n    return steps\n</code></pre> <p>Integration factories become shorter and less error-prone:</p> <pre><code>class BrowserUse(ACERunner):\n    @classmethod\n    def from_roles(cls, *, browser_llm, reflector, skill_manager, skillbook=None, **kwargs):\n        skillbook = skillbook or Skillbook()\n        steps = [\n            BrowserExecuteStep(browser_llm),\n            BrowserToTrace(),\n            *learning_tail(reflector, skill_manager, skillbook, **kwargs),\n        ]\n        return cls(pipeline=Pipeline(steps), skillbook=skillbook)\n</code></pre> <p>Power users building fully custom pipelines can also use it:</p> <pre><code>from ace_next.steps import learning_tail\n\nskillbook = Skillbook.load_from_file(\"expert.json\")\nsteps = [\n    MyCustomExecuteStep(my_agent),\n    MyValidationStep(),  # custom step before learning\n    *learning_tail(reflector, skill_manager, skillbook, dedup_manager=dedup),\n]\nrunner = ACERunner(Pipeline(steps), skillbook)\n</code></pre>"},{"location":"ACE_DESIGN/#traceanalyser-batch-learning-from-recorded-executions","title":"TraceAnalyser \u2014 batch learning from recorded executions","text":"<p>Integrations also support offline learning. When an integration records execution history (browser-use AgentHistory, LangChain intermediate_steps, Claude Code transcripts), it feeds the raw objects directly to TraceAnalyser:</p> <pre><code># Record browser executions\nhistories = [await agent.run(task) for task in tasks]\n\n# Feed raw histories directly \u2014 Reflector analyses them as-is\nanalyser = TraceAnalyser.from_roles(\n    reflector=Reflector(llm_client),\n    skill_manager=SkillManager(llm_client),\n)\nanalyser.run(histories, epochs=2)\nanalyser.save(\"browser_expert.json\")\n</code></pre>"},{"location":"ACE_DESIGN/#live-vs-offline","title":"Live vs offline","text":"Integration Runner TraceAnalyser When Live execution Post-hoc analysis Agent Framework runs it Already ran Feedback Generated live Baked into trace Use case Production deployment Historical batch learning, debugging <p>Both update the same skillbook. A common workflow: TraceAnalyser builds an initial skillbook from historical data, then an integration runner refines it during live deployment.</p>"},{"location":"ACE_DESIGN/#high-level-convenience-api","title":"High-Level Convenience API","text":"<p>Integration runners provide two construction paths directly on the class \u2014 no separate wrapper classes needed:</p> <ol> <li><code>from_roles()</code> \u2014 accepts pre-built role instances (Reflector, SkillManager, etc.)</li> <li><code>from_model()</code> \u2014 accepts a model string and auto-builds roles internally</li> </ol> <p>This keeps the API surface minimal: one class per integration, two ways to construct it.</p> <pre><code># Explicit construction \u2014 bring your own roles\nrunner = BrowserUse.from_roles(\n    browser_llm=browser_llm,\n    reflector=Reflector(llm),\n    skill_manager=SkillManager(llm),\n)\n\n# Convenience construction \u2014 just specify the model\nrunner = BrowserUse.from_model(browser_llm, ace_model=\"gpt-4o-mini\")\n\n# Both return the same BrowserUse instance with the same API\nresults = runner.run([\"Find top HN post\", \"Check weather in NYC\"])\nrunner.save(\"browser_expert.json\")\n</code></pre>"},{"location":"ACE_DESIGN/#from_model-on-integration-runners","title":"<code>from_model()</code> on integration runners","text":"<p>Each integration runner's <code>from_model()</code> builds a <code>LiteLLMClient</code>, wraps it in <code>Reflector</code> and <code>SkillManager</code>, and delegates to <code>from_roles()</code>:</p> <pre><code>class BrowserUse(ACERunner):\n    @classmethod\n    def from_model(cls, browser_llm, *, ace_model=\"gpt-4o-mini\",\n                   ace_max_tokens=2048, ace_llm=None, **kwargs) -&gt; BrowserUse:\n        if ace_llm is None:\n            from ..providers import LiteLLMClient\n            ace_llm = LiteLLMClient(model=ace_model, max_tokens=ace_max_tokens)\n        return cls.from_roles(\n            browser_llm=browser_llm,\n            reflector=Reflector(ace_llm),\n            skill_manager=SkillManager(ace_llm),\n            **kwargs,\n        )\n</code></pre> <p>The same pattern applies to <code>LangChain.from_model(runnable, ...)</code> and <code>ClaudeCode.from_model(working_dir=..., ...)</code>. Providers are imported lazily inside <code>from_model()</code> to avoid hard dependencies on <code>litellm</code> at import time.</p>"},{"location":"ACE_DESIGN/#additional-convenience-on-from_roles","title":"Additional convenience on <code>from_roles()</code>","text":"<p>Integration runners also accept <code>skillbook_path</code> and <code>dedup_config</code> on <code>from_roles()</code> for common resolution patterns:</p> <pre><code>runner = BrowserUse.from_roles(\n    browser_llm=browser_llm,\n    reflector=reflector,\n    skill_manager=skill_manager,\n    skillbook_path=\"browser_expert.json\",   # loads skillbook from file\n    dedup_config=DeduplicationConfig(similarity_threshold=0.85),  # builds DeduplicationManager\n)\n</code></pre>"},{"location":"ACE_DESIGN/#convenience-lifecycle-methods-on-runners","title":"Convenience lifecycle methods on runners","text":"<p>All integration runners provide:</p> <ul> <li><code>get_strategies() -&gt; str</code> \u2014 formatted skillbook strategies for display</li> <li>Backward-compat aliases: <code>save_skillbook</code>, <code>load_skillbook</code>, <code>wait_for_learning</code></li> </ul> <p>These are defined directly on the runner class, not on a separate wrapper.</p>"},{"location":"ACE_DESIGN/#acelitellm-standalone-convenience-wrapper","title":"<code>ACELiteLLM</code> \u2014 standalone convenience wrapper","text":"<p><code>ACELiteLLM</code> is the only standalone wrapper class (not an <code>ACERunner</code> subclass). It exists because it wraps two different runners (<code>ACE</code> and <code>TraceAnalyser</code>) and exposes a fundamentally different API (<code>ask</code>, <code>learn</code>, <code>learn_from_traces</code>, <code>learn_from_feedback</code>).</p> <pre><code>class ACELiteLLM:\n    def __init__(self, llm, *, skillbook=None, environment=None,\n                 opik=False, opik_project=\"ace-framework\", opik_tags=None, ...):\n        self.agent = Agent(llm)\n        self.reflector = Reflector(llm)\n        self.skill_manager = SkillManager(llm)\n        self._skillbook = skillbook or Skillbook()\n        self.environment = environment\n        self._ace: ACE | None = None          # lazy-init\n        self._analyser: TraceAnalyser | None = None  # lazy-init\n\n        # Opik observability (explicit opt-in)\n        self._opik_step = None\n        if opik:\n            self._opik_step = OpikStep(project_name=opik_project, tags=opik_tags)\n            register_opik_litellm_callback(project_name=opik_project)\n\n    @classmethod\n    def from_model(cls, model=\"gpt-4o-mini\", *, max_tokens=2048,\n                   temperature=0.0, opik=False, opik_project=\"ace-framework\",\n                   opik_tags=None, **kwargs) -&gt; ACELiteLLM:\n        \"\"\"Build from a model string.\"\"\"\n        llm = LiteLLMClient(model=model, max_tokens=max_tokens, temperature=temperature)\n        return cls(llm, opik=opik, opik_project=opik_project, opik_tags=opik_tags, **kwargs)\n\n    def _get_extra_steps(self):\n        \"\"\"Return extra pipeline steps (e.g. OpikStep) or None.\"\"\"\n        if self._opik_step is not None:\n            return [self._opik_step]\n        return None\n\n    def _get_ace(self, environment=None):\n        \"\"\"Return (or build) cached ACE runner. Passes extra_steps.\"\"\"\n        ...\n        self._ace = ACE.from_roles(..., extra_steps=self._get_extra_steps())\n        ...\n\n    def _get_analyser(self):\n        \"\"\"Return (or build) cached TraceAnalyser. Passes extra_steps.\"\"\"\n        ...\n        self._analyser = TraceAnalyser.from_roles(..., extra_steps=self._get_extra_steps())\n        ...\n\n    def ask(self, question, context=\"\") -&gt; str:\n        \"\"\"Direct Agent call \u2014 no pipeline. Stores interaction for learn_from_feedback().\"\"\"\n        ...\n\n    def learn(self, samples, environment=None, epochs=1, *, wait=True):\n        \"\"\"Delegate to lazy-init ACE runner.\"\"\"\n        return self._get_ace(environment).run(samples, epochs=epochs, wait=wait)\n\n    def learn_from_traces(self, traces, epochs=1, *, wait=True):\n        \"\"\"Delegate to lazy-init TraceAnalyser.\"\"\"\n        return self._get_analyser().run(traces, epochs=epochs, wait=wait)\n\n    def learn_from_feedback(self, feedback, ground_truth=None) -&gt; bool:\n        \"\"\"Manual single-shot learning from last ask() call.\"\"\"\n        ...\n\n    def load(self, path):\n        \"\"\"Load skillbook \u2014 invalidates cached runners (stale refs).\"\"\"\n        self._skillbook = Skillbook.load_from_file(path)\n        self._ace = None\n        self._analyser = None\n</code></pre> <p>When <code>opik=True</code>, <code>ACELiteLLM</code> creates an <code>OpikStep</code> (pipeline-level per-sample tracing) and calls <code>register_opik_litellm_callback()</code> (LiteLLM per-call token/cost tracking). Both tracing modes are activated together because the runner knows it's LiteLLM-backed. The <code>OpikStep</code> is passed to the runners via <code>extra_steps</code> on <code>from_roles()</code>.</p> <p>Runners are cached and invalidated on <code>load()</code> (new skillbook object means stale references). Since runners are reentrant (no per-call instance state), caching is safe.</p>"},{"location":"ACE_DESIGN/#why-not-separate-wrapper-classes","title":"Why not separate wrapper classes","text":"<p>The original design had separate wrapper classes (<code>ACEAgent</code>, <code>ACELangChain</code>, <code>ACEClaudeCode</code>) that delegated to the corresponding runners. This was rejected because:</p> <ul> <li>Two classes for one concept \u2014 users must understand both <code>BrowserUse</code> (the runner) and <code>ACEAgent</code> (the wrapper), and choose which to use.</li> <li>Thin delegation \u2014 the wrappers only added <code>from_model()</code> and a few lifecycle helpers, all of which fit naturally on the runner itself.</li> <li>No added value \u2014 the runner already manages the pipeline, skillbook, and epoch loop. Adding a wrapper just adds indirection.</li> </ul> <p>The exception is <code>ACELiteLLM</code>, which is genuinely different: it wraps two runners, has <code>ask()</code> (direct Agent call, no pipeline), and has <code>learn_from_feedback()</code> (manual single-shot learning). These don't map to any single runner's API.</p>"},{"location":"ACE_DESIGN/#directory-structure","title":"Directory Structure","text":"<pre><code>ace_next/\n  __init__.py               \u2190 Public API re-exports\n  context.py                \u2190 ACEStepContext, SkillbookView, ACESample\n  skill.py                  \u2190 Skill, SimilarityDecision\n  skillbook.py              \u2190 Skillbook\n  updates.py                \u2190 UpdateOperation, UpdateBatch\n  outputs.py                \u2190 AgentOutput, ReflectorOutput, SkillManagerOutput, etc.\n  environments.py           \u2190 Sample, TaskEnvironment, SimpleEnvironment, EnvironmentResult\n  protocols/                \u2190 Role protocols (one file per protocol)\n    __init__.py\n    agent.py                \u2190 AgentLike\n    reflector.py            \u2190 ReflectorLike\n    skill_manager.py        \u2190 SkillManagerLike\n    deduplication.py        \u2190 DeduplicationConfig, DeduplicationManagerLike\n    llm.py                  \u2190 LLMClientLike\n  implementations/          \u2190 Concrete LLM-based role implementations\n    __init__.py             \u2190 Exports Agent, Reflector, SkillManager\n    agent.py                \u2190 Agent (implements AgentLike)\n    reflector.py            \u2190 Reflector (implements ReflectorLike)\n    skill_manager.py        \u2190 SkillManager (implements SkillManagerLike)\n    helpers.py              \u2190 Shared utilities (extract_cited_skill_ids, etc.)\n    prompts.py              \u2190 Default v2.1 prompt templates\n  deduplication/            \u2190 Skill deduplication subsystem\n    __init__.py             \u2190 Exports DeduplicationManager, SimilarityDetector, etc.\n    detector.py             \u2190 SimilarityDetector (embeddings + cosine similarity)\n    manager.py              \u2190 DeduplicationManager (implements DeduplicationManagerLike)\n    operations.py           \u2190 ConsolidationOperation types + apply logic\n    prompts.py              \u2190 Similarity report generation\n  steps/                    \u2190 Pipeline steps (one file per class)\n    __init__.py             \u2190 learning_tail() helper\n    agent.py                \u2190 AgentStep\n    evaluate.py             \u2190 EvaluateStep\n    reflect.py              \u2190 ReflectStep\n    tag.py                  \u2190 TagStep\n    update.py               \u2190 UpdateStep\n    apply.py                \u2190 ApplyStep\n    deduplicate.py          \u2190 DeduplicateStep\n    checkpoint.py           \u2190 CheckpointStep\n    observability.py        \u2190 ObservabilityStep (logger.info)\n    opik.py                 \u2190 OpikStep\n    persist.py              \u2190 PersistStep\n  runners/                    \u2190 Runner classes (compose Pipeline, manage epoch loop)\n    __init__.py               \u2190 Re-exports ACERunner, TraceAnalyser, ACE, BrowserUse, LangChain, ClaudeCode, ACELiteLLM\n    base.py                   \u2190 ACERunner base class\n    trace_analyser.py         \u2190 TraceAnalyser (learning tail only)\n    ace.py                    \u2190 ACE (full adaptive pipeline)\n    browser_use.py            \u2190 BrowserUse runner (from_roles + from_model)\n    langchain.py              \u2190 LangChain runner (from_roles + from_model)\n    claude_code.py            \u2190 ClaudeCode runner (from_roles + from_model)\n    litellm.py                \u2190 ACELiteLLM convenience wrapper (ask + learn + learn_from_traces)\n  integrations/               \u2190 Integration steps (execute + result type + trace converter)\n    __init__.py               \u2190 Exports steps, result types, ToTrace converters, wrap_skillbook_context\n    browser_use.py            \u2190 BrowserExecuteStep, BrowserResult, BrowserToTrace\n    langchain.py              \u2190 LangChainExecuteStep, LangChainResult, LangChainToTrace\n    claude_code.py            \u2190 ClaudeCodeExecuteStep, ClaudeCodeResult, ClaudeCodeToTrace\n  providers/                  \u2190 LLM client wrappers (not pipeline steps)\n    __init__.py               \u2190 Exports LiteLLMClient, InstructorClient, etc.\n    litellm.py                \u2190 LiteLLMClient, LiteLLMConfig, LLMResponse\n    instructor.py             \u2190 InstructorClient, wrap_with_instructor\n    langchain.py              \u2190 LangChainLiteLLMClient (optional: langchain-litellm)\n    claude_code.py            \u2190 ClaudeCodeLLMClient, ClaudeCodeLLMConfig (optional: claude CLI)\n</code></pre> <p>Each integration provides: (1) an execute step, (2) a result type, and (3) a ToTrace converter step. Runners in <code>ace_next/runners/</code> compose these with <code>learning_tail()</code>. For offline analysis, raw trace objects are passed directly to TraceAnalyser.</p>"},{"location":"ACE_DESIGN/#what-moves-where","title":"What moves where","text":"Old location New location Notes <code>ace/adaptation.py</code> Deleted Replaced by <code>ace_next/runners/</code> <code>ace/async_learning.py</code> Deleted Replaced by pipeline engine <code>async_boundary</code> <code>ace/environments.py</code> <code>ace_next/environments.py</code> <code>Sample</code>, <code>EnvironmentResult</code>, <code>TaskEnvironment</code>, <code>SimpleEnvironment</code> (copied) <code>ace/roles.py</code> (protocols) <code>ace_next/protocols/</code> Protocols extracted from role classes <code>ace/roles.py</code> (implementations) <code>ace_next/implementations/</code> Concrete <code>Agent</code>, <code>Reflector</code>, <code>SkillManager</code> classes <code>ace/llm.py</code> (interface) <code>ace_next/protocols/llm.py</code> <code>LLMClientLike</code> protocol <code>ace/prompts_v2_1.py</code> <code>ace_next/implementations/prompts.py</code> v2.1 prompt templates (self-contained copy) <code>ace/deduplication/</code> <code>ace_next/deduplication/</code> Full dedup subsystem (detector, manager, operations, prompts) <code>ace2/</code> Deleted Superseded by this design New <code>ace_next/steps/tag.py</code> TagStep (split from ReflectStep) New <code>ace_next/steps/apply.py</code> ApplyStep (split from UpdateStep) New <code>ace_next/steps/deduplicate.py</code> DeduplicateStep (extracted from SkillManager) New <code>ace_next/steps/checkpoint.py</code> CheckpointStep New <code>ace_next/steps/observability.py</code> ObservabilityStep (logger.info) New <code>ace_next/steps/opik.py</code> OpikStep (Opik trace logging) New <code>ace_next/steps/persist.py</code> PersistStep New <code>ace_next/runners/</code> ACERunner, TraceAnalyser, ACE, BrowserUse, LangChain, ClaudeCode <code>ace/integrations/browser_use.py</code> <code>ace_next/integrations/browser_use.py</code> + <code>ace_next/runners/browser_use.py</code> Split into execute step + result type + ToTrace converter + runner <code>ace/integrations/langchain.py</code> <code>ace_next/integrations/langchain.py</code> + <code>ace_next/runners/langchain.py</code> Split into execute step + result type + ToTrace converter + runner <code>ace/integrations/claude_code.py</code> <code>ace_next/integrations/claude_code.py</code> + <code>ace_next/runners/claude_code.py</code> Split into execute step + result type + ToTrace converter + runner <code>ace/llm_providers/litellm_client.py</code> <code>ace_next/providers/litellm.py</code> Self-contained: <code>LiteLLMClient</code>, <code>LiteLLMConfig</code>, <code>LLMResponse</code> (no ABC) <code>ace/llm_providers/instructor_client.py</code> <code>ace_next/providers/instructor.py</code> Self-contained: <code>InstructorClient</code>, <code>wrap_with_instructor</code> <code>ace/llm_providers/langchain_client.py</code> <code>ace_next/providers/langchain.py</code> Self-contained: <code>LangChainLiteLLMClient</code> (no ABC) <code>ace/llm_providers/claude_code_client.py</code> <code>ace_next/providers/claude_code.py</code> Self-contained: <code>ClaudeCodeLLMClient</code>, <code>ClaudeCodeLLMConfig</code> (no ABC)"},{"location":"ACE_DESIGN/#async-behaviour","title":"Async Behaviour","text":"<p>Both TraceAnalyser and ACE inherit async capabilities from the pipeline engine. No custom async machinery is needed.</p>"},{"location":"ACE_DESIGN/#reflectstep-as-async-boundary","title":"ReflectStep as async boundary","text":"<p><code>ReflectStep.async_boundary = True</code> means: when the pipeline processes a sample, everything before ReflectStep (Agent, Evaluate) runs in the foreground, and everything from ReflectStep onwards (Tag, Update, Apply, Deduplicate, Checkpoint) runs in a background thread pool.</p> <pre><code>sample 1:  [AgentStep] [EvaluateStep] \u2500\u2500fire\u2500\u2500\u25ba [ReflectStep] [TagStep] [UpdateStep] [ApplyStep]  (background)\nsample 2:  [AgentStep] [EvaluateStep] \u2500\u2500fire\u2500\u2500\u25ba [ReflectStep] [TagStep] [UpdateStep] [ApplyStep]  (background)\n                                       \u2191\n                                 async_boundary\n</code></pre> <p>For TraceAnalyser, there is no AgentStep or EvaluateStep in the foreground. The boundary still applies \u2014 context building is foreground, the learning tail is background:</p> <pre><code>trace 1:  [build_context] \u2500\u2500fire\u2500\u2500\u25ba [ReflectStep] [TagStep] [UpdateStep] [ApplyStep]  (background)\ntrace 2:  [build_context] \u2500\u2500fire\u2500\u2500\u25ba [ReflectStep] [TagStep] [UpdateStep] [ApplyStep]  (background)\n</code></pre>"},{"location":"ACE_DESIGN/#controlling-concurrency","title":"Controlling concurrency","text":"Knob Where Effect <code>ReflectStep.max_workers = 3</code> Step class attribute Up to 3 reflections run in parallel <code>TagStep.max_workers = 1</code> Step class attribute Serialises skill tagging <code>UpdateStep.max_workers = 1</code> Step class attribute Serialises skill manager LLM calls <code>ApplyStep.max_workers = 1</code> Step class attribute Serialises skillbook writes <code>wait_for_background(timeout)</code> Runner method Blocks until background threads drain <p>No custom <code>AsyncLearningPipeline</code> class, no manual thread management, no <code>asyncio.create_task</code> for background learning. The pipeline engine handles all of it.</p>"},{"location":"ACE_DESIGN/#error-handling","title":"Error Handling","text":"<p>Follows the pipeline engine's error model without additions.</p> <p>Per-sample isolation: A failing sample does not abort the run. The pipeline catches the exception, records it in <code>SampleResult.error</code> and <code>SampleResult.failed_at</code>, and continues to the next sample.</p> <p>Background failures: Captured and attached to <code>SampleResult</code> by the pipeline engine. The runner calls <code>wait_for_background()</code> at the end to ensure all results are complete.</p> <p>No retry logic in the runner. Retries are the responsibility of individual steps (e.g., LLM call retries via <code>tenacity</code> in the role classes).</p>"},{"location":"ACE_DESIGN/#usage-examples","title":"Usage Examples","text":""},{"location":"ACE_DESIGN/#traceanalyser-learn-from-browser-use-history","title":"TraceAnalyser \u2014 learn from browser-use history","text":"<pre><code>from ace_next import TraceAnalyser, Reflector, SkillManager, LiteLLMClient, wrap_with_instructor\n\nllm = wrap_with_instructor(LiteLLMClient(model=\"gpt-4o-mini\"))\n\n# Raw traces \u2014 plain dicts, no enforced schema\ntraces = [\n    {\n        \"task\": \"Find the cheapest flight to Tokyo\",\n        \"output\": \"$450 on ANA, departing March 15\",\n        \"feedback\": \"Correct price found in 8 steps\",\n        \"reasoning\": \"Step 1: Navigate to Google Flights...\",\n    },\n    {\n        \"task\": \"Book a hotel in Shibuya\",\n        \"output\": \"Failed: could not find checkout button\",\n        \"feedback\": \"Task failed after 15 steps \u2014 checkout button was behind a cookie modal\",\n        \"reasoning\": \"Step 1: Navigate to Booking.com...\",\n    },\n]\n\n# Analyse \u2014 raw traces go directly to the Reflector via ctx.trace\nanalyser = TraceAnalyser.from_roles(reflector=Reflector(llm), skill_manager=SkillManager(llm))\nresults = analyser.run(traces, epochs=2)\nanalyser.save(\"travel_agent.json\")\n</code></pre>"},{"location":"ACE_DESIGN/#ace-live-qa-training","title":"ACE \u2014 live Q&amp;A training","text":"<pre><code>from ace_next import ACE, Sample, SimpleEnvironment, Agent, Reflector, SkillManager\nfrom ace_next import LiteLLMClient, wrap_with_instructor\n\nllm = wrap_with_instructor(LiteLLMClient(model=\"gpt-4o-mini\"))\n\nsamples = [\n    Sample(question=\"Capital of France?\", ground_truth=\"Paris\"),\n    Sample(question=\"Largest ocean?\", ground_truth=\"Pacific\"),\n]\n\n# Environment provided at construction \u2014 EvaluateStep uses it to generate feedback\nace = ACE.from_roles(\n    agent=Agent(llm),\n    reflector=Reflector(llm),\n    skill_manager=SkillManager(llm),\n    environment=SimpleEnvironment(),\n)\nresults = ace.run(samples, epochs=3)\nace.save(\"geography.json\")\n</code></pre>"},{"location":"ACE_DESIGN/#ace-without-environment","title":"ACE \u2014 without environment","text":"<pre><code># No environment \u2014 trace still contains agent output + ground truth\n# The Reflector learns from ground-truth comparison directly\nace = ACE.from_roles(\n    agent=Agent(llm),\n    reflector=Reflector(llm),\n    skill_manager=SkillManager(llm),\n)\nresults = ace.run(samples, epochs=3)\n</code></pre>"},{"location":"ACE_DESIGN/#ace-single-pass-with-iterable","title":"ACE \u2014 single-pass with iterable","text":"<pre><code># Any Iterable works with epochs=1 (consumed once, not replayed)\nsamples = load_samples_from_csv(\"eval_set.csv\")  # returns a list or generator\n\nace = ACE.from_roles(agent=Agent(llm), reflector=Reflector(llm), skill_manager=SkillManager(llm))\nresults = ace.run(samples, epochs=1)\n</code></pre>"},{"location":"ACE_DESIGN/#ace-with-checkpoints-and-deduplication","title":"ACE \u2014 with checkpoints and deduplication","text":"<pre><code>from ace_next import ACE, Agent, Reflector, SkillManager, SimpleEnvironment\nfrom ace_next.deduplication import DeduplicationManager\nfrom ace_next.protocols.deduplication import DeduplicationConfig\n\nace = ACE.from_roles(\n    agent=Agent(llm),\n    reflector=Reflector(llm),\n    skill_manager=SkillManager(llm),\n    environment=SimpleEnvironment(),\n    dedup_manager=DeduplicationManager(DeduplicationConfig(similarity_threshold=0.85)),\n    checkpoint_dir=\"./checkpoints\",\n    checkpoint_interval=10,\n)\n# Pipeline: Agent \u2192 Evaluate \u2192 Reflect \u2192 Tag \u2192 Update \u2192 Apply \u2192 Deduplicate \u2192 Checkpoint\nresults = ace.run(samples, epochs=3)\n</code></pre>"},{"location":"ACE_DESIGN/#integration-browser-use-runner","title":"Integration \u2014 browser-use runner","text":"<pre><code>from ace_next import BrowserUse, Reflector, SkillManager, LiteLLMClient\nfrom langchain_openai import ChatOpenAI\n\nllm = LiteLLMClient(model=\"gpt-4o-mini\")\nbrowser_llm = ChatOpenAI(model=\"gpt-4o\")\n\n# Explicit construction \u2014 bring your own roles\nrunner = BrowserUse.from_roles(\n    browser_llm=browser_llm,\n    reflector=Reflector(llm),\n    skill_manager=SkillManager(llm),\n)\n\n# Or convenience construction \u2014 just specify the model\nrunner = BrowserUse.from_model(browser_llm, ace_model=\"gpt-4o-mini\")\n\n# Live execution + learning\nresults = runner.run([\"Find top HN post\", \"Check weather in Tokyo\"])\nrunner.save(\"browser_expert.json\")\n</code></pre>"},{"location":"ACE_DESIGN/#integration-langchain-runner-from_model","title":"Integration \u2014 LangChain runner (from_model)","text":"<pre><code>from ace_next import LangChain\nfrom langchain_openai import ChatOpenAI\nfrom langchain_core.prompts import ChatPromptTemplate\n\nchain = ChatPromptTemplate.from_template(\"Answer: {input}\") | ChatOpenAI(model=\"gpt-4o\")\n\n# One-liner construction\nrunner = LangChain.from_model(chain, ace_model=\"gpt-4o-mini\")\nresults = runner.run([{\"input\": \"What is ACE?\"}, {\"input\": \"Explain skillbooks\"}])\nrunner.save(\"chain_expert.json\")\n</code></pre>"},{"location":"ACE_DESIGN/#integration-claude-code-runner-from_model","title":"Integration \u2014 Claude Code runner (from_model)","text":"<pre><code>from ace_next import ClaudeCode\n\nrunner = ClaudeCode.from_model(working_dir=\"./my_project\", ace_model=\"gpt-4o-mini\")\nresults = runner.run([\"Add unit tests for utils.py\", \"Refactor the auth module\"])\nrunner.save(\"code_expert.json\")\n</code></pre>"},{"location":"ACE_DESIGN/#acelitellm-conversational-agent-with-learning","title":"ACELiteLLM \u2014 conversational agent with learning","text":"<pre><code>from ace_next import ACELiteLLM, SimpleEnvironment, Sample\n\nace = ACELiteLLM.from_model(\"gpt-4o-mini\")\n\n# Direct Q&amp;A (no pipeline)\nanswer = ace.ask(\"What is the capital of France?\")\n\n# Batch learning (delegates to ACE runner)\nsamples = [\n    Sample(question=\"Capital of France?\", ground_truth=\"Paris\"),\n    Sample(question=\"Largest ocean?\", ground_truth=\"Pacific\"),\n]\nace.learn(samples, environment=SimpleEnvironment(), epochs=3)\n\n# Manual feedback learning from last ask()\nace.ask(\"What is 2+2?\")\nace.learn_from_feedback(\"The answer should be 4\", ground_truth=\"4\")\n\nace.save(\"learned.json\")\n\n# With Opik observability (explicit opt-in)\nace = ACELiteLLM.from_model(\"gpt-4o-mini\", opik=True, opik_project=\"my-project\")\n\n# With Recursive Reflector + Opik\nfrom ace_next import RRStep, RRConfig, LiteLLMClient\nllm = LiteLLMClient(model=\"gpt-4o-mini\")\nrr = RRStep(llm, config=RRConfig(max_iterations=10))\nace = ACELiteLLM(llm, reflector=rr, opik=True)\n</code></pre>"},{"location":"ACE_DESIGN/#fire-and-forget-get-results-while-learning-continues","title":"Fire-and-forget \u2014 get results while learning continues","text":"<pre><code>ace = ACE.from_roles(\n    agent=Agent(llm),\n    reflector=Reflector(llm),\n    skill_manager=SkillManager(llm),\n)\n\n# wait=False: returns after foreground steps (Agent + Evaluate)\n# Background learning (Reflect \u2192 Tag \u2192 Update \u2192 Apply) continues\nresults = ace.run(samples, epochs=1, wait=False)\n\n# Use agent outputs immediately\nfor r in results:\n    print(r.output.agent_output.final_answer)\n\n# Check learning progress\nprint(ace.learning_stats)\n# {\"active\": 3, \"completed\": 12}\n\n# Block when you need the skillbook finalised\nace.wait_for_background(timeout=60.0)\nace.save(\"learned.json\")\n</code></pre>"},{"location":"ACE_DESIGN/#mixed-workflow-batch-then-live","title":"Mixed workflow \u2014 batch then live","text":"<pre><code>from ace_next import TraceAnalyser, ACE, Skillbook\nfrom ace_next.implementations import Agent, Reflector, SkillManager\n\nreflector = Reflector(llm)\nskill_manager = SkillManager(llm)\n\n# Phase 1: build skillbook from historical traces\nskillbook = Skillbook()\nanalyser = TraceAnalyser.from_roles(\n    reflector=reflector,\n    skill_manager=skill_manager,\n    skillbook=skillbook,\n)\nanalyser.run(historical_traces, epochs=3)\n\n# Phase 2: deploy with live learning (reuse the evolved skillbook)\nace = ACE.from_roles(\n    agent=Agent(llm),\n    reflector=reflector,\n    skill_manager=skill_manager,\n    skillbook=skillbook,\n)\nace.run(live_samples, epochs=1)\nace.save(\"production.json\")\n</code></pre>"},{"location":"ACE_DESIGN/#potential-improvements","title":"Potential Improvements","text":"<p>Issues acknowledged but deferred from this version of the spec.</p> <p>Streaming / lazy iteration: <code>_run()</code> eagerly materializes the full iterable into a list of <code>ACEStepContext</code> objects before passing them to <code>Pipeline.run()</code>. For a large generator with <code>epochs=1</code>, the entire input gets buffered into memory. True streaming would require the pipeline to accept an iterator and process items one-at-a-time (e.g., <code>for ctx in contexts: pipeline.run_one(ctx)</code>), or an async iterator pattern with <code>asyncio.as_completed</code>. This is a deliberate simplification \u2014 batch materialization keeps the epoch loop and error handling straightforward. Revisit if memory pressure from large single-pass runs becomes a real problem.</p> <p>Builder API for custom pipelines (speculative): The current API offers two extremes: factory methods (<code>from_roles</code>) that hide the pipeline entirely, and manual <code>Pipeline([...])</code> construction that requires understanding <code>ACEStepContext</code>, <code>SkillbookView</code>, step contracts, and <code>ACERunner</code> subclassing. Users who want to insert a custom step between Reflect and Update, or swap the execute head while keeping the learning tail, fall into a gap where neither approach serves them well.</p> <p>One possible direction is a builder API that would bridge this gap:</p> <pre><code>from ace import ACEBuilder\n\n# Start from a preset, customise from there\nace = (\n    ACEBuilder(model=\"gpt-4o-mini\")\n    .execute(MyCustomExecuteStep(my_agent))     # replace the execute head\n    .add_step(MyValidationStep(), after=\"reflect\")  # insert custom step\n    .deduplicate(similarity_threshold=0.85)\n    .checkpoint(\"./checkpoints\", interval=10)\n    .build()\n)\nresults = ace.run(samples, environment)\n\n# Or build from the standard ACE preset and tweak\nace = (\n    ACEBuilder.from_preset(\"ace\", model=\"gpt-4o-mini\")\n    .add_step(MyLoggingStep(), after=\"apply\")\n    .build()\n)\n</code></pre> <p>The builder would handle <code>SkillbookView</code> wiring, step ordering validation, and <code>ACERunner</code> construction internally. Users compose by name (\"reflect\", \"apply\") rather than by importing step classes.</p> <p>This would only be worth pursuing when there is evidence of users building custom pipelines with <code>learning_tail()</code> and hitting friction with the manual wiring. The <code>learning_tail()</code> helper (see Integration Pattern section) covers the most common customisation \u2014 custom execute step + standard learning \u2014 without a builder. A builder adds value when users need fine-grained insertion points (between existing steps) or want to compose from presets without understanding the step internals. The main risk is that a builder mirrors the step list, adding a second construction path to document, test, and keep in sync. It can also hide the <code>requires</code>/<code>provides</code> contracts \u2014 when a validation step is inserted at the wrong position, the error comes from the pipeline engine (field missing) rather than the builder (wrong position name), making debugging indirect. Mitigate by having the builder validate the final step chain at <code>build()</code> time and surfacing clear errors.</p> <p>Skillbook rollback and versioning: Currently the skillbook is mutated in place with no way to undo a bad update. If the LLM hallucinates a harmful skill or a batch degrades overall quality, the only recovery is restoring from a checkpoint file. A lightweight versioning mechanism \u2014 e.g., snapshotting skillbook state at epoch boundaries or before each <code>ApplyStep</code>, with a <code>rollback(to_version)</code> method \u2014 would enable automatic revert when a validation metric degrades, A/B comparison between skillbook versions, and safer experimentation with aggressive learning rates. This could live as a <code>VersionedSkillbook</code> wrapper or as an optional <code>SnapshotStep</code> inserted before <code>ApplyStep</code>. Deferred because the current checkpoint-to-disk approach covers the most common recovery scenario (resume after crash), and in-memory versioning adds memory overhead proportional to skillbook size times number of snapshots.</p>"},{"location":"ACE_DESIGN/#what-was-rejected-and-why","title":"What Was Rejected and Why","text":"<p>Runner extends Pipeline: Making TraceAnalyser and ACE subclasses of <code>Pipeline</code> was considered. Rejected \u2014 the runner is not a pipeline. It owns the epoch loop. Composition (<code>self.pipeline</code>) keeps responsibilities separate.</p> <p>Cross-sample state (reflection window): A rolling window of recent reflections that persists across samples was considered, with variants: on the runner, on <code>StepContext</code>, on step instances, via a shared mediator object. All rejected \u2014 each sample should be independent. The only cross-sample coupling is the skillbook itself, which evolves as samples are processed. Adding a reflection window complicates the model (reset between epochs, eventual consistency with background steps, ordering issues with concurrent workers) for marginal benefit.</p> <p>Separate Online and Offline classes: Keeping two runner classes for single-pass and multi-epoch was considered. Rejected \u2014 the only difference is <code>epochs=1</code> vs <code>epochs &gt; 1</code>, which is a parameter, not a class distinction. ACE handles both. TraceAnalyser is a separate class because its input type is fundamentally different (raw traces vs <code>Sample + Environment</code>), not because of epoch count.</p> <p>Structured Trace dataclass: A <code>@dataclass Trace</code> with typed fields (<code>task</code>, <code>output</code>, <code>feedback</code>, <code>reasoning</code>, etc.) was considered. Rejected \u2014 it imposes a schema on trace data that doesn't match reality. External frameworks produce wildly different trace shapes (browser-use <code>AgentHistoryList</code>, LangChain result dicts, Claude Code transcripts). Forcing them through a common dataclass means either losing information (fields that don't map) or adding catch-all <code>metadata: dict</code> buckets that defeat the purpose of typing. Instead, <code>ctx.trace</code> is <code>object | None</code> \u2014 the raw trace as-is. The Reflector receives it directly and is responsible for making sense of it. This gives maximum flexibility for analysis without constraining trace format. Extraction helpers (converter functions, typed intermediate representations) can be layered on later if needed.</p> <p>Steps that accept both traces and samples: Making ReflectStep and UpdateStep polymorphic over input type was considered. Rejected \u2014 steps always receive <code>StepContext</code> with the same named fields. The runner (via <code>_build_context</code>) is responsible for building the context correctly. Steps do not need to know whether the data came from a raw trace or from live execution.</p> <p>Observability in the runner: Keeping observability logic in <code>ACERunner._track_observability_data()</code> was considered. Rejected \u2014 it mixes concerns. A dedicated <code>OpikStep</code> is independently testable, optional, and composable. It is not wired into <code>learning_tail()</code> \u2014 users append it explicitly to avoid coupling observability into the core pipeline.</p> <p>Custom AsyncLearningPipeline: The legacy <code>ace/async_learning.py</code> implements a manual thread pool with reflector and skill manager queues. Rejected \u2014 the pipeline engine's <code>async_boundary</code> and <code>max_workers</code> provide the same functionality with less code and consistent semantics.</p> <p>Per-integration pipeline classes: Having each integration define its own pipeline class was considered. Rejected \u2014 every integration pipeline has the same learning tail; only the execute step differs. Separate pipeline classes duplicate the learning tail wiring and the runner infrastructure. Instead, integrations provide execute steps that compose into an <code>ACERunner</code> subclass, reusing the shared <code>_run()</code> loop and epoch logic.</p> <p>Checkpoints in the runner: Having the runner own checkpoint logic (via <code>run()</code> parameters) was considered. Rejected \u2014 a <code>CheckpointStep</code> at the end of the pipeline tail keeps checkpointing within the pipeline formalism. Checkpoint configuration belongs at construction time (factory methods), not at call time (<code>run()</code>).</p> <p>Mutable Skillbook directly on the context: Storing the real <code>Skillbook</code> as a field on <code>ACEStepContext</code> was the initial design. Rejected \u2014 <code>StepContext</code> is frozen, but <code>Skillbook</code> is a mutable object. Placing it on the context creates the illusion of immutability while allowing any step to mutate shared state through the reference. Instead, the context carries a <code>SkillbookView</code> (read-only projection) that exposes only read methods (<code>as_prompt()</code>, <code>get_skill()</code>, <code>__len__</code>). Write methods don't exist on the view \u2014 calling them raises <code>AttributeError</code> at runtime and a type error at check time. Steps that need to write (TagStep, ApplyStep, DeduplicateStep, CheckpointStep) receive the real <code>Skillbook</code> via constructor injection. This gives us both: pipeline engine validation (skillbook is in <code>requires</code>/<code>provides</code>) and true immutability enforcement on the context.</p> <p>Combined Reflect+Tag and Update+Apply steps: Keeping ReflectStep as both reflection and tagging, and UpdateStep as both generation and application was considered. Rejected \u2014 each combination mixes a pure function (LLM call producing output) with a side effect (skillbook mutation). Splitting them means pure steps can be tested without a skillbook, side-effect steps can be tested without an LLM, and concerns are cleanly separated.</p> <p>Instructor auto-wrapping in implementations: The old <code>ace/roles.py</code> auto-wrapped LLM clients with Instructor if <code>complete_structured</code> was missing (duck-typing check + fallback). This has since been updated: <code>ace/roles.py</code> now checks <code>INSTRUCTOR_AVAILABLE</code> and gracefully falls back to the raw LLM if the <code>instructor</code> package is not installed (it is an optional dependency via <code>pip install ace-framework[instructor]</code>). Rejected for <code>ace_next</code> \u2014 auto-wrapping masks what the implementation actually requires. In <code>ace_next</code>, <code>LLMClientLike</code> explicitly requires both <code>complete()</code> and <code>complete_structured()</code>. Callers wrap their LLM clients before passing them in (e.g. <code>wrap_with_instructor(LiteLLMClient(...))</code> from <code>ace_next.providers</code>). This makes the requirement visible at the call site and keeps implementations dependency-free.</p> <p>Recursive Reflector (initial rejection, now implemented): The old <code>ace/reflector/</code> subsystem supports recursive mode where the Reflector iterates multiple times to deepen analysis. Initially rejected for <code>ace_next</code> due to complexity. Now implemented as <code>RRStep</code> in <code>ace_next/rr/</code> \u2014 a <code>SubRunner</code>-based step that runs an iterative REPL loop (LLM call \u2192 extract code \u2192 sandbox exec \u2192 check result). <code>RRStep</code> satisfies both <code>StepProtocol</code> (composable in any pipeline) and <code>ReflectorLike</code> (usable as a drop-in reflector, e.g. <code>ACELiteLLM(llm, reflector=rr)</code>). Exported from <code>ace_next</code> as <code>RRStep</code> and <code>RRConfig</code>.</p> <p>Observability decorator on implementations: The old <code>ace/roles.py</code> uses <code>@maybe_track()</code> decorators for Opik tracing on every role method. Rejected \u2014 <code>OpikStep</code> handles metrics at the pipeline level with full visibility into all context fields. Adding per-method decorators would double-count and create coupling between implementations and the observability system.</p> <p>Deduplication inside SkillManager: The old <code>ace/roles.py</code> SkillManager integrates with <code>DeduplicationManager</code> directly \u2014 calling <code>get_similarity_report()</code> before the LLM call and <code>apply_operations_from_response()</code> after. Rejected for <code>ace_next</code> \u2014 deduplication is now a separate <code>DeduplicateStep</code> in the pipeline. This is cleaner separation: the SkillManager role only produces <code>SkillManagerOutput</code>, and deduplication runs at a configurable interval as an independent pipeline step. The step takes a <code>DeduplicationManagerLike</code> protocol, keeping it decoupled from the concrete implementation.</p> <p>Shared <code>ace_next/features.py</code> module: Creating a centralized feature detection module (like <code>ace/features.py</code>) for optional dependency checks was considered. Rejected \u2014 the only code that needs feature detection is <code>deduplication/detector.py</code>, which uses a local <code>_has(module)</code> helper with <code>importlib.import_module</code>. A shared module would add a file for a single 4-line function. If more code needs feature detection in the future, the helper can be promoted to a shared location.</p> <p>Separate wrapper classes for integration runners: Having separate convenience classes (<code>ACEAgent</code>, <code>ACELangChain</code>, <code>ACEClaudeCode</code>) that wrap the corresponding runners was the initial design. Each wrapper eagerly built a runner via <code>from_roles()</code> and delegated all calls to it. Rejected \u2014 the wrappers only added <code>from_model()</code> and a few lifecycle helpers (<code>get_strategies()</code>, backward-compat aliases), all of which fit naturally as methods on the runner class itself. Two classes for one concept forces users to understand both and choose which to use, while the runner already manages the pipeline, skillbook, and epoch loop. Instead, <code>from_model()</code> and convenience methods are defined directly on <code>BrowserUse</code>, <code>LangChain</code>, and <code>ClaudeCode</code>. The exception is <code>ACELiteLLM</code>, which is genuinely different: it wraps two runners (<code>ACE</code> and <code>TraceAnalyser</code>), has <code>ask()</code> (direct Agent call, no pipeline), and has <code>learn_from_feedback()</code> (manual single-shot learning). These don't map to any single runner's API.</p>"},{"location":"API_REFERENCE/","title":"\ud83d\udcda ACE Framework API Reference","text":"<p>Complete API documentation for the ACE Framework.</p>"},{"location":"API_REFERENCE/#core-components","title":"Core Components","text":""},{"location":"API_REFERENCE/#agent","title":"Agent","text":"<p>The Agent produces answers using the current skillbook of strategies.</p> <pre><code>from ace import Agent, LiteLLMClient\n\nclient = LiteLLMClient(model=\"gpt-4\")\nagent = Agent(client)\n\noutput = agent.generate(\n    question=\"What is 2+2?\",\n    context=\"Show your work\",\n    skillbook=skillbook,\n    reflection=None  # Optional reflection from previous attempt\n)\n\n# Output contains:\n# - output.final_answer: The generated answer\n# - output.reasoning: Step-by-step reasoning\n# - output.skill_ids: List of skillbook strategies used\n</code></pre>"},{"location":"API_REFERENCE/#reflector","title":"Reflector","text":"<p>The Reflector analyzes what went right or wrong and tags which strategies helped or hurt.</p> <pre><code>from ace import Reflector\n\nreflector = Reflector(client)\n\nreflection = reflector.reflect(\n    question=\"What is 2+2?\",\n    agent_output=output,\n    skillbook=skillbook,\n    ground_truth=\"4\",\n    feedback=\"Correct!\"\n)\n\n# Reflection contains:\n# - reflection.reasoning: Analysis of the outcome\n# - reflection.error_identification: What went wrong (if anything)\n# - reflection.root_cause_analysis: Why it went wrong\n# - reflection.correct_approach: What should have been done\n# - reflection.key_insight: Main lesson learned\n# - reflection.skill_tags: List of (skill_id, tag) pairs\n</code></pre>"},{"location":"API_REFERENCE/#skillmanager","title":"SkillManager","text":"<p>The SkillManager transforms reflections into skillbook updates.</p> <pre><code>from ace import SkillManager\n\nskill_manager = SkillManager(client)\n\nskill_manager_output = skill_manager.update_skills(\n    reflection=reflection,\n    skillbook=skillbook,\n    question_context=\"Math problems\",\n    progress=\"3/5 correct\"\n)\n\n# Apply the updates\nskillbook.apply_update(skill_manager_output.update)\n</code></pre>"},{"location":"API_REFERENCE/#skillbook-management","title":"Skillbook Management","text":""},{"location":"API_REFERENCE/#creating-a-skillbook","title":"Creating a Skillbook","text":"<pre><code>from ace import Skillbook\n\nskillbook = Skillbook()\n\n# Add a strategy\nskill = skillbook.add_skill(\n    section=\"Math Strategies\",\n    content=\"Break complex problems into smaller steps\",\n    metadata={\"helpful\": 5, \"harmful\": 0, \"neutral\": 1}\n)\n</code></pre>"},{"location":"API_REFERENCE/#saving-and-loading","title":"Saving and Loading","text":"<pre><code># Save to file\nskillbook.save_to_file(\"my_strategies.json\")\n\n# Load from file\nloaded_skillbook = Skillbook.load_from_file(\"my_strategies.json\")\n</code></pre>"},{"location":"API_REFERENCE/#skillbook-statistics","title":"Skillbook Statistics","text":"<pre><code>stats = skillbook.stats()\n# Returns:\n# {\n#   \"sections\": 3,\n#   \"skills\": 15,\n#   \"tags\": {\n#     \"helpful\": 45,\n#     \"harmful\": 5,\n#     \"neutral\": 10\n#   }\n# }\n</code></pre>"},{"location":"API_REFERENCE/#skill-deduplication","title":"Skill Deduplication","text":"<p>Optional feature to detect and consolidate similar skills using embeddings.</p>"},{"location":"API_REFERENCE/#deduplicationconfig","title":"DeduplicationConfig","text":"<pre><code>from ace import DeduplicationConfig\n\nconfig = DeduplicationConfig(\n    enabled=True,                              # Default: True\n    embedding_model=\"text-embedding-3-small\",  # OpenAI embedding\n    similarity_threshold=0.85,                 # Pairs above this are similar\n    within_section_only=True                   # Compare within same section\n)\n\n# Use with any integration\nfrom ace import ACELiteLLM\nagent = ACELiteLLM(model=\"gpt-4o-mini\", dedup_config=config)\n</code></pre>"},{"location":"API_REFERENCE/#adapters","title":"Adapters","text":""},{"location":"API_REFERENCE/#offlineace","title":"OfflineACE","text":"<p>Train on a batch of samples.</p> <pre><code>from ace import OfflineACE\nfrom ace import Sample\n\nadapter = OfflineACE(\n    agent=agent,\n    reflector=reflector,\n    skill_manager=skill_manager\n)\n\nsamples = [\n    Sample(\n        question=\"What is 2+2?\",\n        context=\"Calculate\",\n        ground_truth=\"4\"\n    ),\n    # More samples...\n]\n\nresults = adapter.run(\n    samples=samples,\n    environment=environment,\n    epochs=3,\n    verbose=True\n)\n</code></pre>"},{"location":"API_REFERENCE/#onlineace","title":"OnlineACE","text":"<p>Learn from tasks one at a time.</p> <pre><code>from ace import OnlineACE\n\nadapter = OnlineACE(\n    skillbook=existing_skillbook,\n    agent=agent,\n    reflector=reflector,\n    skill_manager=skill_manager\n)\n\nfor task in tasks:\n    result = adapter.process(task, environment)\n    # Skillbook updates automatically after each task\n</code></pre>"},{"location":"API_REFERENCE/#integrations","title":"Integrations","text":"<p>ACE provides ready-to-use integrations with popular agentic frameworks. These classes wrap external agents with ACE learning capabilities.</p>"},{"location":"API_REFERENCE/#acelitellm","title":"ACELiteLLM","text":"<p>Quick-start integration for simple conversational agents.</p> <pre><code>from ace import ACELiteLLM\n\n# Create an ACE-powered conversational agent\nagent = ACELiteLLM(model=\"gpt-4o-mini\")\n\n# Ask questions - agent learns from each interaction\nanswer1 = agent.ask(\"What is the capital of France?\")\nanswer2 = agent.ask(\"What about Spain?\")\n\n# Save learned strategies\nagent.skillbook.save_to_file(\"learned_strategies.json\")\n\n# Load and continue learning\nagent2 = ACELiteLLM(model=\"gpt-4o-mini\", skillbook_path=\"learned_strategies.json\")\n</code></pre> <p>Parameters: - <code>model</code>: LiteLLM model identifier (e.g., \"gpt-4o-mini\", \"claude-3-5-sonnet\") - <code>skillbook</code>: Optional existing Skillbook to start with - <code>ace_model</code>: Model for Reflector/SkillManager (defaults to same as main model) - <code>**llm_kwargs</code>: Additional arguments passed to LiteLLMClient</p>"},{"location":"API_REFERENCE/#aceagent-browser-use","title":"ACEAgent (browser-use)","text":"<p>Self-improving browser automation agent.</p> <pre><code>from ace import ACEAgent\nfrom browser_use import ChatBrowserUse\n\n# Create browser agent\nllm = ChatBrowserUse(model=\"gpt-4o\")\nagent = ACEAgent(llm=llm)\n\n# Run browser tasks - learns from successes and failures\nawait agent.run(task=\"Find the top post on Hacker News\")\nawait agent.run(task=\"Search for ACE framework on GitHub\")\n\n# Skillbook improves with each task\nprint(f\"Learned {len(agent.skillbook.skills())} strategies\")\n</code></pre> <p>Parameters: - <code>llm</code>: Browser-use ChatBrowserUse instance - <code>skillbook</code>: Optional existing Skillbook - <code>ace_model</code>: Model for learning (defaults to \"gpt-4o-mini\")</p> <p>Requires: <code>pip install browser-use</code> (optional dependency)</p>"},{"location":"API_REFERENCE/#acelangchain","title":"ACELangChain","text":"<p>Wrap LangChain chains and agents with ACE learning.</p> <pre><code>from ace import ACELangChain\nfrom langchain_openai import ChatOpenAI\nfrom langchain.chains import LLMChain\nfrom langchain.prompts import PromptTemplate\n\n# Create LangChain chain\nllm = ChatOpenAI(temperature=0)\nprompt = PromptTemplate.from_template(\"Answer this question: {question}\")\nchain = LLMChain(llm=llm, prompt=prompt)\n\n# Wrap with ACE\nace_chain = ACELangChain(runnable=chain)\n\n# Use like normal LangChain - but with learning!\nresult1 = ace_chain.invoke({\"question\": \"What is 2+2?\"})\nresult2 = ace_chain.invoke({\"question\": \"What is 10*5?\"})\n\n# Access learned skillbook\nace_chain.save_skillbook(\"langchain_learned.json\")\n</code></pre> <p>Parameters: - <code>runnable</code>: Any LangChain Runnable (chains, agents, etc.) - <code>skillbook</code>: Optional existing Skillbook - <code>ace_model</code>: Model for learning (defaults to \"gpt-4o-mini\") - <code>environment</code>: Custom evaluation environment (optional)</p> <p>Requires: <code>pip install ace-framework[langchain]</code></p> <p>See also: Integration Guide for advanced patterns and custom integrations.</p>"},{"location":"API_REFERENCE/#environments","title":"Environments","text":""},{"location":"API_REFERENCE/#creating-environments","title":"Creating Environments","text":"<p>All environments should extend the <code>TaskEnvironment</code> base class.</p>"},{"location":"API_REFERENCE/#simple-environment-example","title":"Simple Environment Example","text":"<p>Basic environment that compares output to ground truth using substring matching:</p> <pre><code>from ace import TaskEnvironment, EnvironmentResult\n\nclass SimpleEnvironment(TaskEnvironment):\n    \"\"\"Basic environment for testing - checks if ground truth appears in answer.\"\"\"\n\n    def evaluate(self, sample, agent_output):\n        # Simple substring matching (case-insensitive)\n        correct = str(sample.ground_truth).lower() in str(agent_output.final_answer).lower()\n\n        return EnvironmentResult(\n            feedback=\"Correct!\" if correct else \"Incorrect\",\n            ground_truth=sample.ground_truth,\n        )\n\n# Usage\nenv = SimpleEnvironment()\nresult = env.evaluate(sample, agent_output)\n</code></pre>"},{"location":"API_REFERENCE/#custom-environments","title":"Custom Environments","text":"<pre><code>from ace import TaskEnvironment, EnvironmentResult\n\nclass CodeEnvironment(TaskEnvironment):\n    def evaluate(self, sample, output):\n        # Run the code\n        success = execute_code(output.final_answer)\n\n        return EnvironmentResult(\n            feedback=\"Tests passed\" if success else \"Tests failed\",\n            ground_truth=sample.ground_truth,\n            metrics={\"pass_rate\": 1.0 if success else 0.0}\n        )\n</code></pre>"},{"location":"API_REFERENCE/#llm-clients","title":"LLM Clients","text":""},{"location":"API_REFERENCE/#litellmclient","title":"LiteLLMClient","text":"<p>Support for 100+ LLM providers.</p> <pre><code>from ace import LiteLLMClient\n\n# Basic usage\nclient = LiteLLMClient(model=\"gpt-4\")\n\n# With configuration\nclient = LiteLLMClient(\n    model=\"gpt-4\",\n    temperature=0.7,\n    max_tokens=1000,\n    fallbacks=[\"claude-3-haiku\", \"gpt-3.5-turbo\"]\n)\n\n# Generate completion\nresponse = client.complete(\"What is the meaning of life?\")\nprint(response.text)\n</code></pre>"},{"location":"API_REFERENCE/#langchainlitellmclient","title":"LangChainLiteLLMClient","text":"<p>Integration with LangChain.</p> <pre><code>from ace.llm_providers import LangChainLiteLLMClient\n\nclient = LangChainLiteLLMClient(\n    model=\"gpt-4\",\n    tags=[\"production\"],\n    metadata={\"user\": \"alice\"}\n)\n</code></pre>"},{"location":"API_REFERENCE/#types","title":"Types","text":""},{"location":"API_REFERENCE/#sample","title":"Sample","text":"<pre><code>from ace import Sample\n\nsample = Sample(\n    question=\"Your question here\",\n    context=\"Optional context or requirements\",\n    ground_truth=\"Expected answer (optional)\"\n)\n</code></pre>"},{"location":"API_REFERENCE/#agentoutput","title":"AgentOutput","text":"<pre><code>@dataclass\nclass AgentOutput:\n    reasoning: str\n    final_answer: str\n    skill_ids: List[str]\n    raw: Dict[str, Any]\n</code></pre>"},{"location":"API_REFERENCE/#reflectoroutput","title":"ReflectorOutput","text":"<pre><code>@dataclass\nclass ReflectorOutput:\n    reasoning: str\n    error_identification: str\n    root_cause_analysis: str\n    correct_approach: str\n    key_insight: str\n    skill_tags: List[SkillTag]\n    raw: Dict[str, Any]\n</code></pre>"},{"location":"API_REFERENCE/#environmentresult","title":"EnvironmentResult","text":"<pre><code>@dataclass\nclass EnvironmentResult:\n    feedback: str\n    ground_truth: Optional[str] = None\n    metrics: Optional[Dict[str, float]] = None\n</code></pre>"},{"location":"API_REFERENCE/#update-operations","title":"Update Operations","text":""},{"location":"API_REFERENCE/#updateoperation-types","title":"UpdateOperation Types","text":"<ul> <li><code>ADD</code>: Add new skill to skillbook</li> <li><code>UPDATE</code>: Update existing skill content</li> <li><code>TAG</code>: Update helpful/harmful/neutral counts</li> <li><code>REMOVE</code>: Remove skill from skillbook</li> </ul> <pre><code>from ace.updates import UpdateOperation\n\nop = UpdateOperation(\n    type=\"ADD\",\n    section=\"Math Strategies\",\n    content=\"Always check your work\",\n    skill_id=\"math-00001\"\n)\n</code></pre>"},{"location":"API_REFERENCE/#prompts","title":"Prompts","text":""},{"location":"API_REFERENCE/#using-default-prompts","title":"Using Default Prompts","text":"<pre><code>from ace.prompts import AGENT_PROMPT, REFLECTOR_PROMPT, SKILL_MANAGER_PROMPT\n\nagent = Agent(client, prompt_template=AGENT_PROMPT)\n</code></pre>"},{"location":"API_REFERENCE/#using-v21-prompts-recommended","title":"Using v2.1 Prompts (Recommended)","text":"<p>ACE v2.1 prompts show +17% success rate improvement vs v1.0.</p> <pre><code>from ace.prompts_v2_1 import PromptManager\n\nmanager = PromptManager(default_version=\"2.1\")\n\nagent = Agent(\n    client,\n    prompt_template=manager.get_agent_prompt(domain=\"math\")\n)\n</code></pre> <p>Note: v2.0 prompts (<code>ace.prompts_v2</code>) are deprecated. Use v2.1 for best performance.</p>"},{"location":"API_REFERENCE/#custom-prompts","title":"Custom Prompts","text":"<pre><code>custom_prompt = '''\nSkillbook: {skillbook}\nQuestion: {question}\nContext: {context}\n\nGenerate a JSON response with:\n- reasoning: Your step-by-step thought process\n- skill_ids: List of skillbook IDs you used\n- final_answer: Your answer\n'''\n\nagent = Agent(client, prompt_template=custom_prompt)\n</code></pre>"},{"location":"API_REFERENCE/#async-operations","title":"Async Operations","text":"<pre><code>import asyncio\n\nasync def main():\n    # Async completion\n    response = await client.acomplete(\"What is 2+2?\")\n\n    # Async adapter operations also supported\n    # (Implementation depends on adapter async support)\n\nasyncio.run(main())\n</code></pre>"},{"location":"API_REFERENCE/#streaming","title":"Streaming","text":"<pre><code># Stream responses token by token\nfor chunk in client.complete_with_stream(\"Write a story\"):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"API_REFERENCE/#error-handling","title":"Error Handling","text":"<pre><code>from ace.exceptions import ACEException\n\ntry:\n    output = agent.generate(...)\nexcept ACEException as e:\n    print(f\"ACE error: {e}\")\nexcept Exception as e:\n    print(f\"Unexpected error: {e}\")\n</code></pre>"},{"location":"API_REFERENCE/#configuration","title":"Configuration","text":""},{"location":"API_REFERENCE/#environment-variables","title":"Environment Variables","text":"<pre><code># OpenAI\nexport OPENAI_API_KEY=\"your-key\"\n\n# Anthropic\nexport ANTHROPIC_API_KEY=\"your-key\"\n\n# Google\nexport GOOGLE_API_KEY=\"your-key\"\n\n# Custom endpoint\nexport LITELLM_API_BASE=\"https://your-endpoint.com\"\n</code></pre>"},{"location":"API_REFERENCE/#logging","title":"Logging","text":"<pre><code>import logging\n\n# Enable debug logging\nlogging.basicConfig(level=logging.DEBUG)\n\n# Or just for ACE\nlogging.getLogger(\"ace\").setLevel(logging.DEBUG)\n</code></pre>"},{"location":"API_REFERENCE/#best-practices","title":"Best Practices","text":"<ol> <li>Start with SimpleEnvironment: Get basic training working first</li> <li>Use fallback models: Ensure reliability in production</li> <li>Save skillbooks regularly: Preserve learned strategies</li> <li>Monitor costs: Track token usage with metrics</li> <li>Test with dummy mode: Validate logic without API calls</li> <li>Use appropriate epochs: 2-3 epochs usually sufficient</li> <li>Implement custom environments: Tailor evaluation to your task</li> </ol>"},{"location":"API_REFERENCE/#examples","title":"Examples","text":"<p>See the examples directory for complete working examples:</p> <p>Core Examples: - <code>simple_ace_example.py</code> - Basic usage - <code>skillbook_persistence.py</code> - Save/load strategies</p> <p>By Category: - langchain/ - LangChain integration examples - prompts/ - Prompt engineering examples - browser-use/ - Browser automation</p>"},{"location":"COMPLETE_GUIDE_TO_ACE/","title":"Agentic Context Engineering: Complete Guide","text":"<p>How ACE enables AI agents to improve through in-context learning instead of fine-tuning.</p>"},{"location":"COMPLETE_GUIDE_TO_ACE/#what-is-agentic-context-engineering","title":"What is Agentic Context Engineering?","text":"<p>Agentic Context Engineering (ACE) is a framework introduced by researchers at Stanford University and SambaNova Systems that enables AI agents to improve performance by dynamically curating their own context through execution feedback.</p> <p>Key Innovation: Instead of updating model weights through expensive fine-tuning cycles, ACE treats context as a living \"skillbook\" that evolves based on what strategies actually work in practice.</p> <p>Research Paper: Agentic Context Engineering (arXiv:2510.04618)</p>"},{"location":"COMPLETE_GUIDE_TO_ACE/#the-core-problem","title":"The Core Problem","text":"<p>Modern AI agents face a fundamental limitation: they don't learn from execution history. When an agent makes a mistake, developers must manually intervene\u2014editing prompts, adjusting parameters, or fine-tuning the model.</p> <p>Traditional approaches have major drawbacks: - Repetitive failures: Agents lack institutional memory - Manual intervention: Doesn't scale as complexity increases - Expensive adaptation: Fine-tuning costs $10,000+ per cycle and takes weeks - Black box improvement: Unclear what changed or why</p>"},{"location":"COMPLETE_GUIDE_TO_ACE/#how-ace-works","title":"How ACE Works","text":"<p>ACE introduces a three-agent architecture where specialized roles collaborate to build and maintain a dynamic knowledge base called the \"skillbook.\"</p>"},{"location":"COMPLETE_GUIDE_TO_ACE/#the-three-agents","title":"The Three Agents","text":"<p>1. Agent - Task Execution - Performs the actual work using strategies from the skillbook - Operates like a traditional agent but with access to learned knowledge</p> <p>2. Reflector - Performance Analysis - Analyzes execution outcomes without human supervision - Identifies which strategies worked, which failed, and why - Generates insights that inform skillbook updates</p> <p>3. SkillManager - Knowledge Management - Adds new strategies based on successful executions - Removes or marks strategies that consistently fail - Merges semantically similar strategies to prevent redundancy</p>"},{"location":"COMPLETE_GUIDE_TO_ACE/#the-skillbook","title":"The Skillbook","text":"<p>The skillbook stores learned strategies as structured \"skills\"\u2014discrete pieces of knowledge with metadata:</p> <pre><code>{\n  \"content\": \"When querying financial data, filter by date range first to reduce result set size\",\n  \"helpful_count\": 12,\n  \"harmful_count\": 1,\n  \"section\": \"task_guidance\"\n}\n</code></pre>"},{"location":"COMPLETE_GUIDE_TO_ACE/#the-learning-cycle","title":"The Learning Cycle","text":"<ol> <li>Execution: Agent receives a task and retrieves relevant skillbook skills</li> <li>Action: Agent executes using retrieved strategies</li> <li>Reflection: Reflector analyzes the execution outcome</li> <li>Curation: SkillManager updates the skillbook with update operations</li> <li>Iteration: Process repeats, skillbook grows more refined over time</li> </ol>"},{"location":"COMPLETE_GUIDE_TO_ACE/#insight-levels","title":"Insight Levels","text":"<p>The Reflector can analyze execution at three different levels of scope, producing insights of varying depth:</p> Level Scope What's Analyzed Learning Quality Micro Single interaction + environment Request \u2192 response \u2192 ground truth/feedback Learns from correctness Meso Full agent run Reasoning traces (thoughts, tool calls, observations) Learns from execution patterns Macro Cross-run analysis Patterns across multiple executions Comprehensive (future) <p>Micro-level insights come from the full ACE adaptation loop with environment feedback and ground truth. The Reflector knows whether the answer was correct and learns from that evaluation. Used by OfflineACE and OnlineACE.</p> <p>Meso-level insights come from full agent runs with intermediate steps\u2014the agent's thoughts, tool calls, and observations\u2014but without external ground truth. The Reflector learns from the execution patterns themselves. Used by integration wrappers like ACELangChain with AgentExecutor.</p> <p>Macro-level insights (future) will compare patterns across multiple runs to identify systemic improvements.</p>"},{"location":"COMPLETE_GUIDE_TO_ACE/#key-technical-innovations","title":"Key Technical Innovations","text":""},{"location":"COMPLETE_GUIDE_TO_ACE/#update-operations-preventing-context-collapse","title":"Update Operations (Preventing Context Collapse)","text":"<p>A critical insight from the ACE paper: LLMs exhibit brevity bias when asked to rewrite context. They compress information, losing crucial details.</p> <p>ACE solves this through update operations\u2014incremental modifications that never ask the LLM to regenerate entire contexts:</p> <ul> <li>Add: Insert new skill to skillbook</li> <li>Remove: Delete specific skill by ID</li> <li>Modify: Update specific fields (helpful_count, content refinement)</li> </ul> <p>This preserves the exact wording and structure of learned knowledge.</p>"},{"location":"COMPLETE_GUIDE_TO_ACE/#semantic-deduplication","title":"Semantic Deduplication","text":"<p>As agents learn, they may generate similar but differently-worded strategies. ACE prevents skillbook bloat through embedding-based deduplication, keeping the skillbook concise while capturing diverse knowledge.</p>"},{"location":"COMPLETE_GUIDE_TO_ACE/#hybrid-retrieval","title":"Hybrid Retrieval","text":"<p>Instead of dumping the entire skillbook into context, ACE uses hybrid retrieval to select only the most relevant skills. This:</p> <ul> <li>Keeps context windows manageable</li> <li>Prioritizes proven strategies</li> <li>Reduces token costs</li> </ul>"},{"location":"COMPLETE_GUIDE_TO_ACE/#async-learning-mode","title":"Async Learning Mode","text":"<p>For latency-sensitive applications, ACE supports async learning where the Agent returns immediately while Reflector and SkillManager process in the background:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                       ASYNC LEARNING PIPELINE                         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                       \u2502\n\u2502  Sample 1 \u2500\u2500\u25ba Agent \u2500\u2500\u25ba Env \u2500\u2500\u25ba Reflector \u2500\u2510                         \u2502\n\u2502  Sample 2 \u2500\u2500\u25ba Agent \u2500\u2500\u25ba Env \u2500\u2500\u25ba Reflector \u2500\u253c\u2500\u2500\u25ba Queue \u2500\u2500\u25ba SkillManager\u2502\n\u2502  Sample 3 \u2500\u2500\u25ba Agent \u2500\u2500\u25ba Env \u2500\u2500\u25ba Reflector \u2500\u2518           (serialized)   \u2502\n\u2502             (parallel)        (parallel)                              \u2502\n\u2502                                                                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>Why this architecture: - Parallel Reflectors: Safe to parallelize (read-only analysis, no skillbook writes) - Serialized SkillManager: Must be sequential (writes to skillbook, handles deduplication) - 3x faster learning: Reflector LLM calls run concurrently</p> <p>Usage: <pre><code>adapter = OfflineACE(\n    skillbook=skillbook,\n    agent=agent,\n    reflector=reflector,\n    skill_manager=skill_manager,\n    async_learning=True,        # Enable async mode\n    max_reflector_workers=3,    # Parallel Reflector threads\n)\n\nresults = adapter.run(samples, environment)  # Fast - learning in background\n\n# Control methods\nadapter.learning_stats       # Check progress\nadapter.wait_for_learning()  # Block until complete\nadapter.stop_async_learning() # Shutdown pipeline\n</code></pre></p>"},{"location":"COMPLETE_GUIDE_TO_ACE/#performance-results","title":"Performance Results","text":"<p>The Stanford team evaluated ACE across multiple benchmarks:</p> <p>AppWorld Agent Benchmark: - +17.1 percentage points improvement vs. base LLM (\u224840% relative improvement) - Tested on complex multi-step tasks requiring tool use and reasoning</p> <p>Finance Domain (FiNER): - +8.6 percentage points improvement on financial reasoning tasks</p> <p>Adaptation Efficiency: - 86.9% lower adaptation latency compared to existing context-adaptation methods</p> <p>Key Insight: Performance improvements compound over time. As the skillbook grows, agents make fewer mistakes on similar tasks, creating a positive feedback loop.</p>"},{"location":"COMPLETE_GUIDE_TO_ACE/#when-to-use-ace","title":"When to Use ACE","text":""},{"location":"COMPLETE_GUIDE_TO_ACE/#best-fit-use-cases","title":"Best Fit Use Cases","text":"<p>Software Development Agents - Learn project-specific patterns (naming conventions, error handling) - Build knowledge of common bugs and solutions - Accumulate code review guidelines</p> <p>Customer Support Automation - Learn which issues need human escalation - Discover effective communication patterns - Build institutional knowledge of edge cases</p> <p>Data Analysis Agents - Learn efficient query patterns - Discover which visualizations work for which data types - Build baseline expectations from execution history</p> <p>Research Assistants - Learn effective search strategies per domain - Discover citation patterns and summarization techniques - Build knowledge of reliable sources</p>"},{"location":"COMPLETE_GUIDE_TO_ACE/#when-not-to-use-ace","title":"When NOT to Use ACE","text":"<p>ACE may not be the right fit when: - Single-use tasks: No benefit from learning if task never repeats - Perfect first-time execution required: ACE learns through iteration - Purely factual retrieval: Traditional RAG may be more appropriate</p>"},{"location":"COMPLETE_GUIDE_TO_ACE/#ace-vs-other-approaches","title":"ACE vs. Other Approaches","text":""},{"location":"COMPLETE_GUIDE_TO_ACE/#vs-fine-tuning","title":"vs. Fine-Tuning","text":"Aspect ACE Fine-Tuning Speed Immediate (after single execution) Days to weeks Cost Inference only $10K+ per iteration Interpretability Readable skillbook Black box weights Reversibility Edit/remove strategies easily Requires retraining"},{"location":"COMPLETE_GUIDE_TO_ACE/#vs-rag","title":"vs. RAG","text":"Aspect ACE RAG Knowledge Source Learned from execution Static documents Update Mechanism Autonomous skill updates Manual updates Content Type Strategies, patterns Facts, references Optimization Self-improving Requires query tuning"},{"location":"COMPLETE_GUIDE_TO_ACE/#getting-started","title":"Getting Started","text":"<p>Ready to build self-learning agents? Check out these resources:</p> <ul> <li>Quick Start Guide - Get running in 5 minutes</li> <li>Integration Guide - Add ACE to existing agents</li> <li>API Reference - Complete API documentation</li> <li>Examples - Ready-to-run code examples</li> </ul>"},{"location":"COMPLETE_GUIDE_TO_ACE/#additional-resources","title":"Additional Resources","text":""},{"location":"COMPLETE_GUIDE_TO_ACE/#research","title":"Research","text":"<ul> <li>Original ACE Paper (arXiv)</li> </ul>"},{"location":"COMPLETE_GUIDE_TO_ACE/#community","title":"Community","text":"<ul> <li>Discord Server</li> <li>GitHub</li> <li>Kayba Website</li> </ul> <p>Last Updated: November 2025</p>"},{"location":"INSIGHT_SOURCES/","title":"Insight Source Tracing","text":"<p>Insight source tracing tracks the provenance of every skill in an ACE skillbook. When the framework learns a new strategy or updates an existing one, metadata is attached describing why the skill was created: which sample triggered the learning, what error was observed, and the reflector's diagnosis.</p>"},{"location":"INSIGHT_SOURCES/#whats-captured","title":"What's Captured","text":"<p>Every skill that originates from the ACE learning loop carries an <code>InsightSource</code> dict in its <code>sources</code> list:</p> Field Type Description <code>sample_question</code> <code>str</code> The question/task that triggered the learning (capped at 200 chars) <code>epoch</code> <code>int</code> Training epoch number <code>step</code> <code>int</code> Step index within the epoch <code>trace_refs</code> <code>list</code> Trace references pointing to agent reasoning (see below) <code>learning_text</code> <code>str?</code> The matched <code>ExtractedLearning.learning</code> text, resolved via <code>learning_index</code> <code>error_identification</code> <code>str?</code> The reflector's raw error diagnosis text (capped at 200 chars) <code>sample_id</code> <code>str?</code> Caller-defined identifier (e.g., trace filename, session ID)"},{"location":"INSIGHT_SOURCES/#how-learning_text-is-populated","title":"How <code>learning_text</code> is Populated","text":"<p>The SkillManager outputs a <code>learning_index</code> (0-based int) on each ADD/UPDATE operation, referencing which <code>extracted_learning</code> from the reflector's output produced the operation. <code>build_insight_source()</code> uses this index to look up the exact learning text:</p> <pre><code>Reflector outputs extracted_learnings: [\n  {learning: \"Verify multiplication\", ...},    // index 0\n  {learning: \"Check boundary conditions\", ...} // index 1\n]\n\nSkillManager outputs operations: [\n  {type: \"ADD\", content: \"Double-check math\", learning_index: 0, ...},\n]\n\nbuild_insight_source() reads op.learning_index=0\n  \u2192 extracted_learnings[0].learning = \"Verify multiplication\"\n  \u2192 stores as learning_text on InsightSource\n</code></pre> <p>If <code>learning_index</code> is <code>None</code> or out of range, <code>learning_text</code> is set to <code>None</code>.</p>"},{"location":"INSIGHT_SOURCES/#trace-references","title":"Trace References","text":"<p>Each <code>TraceReference</code> in <code>trace_refs</code> uses one of two formats:</p> <ul> <li>Structured (when <code>TraceContext</code> is available): <code>step_indices</code> and <code>action_types</code> pointing to specific execution steps.</li> <li>Text fallback (default): A text excerpt from the agent's reasoning (up to 200 chars) with <code>excerpt_location</code> set to <code>\"reasoning\"</code>.</li> </ul>"},{"location":"INSIGHT_SOURCES/#querying-sources","title":"Querying Sources","text":""},{"location":"INSIGHT_SOURCES/#source_map-raw-source-data","title":"<code>source_map()</code> \u2014 Raw source data","text":"<p>Returns a dict mapping skill IDs to their source lists. Only includes skills that have sources.</p> <pre><code>source_map = skillbook.source_map()\nfor skill_id, sources in source_map.items():\n    print(f\"{skill_id}: {len(sources)} source(s)\")\n    for src in sources:\n        print(f\"  epoch={src['epoch']}\")\n        if src.get(\"error_identification\"):\n            print(f\"  diagnosis: {src['error_identification'][:80]}...\")\n</code></pre>"},{"location":"INSIGHT_SOURCES/#source_summary-aggregated-statistics","title":"<code>source_summary()</code> \u2014 Aggregated statistics","text":"<p>Returns distributions of epochs and sample questions across all sources.</p> <pre><code>summary = skillbook.source_summary()\nprint(f\"Total sources: {summary['total_sources']}\")\nprint(f\"Epochs: {summary['epochs']}\")\nprint(f\"Top questions: {summary['sample_questions']}\")\n</code></pre>"},{"location":"INSIGHT_SOURCES/#source_filter-query-by-criteria","title":"<code>source_filter()</code> \u2014 Query by criteria","text":"<p>Filter sources by epoch or sample question (substring match).</p> <pre><code># All skills learned in epoch 2\nepoch_2 = skillbook.source_filter(epoch=2)\n\n# All skills triggered by a specific question\nmath_q = skillbook.source_filter(sample_question=\"2+2\")\n\n# Combined criteria (AND logic)\nspecific = skillbook.source_filter(epoch=1, sample_question=\"capital\")\n</code></pre> <p>Tip for prompt optimization: When <code>sample_question</code> is a placeholder (e.g., <code>\"-\"</code>), use <code>sample_id</code> via <code>source_map()</code> to identify which trace file produced each skill:</p> <pre><code>for skill_id, sources in skillbook.source_map().items():\n    for src in sources:\n        print(f\"{skill_id} \u2190 {src.get('sample_id', 'unknown')}\")\n</code></pre>"},{"location":"INSIGHT_SOURCES/#how-to-interpret-sources","title":"How to Interpret Sources","text":"<p>Reading an insight source tells you the full story of a skill's creation:</p> <ol> <li><code>sample_question</code> \u2014 What task was being attempted?</li> <li><code>error_identification</code> \u2014 What went wrong (or right)? The reflector's raw diagnosis.</li> <li><code>trace_refs</code> \u2014 What was the agent thinking? The text excerpt shows the agent's reasoning at the time.</li> <li><code>learning_text</code> \u2014 What specific learning was extracted by the reflector? (resolved via <code>learning_index</code>)</li> <li><code>sample_id</code> \u2014 Caller-defined identifier for the source (e.g., trace filename). Useful for prompt optimization workflows where <code>sample_question</code> may be a placeholder.</li> </ol> <p>Skills with multiple entries in <code>sources</code> were updated across multiple learning iterations. Check <code>epoch</code> and <code>step</code> to see the evolution.</p>"},{"location":"INSIGHT_SOURCES/#architecture","title":"Architecture","text":"<p>Insight sources are attached during the ACE learning loop, after the SkillManager produces update operations but before they are applied to the skillbook:</p> <pre><code>Sample (carries optional .id for caller-defined identification)\n  \u2192 Agent.generate()        produces AgentOutput (with reasoning)\n  \u2192 Environment.evaluate()  produces EnvironmentResult (with feedback)\n  \u2192 Reflector.reflect()     produces ReflectorOutput (with error_identification, learnings)\n  \u2192 SkillManager.update()   produces UpdateOperations (ADD, UPDATE, TAG, REMOVE)\n                             ADD/UPDATE ops carry learning_index\n  \u2192 build_insight_source()  attaches InsightSource metadata to ADD/UPDATE ops\n                             resolves learning_index \u2192 learning_text\n                             copies Sample.id \u2192 InsightSource.sample_id\n  \u2192 Skillbook.apply_update() stores ops with their sources\n</code></pre> <p>The <code>build_insight_source()</code> function in <code>ace/insight_source.py</code> handles: - Building trace references from agent output - Resolving each operation's <code>learning_index</code> to its <code>learning_text</code> - Storing the reflector's raw error identification</p> <p>This metadata is preserved through serialization (<code>save_to_file</code> / <code>load_from_file</code>), so you can analyze provenance of skills trained in previous sessions.</p>"},{"location":"INTEGRATION_GUIDE/","title":"ACE Integration Guide","text":"<p>Comprehensive guide for integrating ACE learning with your agentic system.</p>"},{"location":"INTEGRATION_GUIDE/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Integration vs Full Pipeline</li> <li>The Base Integration Pattern</li> <li>Building a Custom Integration</li> <li>Reference Implementations</li> <li>Integration Patterns</li> <li>Advanced Topics</li> <li>Troubleshooting</li> </ol>"},{"location":"INTEGRATION_GUIDE/#integration-vs-full-pipeline","title":"Integration vs Full Pipeline","text":""},{"location":"INTEGRATION_GUIDE/#decision-tree-which-approach-should-you-use","title":"Decision Tree: Which Approach Should You Use?","text":"<pre><code>Do you have an existing agentic system?\n\u2502\n\u251c\u2500 YES \u2192 Use INTEGRATION PATTERN\n\u2502   \u2502\n\u2502   \u251c\u2500 Browser automation? \u2192 Use ACEAgent (browser-use)\n\u2502   \u251c\u2500 LangChain chains/agents? \u2192 Use ACELangChain\n\u2502   \u2514\u2500 Custom agent? \u2192 Follow this guide\n\u2502\n\u2514\u2500 NO \u2192 Use FULL ACE PIPELINE\n    \u2502\n    \u251c\u2500 Simple tasks (Q&amp;A, classification)? \u2192 Use ACELiteLLM\n    \u2514\u2500 Complex tasks (tools, workflows)? \u2192 Consider LangChain + ACELangChain\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#whats-the-difference","title":"What's the Difference?","text":"<p>INTEGRATION PATTERN (this guide): - Your agent executes tasks (browser-use, LangChain, custom API) - ACE learns from results (doesn't execute) - Components: Skillbook + Reflector + SkillManager (NO Agent) - Use case: Wrapping existing agents with learning</p> <p>FULL ACE PIPELINE (not this guide): - ACE Agent executes tasks - Full ACE components: Skillbook + Agent + Reflector + SkillManager - Use case: Building new agents from scratch - See: <code>ace.integrations.ACELiteLLM</code> class</p>"},{"location":"INTEGRATION_GUIDE/#the-base-integration-pattern","title":"The Base Integration Pattern","text":"<p>All ACE integrations follow a three-step pattern:</p>"},{"location":"INTEGRATION_GUIDE/#step-1-inject-optional-but-recommended","title":"Step 1: INJECT (Optional but Recommended)","text":"<p>Add learned strategies from the skillbook to your agent's input.</p> <pre><code>from ace.integrations.base import wrap_skillbook_context\nfrom ace import Skillbook\n\nskillbook = Skillbook()  # or load existing: Skillbook.load_from_file(\"expert.json\")\ntask = \"Process user request\"\n\n# Inject skillbook context\nif skillbook.skills():\n    enhanced_task = f\"{task}\\n\\n{wrap_skillbook_context(skillbook)}\"\nelse:\n    enhanced_task = task  # No learned strategies yet\n</code></pre> <p>What does <code>wrap_skillbook_context()</code> do? - Formats learned strategies with success rates - Adds usage instructions for the agent - Returns empty string if no skills (safe to call always)</p>"},{"location":"INTEGRATION_GUIDE/#step-2-execute","title":"Step 2: EXECUTE","text":"<p>Your agent runs normally - ACE doesn't interfere.</p> <pre><code># Your agent (any framework/API)\nresult = your_agent.execute(enhanced_task)\n\n# Examples:\n# - Browser-use: await agent.run(task=enhanced_task)\n# - LangChain: chain.invoke({\"input\": enhanced_task})\n# - API: requests.post(\"/execute\", json={\"task\": enhanced_task})\n# - Custom: my_agent.run(enhanced_task)\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#step-3-learn","title":"Step 3: LEARN","text":"<p>ACE analyzes the result and updates the skillbook.</p> <pre><code>from ace import LiteLLMClient, Reflector, SkillManager\nfrom ace.roles import AgentOutput\n\n# Setup ACE learning components (do this once)\nllm = LiteLLMClient(model=\"gpt-4o-mini\", max_tokens=2048)\nreflector = Reflector(llm)\nskill_manager = SkillManager(llm)\n\n# Create adapter for Reflector interface\nagent_output = AgentOutput(\n    reasoning=f\"Task: {task}\",  # What happened\n    final_answer=result.output,  # Agent's output\n    skill_ids=[],  # External agents don't cite skills\n    raw={\"success\": result.success, \"steps\": result.steps}  # Metadata\n)\n\n# Build feedback string\nfeedback = f\"Task {'succeeded' if result.success else 'failed'}. Output: {result.output}\"\n\n# Reflect: Analyze what worked/failed\nreflection = reflector.reflect(\n    question=task,\n    agent_output=agent_output,\n    skillbook=skillbook,\n    ground_truth=None,  # Optional: expected output\n    feedback=feedback\n)\n\n# Update skills: Generate skillbook updates\nskill_manager_output = skill_manager.update_skills(\n    reflection=reflection,\n    skillbook=skillbook,\n    question_context=f\"task: {task}\",\n    progress=f\"Executing: {task}\"\n)\n\n# Apply updates\nskillbook.apply_update(skill_manager_output.update)\n\n# Save for next time\nskillbook.save_to_file(\"learned_strategies.json\")\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#building-a-custom-integration","title":"Building a Custom Integration","text":""},{"location":"INTEGRATION_GUIDE/#wrapper-class-pattern-recommended","title":"Wrapper Class Pattern (Recommended)","text":"<p>Create a wrapper class that bundles your agent with ACE learning:</p> <pre><code>from ace import Skillbook, LiteLLMClient, Reflector, SkillManager\nfrom ace.integrations.base import wrap_skillbook_context\nfrom ace.roles import AgentOutput\n\nclass ACEWrapper:\n    \"\"\"Wraps your custom agent with ACE learning.\"\"\"\n\n    def __init__(\n        self,\n        agent,\n        ace_model: str = \"gpt-4o-mini\",\n        skillbook_path: str = None,\n        is_learning: bool = True\n    ):\n        \"\"\"\n        Args:\n            agent: Your agent instance\n            ace_model: Model for ACE learning (Reflector/SkillManager)\n            skillbook_path: Path to existing skillbook (optional)\n            is_learning: Enable/disable learning\n        \"\"\"\n        self.agent = agent\n        self.is_learning = is_learning\n\n        # Load or create skillbook\n        if skillbook_path:\n            self.skillbook = Skillbook.load_from_file(skillbook_path)\n        else:\n            self.skillbook = Skillbook()\n\n        # Setup ACE learning components\n        self.llm = LiteLLMClient(model=ace_model, max_tokens=2048)\n        self.reflector = Reflector(self.llm)\n        self.skill_manager = SkillManager(self.llm)\n\n    def run(self, task: str):\n        \"\"\"Execute task with ACE learning.\"\"\"\n        # STEP 1: Inject skillbook context\n        enhanced_task = self._inject_context(task)\n\n        # STEP 2: Execute\n        result = self.agent.execute(enhanced_task)\n\n        # STEP 3: Learn (if enabled)\n        if self.is_learning:\n            self._learn(task, result)\n\n        return result\n\n    def _inject_context(self, task: str) -&gt; str:\n        \"\"\"Add skillbook strategies to task.\"\"\"\n        if self.skillbook.skills():\n            return f\"{task}\\n\\n{wrap_skillbook_context(self.skillbook)}\"\n        return task\n\n    def _learn(self, task: str, result):\n        \"\"\"Run ACE learning pipeline.\"\"\"\n        # Adapt result to ACE interface\n        agent_output = AgentOutput(\n            reasoning=f\"Task: {task}\",\n            final_answer=result.output,\n            skill_ids=[],\n            raw={\"success\": result.success}\n        )\n\n        # Build feedback\n        feedback = f\"Task {'succeeded' if result.success else 'failed'}\"\n\n        # Reflect\n        reflection = self.reflector.reflect(\n            question=task,\n            agent_output=agent_output,\n            skillbook=self.skillbook,\n            feedback=feedback\n        )\n\n        # Update skills\n        skill_manager_output = self.skill_manager.update_skills(\n            reflection=reflection,\n            skillbook=self.skillbook,\n            question_context=f\"task: {task}\",\n            progress=task\n        )\n\n        # Update skillbook\n        self.skillbook.apply_update(skill_manager_output.update)\n\n    def save_skillbook(self, path: str):\n        \"\"\"Save learned strategies.\"\"\"\n        self.skillbook.save_to_file(path)\n\n    def load_skillbook(self, path: str):\n        \"\"\"Load existing strategies.\"\"\"\n        self.skillbook = Skillbook.load_from_file(path)\n\n    def enable_learning(self):\n        \"\"\"Enable learning.\"\"\"\n        self.is_learning = True\n\n    def disable_learning(self):\n        \"\"\"Disable learning (execution only).\"\"\"\n        self.is_learning = False\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#usage-example","title":"Usage Example","text":"<pre><code># Your custom agent\nclass MyAgent:\n    def execute(self, task: str):\n        # Your agent logic\n        return {\"output\": \"result\", \"success\": True}\n\n# Wrap with ACE\nmy_agent = MyAgent()\nace_agent = ACEWrapper(my_agent, is_learning=True)\n\n# Use it\nresult = ace_agent.run(\"Process data\")\nprint(f\"Result: {result.output}\")\nprint(f\"Learned {len(ace_agent.skillbook.skills())} strategies\")\n\n# Save learned knowledge\nace_agent.save_skillbook(\"my_agent_learned.json\")\n\n# Next session: Load previous knowledge\nace_agent = ACEWrapper(MyAgent(), skillbook_path=\"my_agent_learned.json\")\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#reference-implementations","title":"Reference Implementations","text":""},{"location":"INTEGRATION_GUIDE/#browser-use-integration","title":"Browser-Use Integration","text":"<p>See <code>ace/integrations/browser_use.py</code> for a complete reference implementation.</p> <p>Key Design Decisions:</p> <ol> <li> <p>Context Injection (line 182-189): <pre><code>if self.is_learning and self.skillbook.skills():\n    skillbook_context = wrap_skillbook_context(self.skillbook)\n    enhanced_task = f\"{current_task}\\n\\n{skillbook_context}\"\n</code></pre></p> </li> <li> <p>Rich Feedback Extraction (line 234-403):</p> </li> <li>Extracts chronological execution trace</li> <li>Includes agent thoughts, actions, results</li> <li> <p>Provides detailed context for Reflector</p> </li> <li> <p>Citation Extraction (line 405-434):</p> </li> <li>Parses agent's reasoning for skill citations</li> <li> <p>Filters invalid IDs (graceful degradation)</p> </li> <li> <p>Learning Pipeline (line 436-510):</p> </li> <li>Creates AgentOutput adapter</li> <li>Passes full trace to Reflector in <code>reasoning</code> field</li> <li>Updates skillbook via SkillManager</li> </ol> <p>Why Browser-Use is a Good Reference: - Shows rich feedback extraction - Handles async execution - Robust error handling - Learning toggle - Skillbook persistence</p>"},{"location":"INTEGRATION_GUIDE/#runnable-examples","title":"Runnable Examples","text":"<p>See these working examples in the repository:</p> <ul> <li>Browser automation: <code>examples/browser-use/simple_ace_agent.py</code></li> <li>Custom integration: <code>examples/custom_integration_example.py</code></li> <li>LangChain chains/agents: <code>examples/langchain/simple_chain_example.py</code></li> </ul> <p>Full list: <code>examples/README.md</code></p>"},{"location":"INTEGRATION_GUIDE/#integration-patterns","title":"Integration Patterns","text":"<p>Common patterns for integrating ACE with different types of agents. Each pattern includes complete code examples, when to use it, and key considerations.</p>"},{"location":"INTEGRATION_GUIDE/#rest-api-based-agents","title":"REST API-Based Agents","text":""},{"location":"INTEGRATION_GUIDE/#when-to-use","title":"When to Use","text":"<ul> <li>Your agent is a REST API service</li> <li>Remote execution (cloud-based agents)</li> <li>Stateless request/response pattern</li> </ul>"},{"location":"INTEGRATION_GUIDE/#pattern","title":"Pattern","text":"<pre><code>from ace import Skillbook, LiteLLMClient, Reflector, SkillManager\nfrom ace.integrations.base import wrap_skillbook_context\nfrom ace.roles import AgentOutput\nimport requests\n\nclass ACEAPIAgent:\n    \"\"\"Wraps REST API agent with ACE learning.\"\"\"\n\n    def __init__(self, api_url: str, api_key: str = None, ace_model: str = \"gpt-4o-mini\"):\n        self.api_url = api_url\n        self.api_key = api_key\n        self.skillbook = Skillbook()\n\n        # ACE components\n        llm = LiteLLMClient(model=ace_model, max_tokens=2048)\n        self.reflector = Reflector(llm)\n        self.skill_manager = SkillManager(llm)\n\n    def execute(self, task: str):\n        \"\"\"Execute task via API with ACE learning.\"\"\"\n        # Inject context\n        if self.skillbook.skills():\n            task = f\"{task}\\n\\n{wrap_skillbook_context(self.skillbook)}\"\n\n        # API call\n        headers = {\"Authorization\": f\"Bearer {self.api_key}\"} if self.api_key else {}\n        response = requests.post(\n            f\"{self.api_url}/execute\",\n            json={\"task\": task},\n            headers=headers,\n            timeout=60\n        )\n\n        # Extract result\n        success = response.status_code == 200\n        output = response.json().get(\"result\", \"\") if success else response.text\n\n        # Learn\n        self._learn(task, output, success)\n\n        return {\"output\": output, \"success\": success}\n\n    def _learn(self, task: str, output: str, success: bool):\n        # Create adapter\n        agent_output = AgentOutput(\n            reasoning=f\"API call for task: {task}\",\n            final_answer=output,\n            skill_ids=[],\n            raw={\"success\": success}\n        )\n\n        # Feedback\n        feedback = f\"API call {'succeeded' if success else 'failed'}. Output: {output[:200]}\"\n\n        # Reflect + Update skills\n        reflection = self.reflector.reflect(\n            question=task,\n            agent_output=agent_output,\n            skillbook=self.skillbook,\n            feedback=feedback\n        )\n\n        skill_manager_output = self.skill_manager.update_skills(\n            reflection=reflection,\n            skillbook=self.skillbook,\n            question_context=f\"API task: {task}\",\n            progress=task\n        )\n\n        self.skillbook.apply_update(skill_manager_output.update)\n\n# Usage\nagent = ACEAPIAgent(api_url=\"https://api.example.com\", api_key=\"...\")\nresult = agent.execute(\"Process user data\")\nagent.skillbook.save_to_file(\"api_agent_learned.json\")\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#key-considerations","title":"Key Considerations","text":"<ul> <li>Handle timeouts and retries</li> <li>Parse API error messages for better feedback</li> <li>Consider rate limiting (don't learn on every call if high volume)</li> </ul>"},{"location":"INTEGRATION_GUIDE/#multi-step-workflow-agents","title":"Multi-Step Workflow Agents","text":""},{"location":"INTEGRATION_GUIDE/#when-to-use_1","title":"When to Use","text":"<ul> <li>Agent executes multiple sequential steps</li> <li>Each step has its own outcome</li> <li>Want to learn from entire workflow</li> </ul>"},{"location":"INTEGRATION_GUIDE/#pattern_1","title":"Pattern","text":"<pre><code>from dataclasses import dataclass\nfrom typing import List\n\n@dataclass\nclass WorkflowStep:\n    action: str\n    outcome: str\n    success: bool\n    duration: float\n\n@dataclass\nclass WorkflowResult:\n    steps: List[WorkflowStep]\n    final_output: str\n    overall_success: bool\n\nclass ACEWorkflowAgent:\n    \"\"\"Wraps multi-step workflow agent with rich trace learning.\"\"\"\n\n    def __init__(self, workflow_agent, ace_model: str = \"gpt-4o-mini\"):\n        self.agent = workflow_agent\n        self.skillbook = Skillbook()\n\n        llm = LiteLLMClient(model=ace_model, max_tokens=2048)\n        self.reflector = Reflector(llm)\n        self.skill_manager = SkillManager(llm)\n\n    def run(self, task: str) -&gt; WorkflowResult:\n        \"\"\"Execute workflow with ACE learning.\"\"\"\n        # Inject context\n        if self.skillbook.skills():\n            task = f\"{task}\\n\\n{wrap_skillbook_context(self.skillbook)}\"\n\n        # Execute workflow (returns WorkflowResult)\n        result = self.agent.execute_workflow(task)\n\n        # Learn from entire workflow\n        self._learn(task, result)\n\n        return result\n\n    def _learn(self, task: str, result: WorkflowResult):\n        \"\"\"Learn from complete workflow trace.\"\"\"\n        # Build rich feedback with all steps\n        feedback_parts = [\n            f\"Workflow {'succeeded' if result.overall_success else 'failed'} \"\n            f\"in {len(result.steps)} steps\\n\"\n        ]\n\n        for i, step in enumerate(result.steps, 1):\n            status = \"\u2713\" if step.success else \"\u2717\"\n            feedback_parts.append(\n                f\"Step {i} [{status}]: {step.action}\\n\"\n                f\"  \u2192 Outcome: {step.outcome}\\n\"\n                f\"  \u2192 Duration: {step.duration:.2f}s\"\n            )\n\n        feedback = \"\\n\".join(feedback_parts)\n\n        # Create adapter with full trace\n        agent_output = AgentOutput(\n            reasoning=feedback,  # Full workflow trace\n            final_answer=result.final_output,\n            skill_ids=[],\n            raw={\n                \"total_steps\": len(result.steps),\n                \"successful_steps\": sum(1 for s in result.steps if s.success),\n                \"total_duration\": sum(s.duration for s in result.steps)\n            }\n        )\n\n        # Reflect + Update skills\n        reflection = self.reflector.reflect(\n            question=task,\n            agent_output=agent_output,\n            skillbook=self.skillbook,\n            feedback=feedback\n        )\n\n        skill_manager_output = self.skill_manager.update_skills(\n            reflection=reflection,\n            skillbook=self.skillbook,\n            question_context=f\"Multi-step workflow: {task}\",\n            progress=f\"Completed {len(result.steps)} steps\"\n        )\n\n        self.skillbook.apply_update(skill_manager_output.update)\n\n# Usage\nworkflow_agent = MyWorkflowAgent()\nace_agent = ACEWorkflowAgent(workflow_agent)\nresult = ace_agent.run(\"Complete data pipeline\")\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#key-considerations_1","title":"Key Considerations","text":"<ul> <li>Include step-by-step trace in feedback for better learning</li> <li>Track timing information to learn performance patterns</li> <li>Distinguish partial failures (some steps succeed) from total failures</li> </ul>"},{"location":"INTEGRATION_GUIDE/#tool-using-agents","title":"Tool-Using Agents","text":""},{"location":"INTEGRATION_GUIDE/#when-to-use_2","title":"When to Use","text":"<ul> <li>Agent has access to external tools/functions</li> <li>Tool selection and usage is part of learning</li> <li>Want to inject context into system message or tool descriptions</li> </ul>"},{"location":"INTEGRATION_GUIDE/#pattern_2","title":"Pattern","text":"<pre><code>class ACEToolAgent:\n    \"\"\"Wraps tool-using agent with ACE learning.\"\"\"\n\n    def __init__(self, agent, ace_model: str = \"gpt-4o-mini\"):\n        self.agent = agent\n        self.skillbook = Skillbook()\n        self.original_system_message = agent.system_message\n\n        llm = LiteLLMClient(model=ace_model, max_tokens=2048)\n        self.reflector = Reflector(llm)\n        self.skill_manager = SkillManager(llm)\n\n    def run(self, task: str):\n        \"\"\"Execute with tool access and ACE learning.\"\"\"\n        # Inject skillbook into system message (not task)\n        if self.skillbook.skills():\n            context = wrap_skillbook_context(self.skillbook)\n            self.agent.system_message = f\"{self.original_system_message}\\n\\n{context}\"\n\n        # Execute (agent selects and uses tools)\n        result = self.agent.execute(task)\n\n        # Restore original system message\n        self.agent.system_message = self.original_system_message\n\n        # Learn\n        self._learn(task, result)\n\n        return result\n\n    def _learn(self, task: str, result):\n        \"\"\"Learn from tool usage patterns.\"\"\"\n        # Extract tool usage information\n        tools_used = result.get(\"tools_used\", [])\n        tool_results = result.get(\"tool_results\", [])\n\n        # Build rich feedback\n        feedback_parts = [\n            f\"Task {'succeeded' if result['success'] else 'failed'}\",\n            f\"Tools used: {', '.join(t['name'] for t in tools_used)}\"\n        ]\n\n        for tool, tool_result in zip(tools_used, tool_results):\n            feedback_parts.append(\n                f\"  {tool['name']}({tool['args']}) \u2192 {tool_result['outcome']}\"\n            )\n\n        feedback = \"\\n\".join(feedback_parts)\n\n        # Adapter\n        agent_output = AgentOutput(\n            reasoning=feedback,\n            final_answer=result[\"output\"],\n            skill_ids=[],\n            raw={\"tools_used\": [t[\"name\"] for t in tools_used]}\n        )\n\n        # Reflect + Update skills\n        reflection = self.reflector.reflect(\n            question=task,\n            agent_output=agent_output,\n            skillbook=self.skillbook,\n            feedback=feedback\n        )\n\n        skill_manager_output = self.skill_manager.update_skills(\n            reflection=reflection,\n            skillbook=self.skillbook,\n            question_context=f\"Tool-using task: {task}\",\n            progress=f\"Used {len(tools_used)} tools\"\n        )\n\n        self.skillbook.apply_update(skill_manager_output.update)\n\n# Usage\ntool_agent = MyToolUsingAgent(tools=[...])\nace_agent = ACEToolAgent(tool_agent)\nresult = ace_agent.run(\"Analyze data and send report\")\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#key-considerations_2","title":"Key Considerations","text":"<ul> <li>Inject context into system message (not task) for better tool selection</li> <li>Track which tools were used for learning tool selection patterns</li> <li>Include tool outcomes in feedback</li> </ul>"},{"location":"INTEGRATION_GUIDE/#async-agents","title":"Async Agents","text":""},{"location":"INTEGRATION_GUIDE/#when-to-use_3","title":"When to Use","text":"<ul> <li>Agent operations are async (browser automation, async APIs)</li> <li>Need non-blocking execution</li> <li>Want to maintain async interface</li> </ul>"},{"location":"INTEGRATION_GUIDE/#pattern_3","title":"Pattern","text":"<pre><code>import asyncio\n\nclass ACEAsyncAgent:\n    \"\"\"Wraps async agent with ACE learning.\"\"\"\n\n    def __init__(self, async_agent, ace_model: str = \"gpt-4o-mini\"):\n        self.agent = async_agent\n        self.skillbook = Skillbook()\n\n        llm = LiteLLMClient(model=ace_model, max_tokens=2048)\n        self.reflector = Reflector(llm)\n        self.skill_manager = SkillManager(llm)\n\n    async def run(self, task: str):\n        \"\"\"Async execution with ACE learning.\"\"\"\n        # Inject context\n        if self.skillbook.skills():\n            task = f\"{task}\\n\\n{wrap_skillbook_context(self.skillbook)}\"\n\n        # Execute (async)\n        result = await self.agent.execute(task)\n\n        # Learn (sync operations in thread)\n        await asyncio.to_thread(self._learn, task, result)\n\n        return result\n\n    def _learn(self, task: str, result):\n        \"\"\"Sync learning pipeline (runs in thread).\"\"\"\n        agent_output = AgentOutput(\n            reasoning=f\"Async task: {task}\",\n            final_answer=result[\"output\"],\n            skill_ids=[],\n            raw={\"success\": result[\"success\"]}\n        )\n\n        feedback = f\"Async task {'succeeded' if result['success'] else 'failed'}\"\n\n        reflection = self.reflector.reflect(\n            question=task,\n            agent_output=agent_output,\n            skillbook=self.skillbook,\n            feedback=feedback\n        )\n\n        skill_manager_output = self.skill_manager.update_skills(\n            reflection=reflection,\n            skillbook=self.skillbook,\n            question_context=task,\n            progress=task\n        )\n\n        self.skillbook.apply_update(skill_manager_output.update)\n\n# Usage\nasync def main():\n    async_agent = MyAsyncAgent()\n    ace_agent = ACEAsyncAgent(async_agent)\n\n    result = await ace_agent.run(\"Fetch and process data\")\n    print(f\"Result: {result}\")\n\nasyncio.run(main())\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#key-considerations_3","title":"Key Considerations","text":"<ul> <li>Use <code>asyncio.to_thread()</code> to run sync Reflector/SkillManager in background</li> <li>Don't block async event loop with sync ACE operations</li> <li>Consider batching learning for high-throughput async systems</li> </ul>"},{"location":"INTEGRATION_GUIDE/#chat-based-agents","title":"Chat-Based Agents","text":""},{"location":"INTEGRATION_GUIDE/#when-to-use_4","title":"When to Use","text":"<ul> <li>Agent maintains conversation history</li> <li>Multi-turn interactions</li> <li>Want to learn from entire conversation</li> </ul>"},{"location":"INTEGRATION_GUIDE/#pattern_4","title":"Pattern","text":"<pre><code>class ACEChatAgent:\n    \"\"\"Wraps chat agent with per-conversation learning.\"\"\"\n\n    def __init__(self, chat_agent, ace_model: str = \"gpt-4o-mini\"):\n        self.agent = chat_agent\n        self.skillbook = Skillbook()\n        self.conversation_history = []\n\n        llm = LiteLLMClient(model=ace_model, max_tokens=2048)\n        self.reflector = Reflector(llm)\n        self.skill_manager = SkillManager(llm)\n\n    def chat(self, message: str) -&gt; str:\n        \"\"\"Single chat turn with context injection.\"\"\"\n        # Inject skillbook on first message\n        if len(self.conversation_history) == 0 and self.skillbook.skills():\n            system_context = wrap_skillbook_context(self.skillbook)\n            self.agent.add_system_message(system_context)\n\n        # Chat\n        response = self.agent.chat(message)\n\n        # Track conversation\n        self.conversation_history.append({\"user\": message, \"assistant\": response})\n\n        return response\n\n    def end_conversation(self, success: bool = True, feedback: str = \"\"):\n        \"\"\"Learn from entire conversation at the end.\"\"\"\n        if not self.conversation_history:\n            return\n\n        # Build conversation summary\n        conversation = \"\\n\".join(\n            f\"User: {turn['user']}\\nAssistant: {turn['assistant']}\"\n            for turn in self.conversation_history\n        )\n\n        # Learn from full conversation\n        agent_output = AgentOutput(\n            reasoning=conversation,\n            final_answer=self.conversation_history[-1][\"assistant\"],\n            skill_ids=[],\n            raw={\"turns\": len(self.conversation_history)}\n        )\n\n        feedback_text = (\n            f\"Conversation {'succeeded' if success else 'failed'} \"\n            f\"over {len(self.conversation_history)} turns. {feedback}\"\n        )\n\n        reflection = self.reflector.reflect(\n            question=self.conversation_history[0][\"user\"],\n            agent_output=agent_output,\n            skillbook=self.skillbook,\n            feedback=feedback_text\n        )\n\n        skill_manager_output = self.skill_manager.update_skills(\n            reflection=reflection,\n            skillbook=self.skillbook,\n            question_context=f\"Multi-turn conversation ({len(self.conversation_history)} turns)\",\n            progress=\"Conversation completed\"\n        )\n\n        self.skillbook.apply_update(skill_manager_output.update)\n\n        # Reset for next conversation\n        self.conversation_history = []\n\n# Usage\nchat_agent = MyChatAgent()\nace_agent = ACEChatAgent(chat_agent)\n\n# Multi-turn conversation\nace_agent.chat(\"Hello, I need help with X\")\nace_agent.chat(\"Can you clarify Y?\")\nace_agent.chat(\"Thanks, that works!\")\n\n# Learn from entire conversation\nace_agent.end_conversation(success=True, feedback=\"User satisfied\")\nace_agent.skillbook.save_to_file(\"chat_agent_learned.json\")\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#key-considerations_4","title":"Key Considerations","text":"<ul> <li>Learn from complete conversation (not individual turns)</li> <li>Inject skillbook context at conversation start</li> <li>Allow manual feedback at conversation end</li> </ul>"},{"location":"INTEGRATION_GUIDE/#batch-processing-agents","title":"Batch Processing Agents","text":""},{"location":"INTEGRATION_GUIDE/#when-to-use_5","title":"When to Use","text":"<ul> <li>Processing large batches of similar tasks</li> <li>Want to amortize learning costs</li> <li>Need high throughput</li> </ul>"},{"location":"INTEGRATION_GUIDE/#pattern_5","title":"Pattern","text":"<pre><code>class ACEBatchAgent:\n    \"\"\"Wraps agent with batched learning.\"\"\"\n\n    def __init__(self, agent, ace_model: str = \"gpt-4o-mini\", learn_every: int = 10):\n        self.agent = agent\n        self.skillbook = Skillbook()\n        self.learn_every = learn_every\n        self.pending_results = []\n\n        llm = LiteLLMClient(model=ace_model, max_tokens=2048)\n        self.reflector = Reflector(llm)\n        self.skill_manager = SkillManager(llm)\n\n    def process(self, task: str):\n        \"\"\"Process single task (learn in batches).\"\"\"\n        # Inject context\n        if self.skillbook.skills():\n            task = f\"{task}\\n\\n{wrap_skillbook_context(self.skillbook)}\"\n\n        # Execute\n        result = self.agent.execute(task)\n\n        # Add to pending\n        self.pending_results.append((task, result))\n\n        # Learn when batch is full\n        if len(self.pending_results) &gt;= self.learn_every:\n            self._learn_from_batch()\n\n        return result\n\n    def _learn_from_batch(self):\n        \"\"\"Learn from accumulated results.\"\"\"\n        if not self.pending_results:\n            return\n\n        # Aggregate feedback\n        successes = sum(1 for _, r in self.pending_results if r[\"success\"])\n        failures = len(self.pending_results) - successes\n\n        # Learn from batch summary\n        feedback = (\n            f\"Batch of {len(self.pending_results)} tasks: \"\n            f\"{successes} succeeded, {failures} failed\"\n        )\n\n        # Use first task as representative\n        task, result = self.pending_results[0]\n\n        agent_output = AgentOutput(\n            reasoning=f\"Batch processing: {feedback}\",\n            final_answer=result[\"output\"],\n            skill_ids=[],\n            raw={\"batch_size\": len(self.pending_results), \"success_rate\": successes / len(self.pending_results)}\n        )\n\n        reflection = self.reflector.reflect(\n            question=task,\n            agent_output=agent_output,\n            skillbook=self.skillbook,\n            feedback=feedback\n        )\n\n        skill_manager_output = self.skill_manager.update_skills(\n            reflection=reflection,\n            skillbook=self.skillbook,\n            question_context=f\"Batch processing ({len(self.pending_results)} items)\",\n            progress=\"Batch completed\"\n        )\n\n        self.skillbook.apply_update(skill_manager_output.update)\n\n        # Clear pending\n        self.pending_results = []\n\n    def flush(self):\n        \"\"\"Force learning from remaining pending results.\"\"\"\n        self._learn_from_batch()\n\n# Usage\nagent = MyBatchAgent()\nace_agent = ACEBatchAgent(agent, learn_every=10)\n\n# Process many tasks\nfor task in tasks:\n    ace_agent.process(task)\n\n# Learn from remainder\nace_agent.flush()\nace_agent.skillbook.save_to_file(\"batch_learned.json\")\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#key-considerations_5","title":"Key Considerations","text":"<ul> <li>Balance learning frequency vs cost (learn_every parameter)</li> <li>Call <code>flush()</code> at end to learn from remaining items</li> <li>Consider success rate in batch feedback</li> </ul>"},{"location":"INTEGRATION_GUIDE/#streaming-agents","title":"Streaming Agents","text":""},{"location":"INTEGRATION_GUIDE/#when-to-use_6","title":"When to Use","text":"<ul> <li>Agent streams responses token-by-token</li> <li>Want to maintain streaming interface</li> <li>Learn after complete stream</li> </ul>"},{"location":"INTEGRATION_GUIDE/#pattern_6","title":"Pattern","text":"<pre><code>class ACEStreamingAgent:\n    \"\"\"Wraps streaming agent with post-stream learning.\"\"\"\n\n    def __init__(self, streaming_agent, ace_model: str = \"gpt-4o-mini\"):\n        self.agent = streaming_agent\n        self.skillbook = Skillbook()\n\n        llm = LiteLLMClient(model=ace_model, max_tokens=2048)\n        self.reflector = Reflector(llm)\n        self.skill_manager = SkillManager(llm)\n\n    def stream(self, task: str):\n        \"\"\"Stream response with learning after completion.\"\"\"\n        # Inject context\n        if self.skillbook.skills():\n            task = f\"{task}\\n\\n{wrap_skillbook_context(self.skillbook)}\"\n\n        # Collect full response while streaming\n        full_response = []\n\n        for chunk in self.agent.stream(task):\n            full_response.append(chunk)\n            yield chunk  # Stream to caller\n\n        # Learn after stream completes\n        complete_response = \"\".join(full_response)\n        self._learn(task, complete_response)\n\n    def _learn(self, task: str, response: str):\n        \"\"\"Learn from complete streamed response.\"\"\"\n        agent_output = AgentOutput(\n            reasoning=f\"Streamed response for: {task}\",\n            final_answer=response,\n            skill_ids=[],\n            raw={\"response_length\": len(response)}\n        )\n\n        feedback = f\"Streamed {len(response)} characters\"\n\n        reflection = self.reflector.reflect(\n            question=task,\n            agent_output=agent_output,\n            skillbook=self.skillbook,\n            feedback=feedback\n        )\n\n        skill_manager_output = self.skill_manager.update_skills(\n            reflection=reflection,\n            skillbook=self.skillbook,\n            question_context=task,\n            progress=\"Streaming completed\"\n        )\n\n        self.skillbook.apply_update(skill_manager_output.update)\n\n# Usage\nstreaming_agent = MyStreamingAgent()\nace_agent = ACEStreamingAgent(streaming_agent)\n\nfor chunk in ace_agent.stream(\"Generate report\"):\n    print(chunk, end=\"\", flush=True)\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#key-considerations_6","title":"Key Considerations","text":"<ul> <li>Collect full response before learning</li> <li>Don't block streaming (learn after completion)</li> <li>Maintain streaming interface for caller</li> </ul>"},{"location":"INTEGRATION_GUIDE/#error-prone-agents","title":"Error-Prone Agents","text":""},{"location":"INTEGRATION_GUIDE/#when-to-use_7","title":"When to Use","text":"<ul> <li>Agent frequently fails or throws exceptions</li> <li>Want to learn from failures</li> <li>Need robust error handling</li> </ul>"},{"location":"INTEGRATION_GUIDE/#pattern_7","title":"Pattern","text":"<pre><code>class ACERobustAgent:\n    \"\"\"Wraps agent with error handling and failure learning.\"\"\"\n\n    def __init__(self, agent, ace_model: str = \"gpt-4o-mini\", max_retries: int = 3):\n        self.agent = agent\n        self.skillbook = Skillbook()\n        self.max_retries = max_retries\n\n        llm = LiteLLMClient(model=ace_model, max_tokens=2048)\n        self.reflector = Reflector(llm)\n        self.skill_manager = SkillManager(llm)\n\n    def run(self, task: str):\n        \"\"\"Execute with retries and error learning.\"\"\"\n        # Inject context\n        if self.skillbook.skills():\n            task = f\"{task}\\n\\n{wrap_skillbook_context(self.skillbook)}\"\n\n        last_error = None\n        for attempt in range(self.max_retries):\n            try:\n                result = self.agent.execute(task)\n                # Success - learn from it\n                self._learn(task, result, success=True)\n                return result\n\n            except Exception as e:\n                last_error = str(e)\n                if attempt &lt; self.max_retries - 1:\n                    # Retry\n                    continue\n                else:\n                    # Final failure - learn from it\n                    self._learn(task, None, success=False, error=last_error)\n                    raise\n\n    def _learn(self, task: str, result, success: bool, error: str = None):\n        \"\"\"Learn from both successes and failures.\"\"\"\n        try:\n            # Build feedback\n            if success:\n                feedback = f\"Task succeeded. Output: {result['output']}\"\n                final_answer = result[\"output\"]\n            else:\n                feedback = f\"Task failed after {self.max_retries} attempts. Error: {error}\"\n                final_answer = \"\"\n\n            # Adapter\n            agent_output = AgentOutput(\n                reasoning=f\"Task: {task}. {feedback}\",\n                final_answer=final_answer,\n                skill_ids=[],\n                raw={\"success\": success, \"error\": error}\n            )\n\n            # Reflect + Update skills\n            reflection = self.reflector.reflect(\n                question=task,\n                agent_output=agent_output,\n                skillbook=self.skillbook,\n                feedback=feedback\n            )\n\n            skill_manager_output = self.skill_manager.update_skills(\n                reflection=reflection,\n                skillbook=self.skillbook,\n                question_context=f\"Task ({'success' if success else 'failure'}): {task}\",\n                progress=\"Execution completed\"\n            )\n\n            self.skillbook.apply_update(skill_manager_output.update)\n\n        except Exception as learning_error:\n            # Never crash due to learning failures\n            print(f\"Learning failed: {learning_error}\")\n\n# Usage\nerror_prone_agent = MyUnreliableAgent()\nace_agent = ACERobustAgent(error_prone_agent, max_retries=3)\n\ntry:\n    result = ace_agent.run(\"Risky task\")\nexcept Exception as e:\n    print(f\"Task failed: {e}\")\n    # But skillbook learned from the failure!\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#key-considerations_7","title":"Key Considerations","text":"<ul> <li>Learn from both successes AND failures</li> <li>Wrap learning in try/except (never crash from learning)</li> <li>Include error details in feedback for failure pattern learning</li> </ul>"},{"location":"INTEGRATION_GUIDE/#advanced-topics","title":"Advanced Topics","text":""},{"location":"INTEGRATION_GUIDE/#rich-feedback-extraction","title":"Rich Feedback Extraction","text":"<p>The quality of ACE learning depends on the feedback you provide. The more detailed, the better.</p> <p>Basic Feedback (Minimal): <pre><code>feedback = f\"Task {'succeeded' if success else 'failed'}\"\n</code></pre></p> <p>Good Feedback (Contextual): <pre><code>feedback = f\"\"\"\nTask {'succeeded' if success else 'failed'} in {steps} steps.\nDuration: {duration}s\nFinal output: {output[:200]}...\n\"\"\"\n</code></pre></p> <p>Rich Feedback (Detailed Trace): <pre><code># For agents with step-by-step execution\nfeedback_parts = []\nfeedback_parts.append(f\"Task {status} in {steps} steps\")\n\n# Add execution trace\nfor i, step in enumerate(execution_steps, 1):\n    feedback_parts.append(f\"\\nStep {i}:\")\n    feedback_parts.append(f\"  Thought: {step.thought}\")\n    feedback_parts.append(f\"  Action: {step.action}\")\n    feedback_parts.append(f\"  Result: {step.result}\")\n\nfeedback = \"\\n\".join(feedback_parts)\n</code></pre></p> <p>Benefits of Rich Feedback: - Learns action sequencing patterns - Understands timing requirements - Recognizes error patterns - Captures domain-specific knowledge</p>"},{"location":"INTEGRATION_GUIDE/#citation-based-strategy-tracking","title":"Citation-Based Strategy Tracking","text":"<p>ACE uses citations to track which strategies were used:</p> <p>How It Works: 1. Strategies are formatted with IDs: <code>[section-00001]</code> 2. Agent cites them in reasoning: <code>\"Following [navigation-00042], I will...\"</code> 3. ACE extracts citations automatically</p> <p>Extracting Citations: <pre><code>from ace.roles import extract_cited_skill_ids\n\n# Agent's reasoning with citations\nreasoning = \"\"\"\nStep 1: Following [navigation-00042], navigate to main page.\nStep 2: Using [extraction-00003], extract title element.\n\"\"\"\n\n# Extract citations\ncited_ids = extract_cited_skill_ids(reasoning)\n# Returns: ['navigation-00042', 'extraction-00003']\n\n# Pass to AgentOutput\nagent_output = AgentOutput(\n    reasoning=reasoning,\n    final_answer=result,\n    skill_ids=cited_ids,\n    raw={}\n)\n</code></pre></p> <p>For External Agents: <pre><code># Extract from agent's thought process\nif hasattr(history, 'model_thoughts'):\n    thoughts = history.model_thoughts()\n    thoughts_text = \"\\n\".join(t.thinking for t in thoughts)\n    cited_ids = extract_cited_skill_ids(thoughts_text)\n</code></pre></p>"},{"location":"INTEGRATION_GUIDE/#handling-async-agents","title":"Handling Async Agents","text":"<p>If your agent is async, wrap the learning in a sync function:</p> <pre><code>async def run(self, task: str):\n    # Inject context\n    enhanced_task = self._inject_context(task)\n\n    # Execute (async)\n    result = await self.agent.execute(enhanced_task)\n\n    # Learn (sync Reflector/SkillManager)\n    if self.is_learning:\n        await asyncio.to_thread(self._learn, task, result)\n\n    return result\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#error-handling","title":"Error Handling","text":"<p>Always wrap learning in try/except to prevent crashes:</p> <pre><code>def _learn(self, task: str, result):\n    try:\n        # Reflection\n        reflection = self.reflector.reflect(...)\n\n        # Update skills\n        skill_manager_output = self.skill_manager.update_skills(...)\n\n        # Update\n        self.skillbook.apply_update(skill_manager_output.update)\n\n    except Exception as e:\n        logger.error(f\"ACE learning failed: {e}\")\n        # Continue without learning - don't crash!\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#token-limits","title":"Token Limits","text":"<p>ACE learning components need sufficient tokens:</p> <pre><code># Reflector: 400-800 tokens typical\n# SkillManager: 300-1000 tokens typical\nllm = LiteLLMClient(model=\"gpt-4o-mini\", max_tokens=2048)  # Recommended\n\n# For complex tasks with long traces:\nllm = LiteLLMClient(model=\"gpt-4o-mini\", max_tokens=4096)\n</code></pre>"},{"location":"INTEGRATION_GUIDE/#troubleshooting","title":"Troubleshooting","text":""},{"location":"INTEGRATION_GUIDE/#problem-json-parsing-errors-from-skillmanager","title":"Problem: JSON Parsing Errors from SkillManager","text":"<p>Cause: Insufficient <code>max_tokens</code> for structured output</p> <p>Solution: <pre><code>llm = LiteLLMClient(model=\"gpt-4o-mini\", max_tokens=2048)  # or higher\n</code></pre></p>"},{"location":"INTEGRATION_GUIDE/#problem-not-learning-anything","title":"Problem: Not Learning Anything","text":"<p>Checks: 1. Is <code>is_learning=True</code>? 2. Is SkillManager output non-empty? <code>print(skill_manager_output.update)</code> 3. Is skillbook being saved? <code>skillbook.save_to_file(...)</code></p>"},{"location":"INTEGRATION_GUIDE/#problem-too-many-skills","title":"Problem: Too Many Skills","text":"<p>Solution: SkillManager automatically manages skills via TAG operations. Review with: <pre><code>skills = skillbook.skills()\nprint(f\"Total: {len(skills)}\")\nfor s in skills[:10]:\n    print(f\"[{s.id}] +{s.helpful}/-{s.harmful}: {s.content}\")\n</code></pre></p>"},{"location":"INTEGRATION_GUIDE/#problem-high-api-costs","title":"Problem: High API Costs","text":"<p>Solutions: - Use cheaper model: <code>ace_model=\"gpt-4o-mini\"</code> - Disable learning for simple tasks: <code>is_learning=False</code> - Batch learning: Learn only every N tasks</p>"},{"location":"INTEGRATION_GUIDE/#problem-agent-ignores-skillbook-strategies","title":"Problem: Agent Ignores Skillbook Strategies","text":"<p>Checks: 1. Are you actually injecting context? <code>print(enhanced_task)</code> 2. Does skillbook have skills? <code>print(len(skillbook.skills()))</code> 3. Is context clear enough for your agent?</p>"},{"location":"INTEGRATION_GUIDE/#next-steps","title":"Next Steps","text":"<ol> <li>Start Simple: Use the wrapper class template above</li> <li>Adapt <code>_learn()</code>: Customize for your agent's output format</li> <li>Test Without Learning: Set <code>is_learning=False</code> first</li> <li>Enable Learning: Turn on and monitor skillbook growth</li> <li>Iterate: Improve feedback extraction for better learning</li> </ol>"},{"location":"INTEGRATION_GUIDE/#see-also","title":"See Also","text":"<ul> <li>Out-of-box integrations: ACELiteLLM, ACEAgent (browser-use), ACELangChain</li> <li>Full ACE guide: COMPLETE_GUIDE_TO_ACE.md</li> <li>API reference: API_REFERENCE.md</li> </ul> <p>Questions? Join our Discord</p>"},{"location":"OPIK/","title":"Opik Integration Guide","text":"<p>This guide covers how to use Opik for tracing, monitoring, and cost tracking with the ACE framework.</p>"},{"location":"OPIK/#overview","title":"Overview","text":"<p>ACE framework includes built-in Opik integration for: - Tracing: Track Agent, Reflector, and SkillManager interactions - Cost Tracking: Automatic token usage and LLM cost monitoring - Performance Analysis: Visualize learning loops and skill evolution - Debugging: Inspect detailed traces of the ACE pipeline</p>"},{"location":"OPIK/#installation","title":"Installation","text":"<pre><code># Install ACE with observability features\npip install ace-framework[observability]\n\n# Or for development (includes Opik in optional deps)\nuv sync\n</code></pre>"},{"location":"OPIK/#starting-opik","title":"Starting Opik","text":""},{"location":"OPIK/#local-development-recommended","title":"Local Development (Recommended)","text":"<p>Run the local Opik server using Docker:</p> <pre><code># Start Opik server\ndocker run -d -p 5173:5173 --name opik ghcr.io/comet-ml/opik:latest\n\n# View traces at http://localhost:5173\n</code></pre>"},{"location":"OPIK/#comet-cloud","title":"Comet Cloud","text":"<p>For production, use Comet's hosted Opik:</p> <pre><code># Set your Comet API key\nexport COMET_API_KEY=\"your-api-key\"\n</code></pre>"},{"location":"OPIK/#quick-start","title":"Quick Start","text":""},{"location":"OPIK/#script-initialization","title":"Script Initialization","text":"<p>Add this at the start of your <code>main()</code> function:</p> <pre><code>import os\nfrom ace.observability.opik_integration import configure_opik\n\ndef main():\n    # Initialize Opik with project name\n    project_name = os.environ.get(\"OPIK_PROJECT_NAME\", \"ace-default\")\n    opik_integration = configure_opik(project_name=project_name)\n\n    if opik_integration.is_available():\n        # Register LiteLLM callback for automatic token tracking\n        opik_integration.setup_litellm_callback()\n        print(f\"Opik tracing enabled for project: {project_name}\")\n\n    # Your ACE code here...\n</code></pre>"},{"location":"OPIK/#running-with-tracing","title":"Running with Tracing","text":"<pre><code># Run with custom project name\nOPIK_PROJECT_NAME=\"my-experiment\" uv run python my_script.py\n\n# View traces at http://localhost:5173\n</code></pre>"},{"location":"OPIK/#environment-variables","title":"Environment Variables","text":"Variable Description Default <code>OPIK_PROJECT_NAME</code> Project name for organizing traces <code>ace-framework</code> <code>OPIK_DISABLED=true</code> Disable all Opik tracing Not set <code>OPIK_ENABLED=false</code> Alternative way to disable tracing Not set <code>OPIK_URL_OVERRIDE</code> Custom Opik server URL <code>http://localhost:5173/api</code> <code>OPIK_WORKSPACE</code> Opik workspace name <code>default</code>"},{"location":"OPIK/#component-tracing","title":"Component Tracing","text":""},{"location":"OPIK/#automatically-traced-components","title":"Automatically Traced Components","text":"<p>The following ACE components have built-in tracing:</p> Component Trace Name Tags <code>Agent.generate()</code> <code>agent_generate</code> <code>[\"agent\"]</code> <code>Reflector.reflect()</code> <code>reflector_reflect</code> <code>[\"reflector\"]</code> <code>SkillManager.update_skills()</code> <code>skill_manager_update_skills</code> <code>[\"skill_manager\"]</code> <code>RecursiveReflector.reflect()</code> <code>recursive_reflector</code> <code>[\"reflector\", \"recursive\"]</code> <code>ReplayAgent.generate()</code> <code>replay_agent_generate</code> <code>[\"agent\", \"replay\"]</code>"},{"location":"OPIK/#adding-tracing-to-custom-components","title":"Adding Tracing to Custom Components","text":"<p>Use the <code>@maybe_track</code> decorator to add tracing to your own components:</p> <pre><code>from ace.observability.tracers import maybe_track\n\nclass MyCustomComponent:\n    @maybe_track(name=\"my_component_process\", tags=[\"custom\", \"processing\"])\n    def process(self, input_data):\n        # Your processing logic here\n        result = self._do_work(input_data)\n        return result\n</code></pre> <p>The <code>@maybe_track</code> decorator: - Only applies tracing when Opik is installed and enabled - Gracefully degrades to no-op when Opik is unavailable - Respects <code>OPIK_DISABLED</code> environment variable</p>"},{"location":"OPIK/#automatic-token-cost-tracking","title":"Automatic Token &amp; Cost Tracking","text":"<p>When you call <code>setup_litellm_callback()</code>, all LiteLLM calls are automatically tracked with:</p> <ul> <li>Input/output tokens</li> <li>Model used</li> <li>Cost per call</li> <li>Latency</li> </ul> <pre><code>from ace.observability.opik_integration import configure_opik\nfrom ace.llm_providers.litellm_client import LiteLLMClient\n\n# Initialize Opik\nopik_integration = configure_opik(project_name=\"cost-tracking-demo\")\nif opik_integration.is_available():\n    opik_integration.setup_litellm_callback()\n\n# All LLM calls are now tracked\nclient = LiteLLMClient(model=\"gpt-4\")\nresponse = client.complete(\"What is 2+2?\")\n# Token usage and cost automatically logged to Opik\n</code></pre>"},{"location":"OPIK/#viewing-traces","title":"Viewing Traces","text":""},{"location":"OPIK/#local-opik-ui","title":"Local Opik UI","text":"<ol> <li>Start the Opik server (see above)</li> <li>Open http://localhost:5173 in your browser</li> <li>Select your project from the dropdown</li> <li>Browse traces, spans, and metrics</li> </ol>"},{"location":"OPIK/#trace-hierarchy","title":"Trace Hierarchy","text":"<p>ACE traces are organized hierarchically:</p> <pre><code>[Project: my-experiment]\n  \u2514\u2500\u2500 [Trace: script_run_12345]\n        \u251c\u2500\u2500 [Span: agent_generate]\n        \u2502     \u2514\u2500\u2500 [LLM Call: gpt-4]\n        \u251c\u2500\u2500 [Span: reflector_reflect]\n        \u2502     \u2514\u2500\u2500 [LLM Call: gpt-4]\n        \u2514\u2500\u2500 [Span: skill_manager_update_skills]\n              \u2514\u2500\u2500 [LLM Call: gpt-4]\n</code></pre>"},{"location":"OPIK/#advanced-usage","title":"Advanced Usage","text":""},{"location":"OPIK/#logging-custom-metrics","title":"Logging Custom Metrics","text":"<p>Use the <code>OpikIntegration</code> class to log custom metrics:</p> <pre><code>from ace.observability.opik_integration import get_integration\n\nopik = get_integration()\n\n# Log skill evolution\nopik.log_skill_evolution(\n    skill_id=\"skill_001\",\n    skill_content=\"Always validate input...\",\n    helpful_count=5,\n    harmful_count=1,\n    neutral_count=2,\n    section=\"error_handling\"\n)\n\n# Log adaptation metrics\nopik.log_adaptation_metrics(\n    epoch=2,\n    step=15,\n    performance_score=0.85,\n    skill_count=12,\n    successful_predictions=85,\n    total_predictions=100\n)\n</code></pre>"},{"location":"OPIK/#disabling-tracing-for-tests","title":"Disabling Tracing for Tests","text":"<pre><code># Disable in CI/tests\nOPIK_DISABLED=true pytest tests/\n\n# Or in code\nimport os\nos.environ[\"OPIK_DISABLED\"] = \"true\"\n</code></pre>"},{"location":"OPIK/#using-with-async-code","title":"Using with Async Code","text":"<p>The <code>@maybe_track</code> decorator works with both sync and async functions:</p> <pre><code>from ace.observability.tracers import maybe_track\n\nclass AsyncComponent:\n    @maybe_track(name=\"async_process\", tags=[\"async\"])\n    async def process(self, data):\n        result = await self._async_work(data)\n        return result\n</code></pre>"},{"location":"OPIK/#troubleshooting","title":"Troubleshooting","text":""},{"location":"OPIK/#opik-not-available","title":"\"Opik not available\"","text":"<p>Install the observability extras: <pre><code>pip install ace-framework[observability]\n# or\nuv add opik\n</code></pre></p>"},{"location":"OPIK/#traces-not-appearing","title":"Traces not appearing","text":"<ol> <li>Check Opik server is running: <code>curl http://localhost:5173/api/health</code></li> <li>Verify <code>OPIK_DISABLED</code> is not set</li> <li>Ensure <code>configure_opik()</code> is called before any traced functions</li> </ol>"},{"location":"OPIK/#async-warnings","title":"Async warnings","text":"<p>Warnings like \"coroutine was never awaited\" in synchronous contexts are harmless and can be ignored. They occur when Opik's async internals interact with sync code.</p>"},{"location":"OPIK/#high-memory-usage","title":"High memory usage","text":"<p>For long-running scripts, traces accumulate in memory. Consider: - Running in batches with separate processes - Flushing traces periodically (if supported by Opik)</p>"},{"location":"OPIK/#example-full-ace-pipeline-with-tracing","title":"Example: Full ACE Pipeline with Tracing","text":"<pre><code>import os\nfrom ace import Skillbook, Agent, Reflector, SkillManager, OfflineACE\nfrom ace.llm_providers.litellm_client import LiteLLMClient\nfrom ace.observability.opik_integration import configure_opik\n\ndef main():\n    # 1. Initialize Opik\n    project_name = os.environ.get(\"OPIK_PROJECT_NAME\", \"ace-training\")\n    opik_integration = configure_opik(project_name=project_name)\n    if opik_integration.is_available():\n        opik_integration.setup_litellm_callback()\n        print(f\"Tracing enabled: {project_name}\")\n\n    # 2. Create ACE components (all LLM calls will be traced)\n    llm = LiteLLMClient(model=\"gpt-4\")\n    skillbook = Skillbook()\n    agent = Agent(llm)\n    reflector = Reflector(llm)\n    skill_manager = SkillManager(llm)\n\n    # 3. Run adaptation (traces appear in Opik)\n    adapter = OfflineACE(\n        skillbook=skillbook,\n        agent=agent,\n        reflector=reflector,\n        skill_manager=skill_manager\n    )\n\n    results = adapter.run(samples, environment, epochs=3)\n\n    # 4. View traces at http://localhost:5173\n\nif __name__ == \"__main__\":\n    main()\n</code></pre> <p>Run with: <pre><code>OPIK_PROJECT_NAME=\"training-run-001\" uv run python train.py\n</code></pre></p>"},{"location":"OPIK/#related-documentation","title":"Related Documentation","text":"<ul> <li>Quick Start Guide</li> <li>Integration Guide</li> <li>API Reference</li> </ul>"},{"location":"PIPELINE_DESIGN/","title":"Pipeline Architecture Design","text":""},{"location":"PIPELINE_DESIGN/#design-decisions-for-the-generalized-pipeline-system-trying-to-keep-is-as-generic-as-possible","title":"Design decisions for the generalized pipeline system. Trying to keep is as generic as possible.","text":""},{"location":"PIPELINE_DESIGN/#core-primitives","title":"Core Primitives","text":"<p>Everything in the framework composes from three primitives:</p> <pre><code>Sequential:  A \u2192 B \u2192 C\nBranch:      A \u2192 (B \u2225 C) \u2192 D    (fork + implicit join)\nPipeline:    a step that is itself a pipeline (nesting / reuse)\n</code></pre>"},{"location":"PIPELINE_DESIGN/#step","title":"Step","text":"<p>A <code>Step</code> is the smallest unit of work. It receives a <code>StepContext</code>, does one focused thing, and returns the context.</p> <pre><code>class MyStep:\n    requires = {\"agent_output\"}   # fields it reads\n    provides = {\"reflection\"}     # fields it writes\n\n    def __call__(self, ctx: StepContext) -&gt; StepContext:\n        ...\n        return ctx\n</code></pre> <p>Rules: - Always synchronous within its own execution - Must declare <code>requires</code> and <code>provides</code> \u2014 the pipeline validates ordering at construction time - Steps declare their own parallelism constraints (see below)</p>"},{"location":"PIPELINE_DESIGN/#step-protocol","title":"Step protocol","text":"<p>For static type checking, the framework exposes a <code>typing.Protocol</code>:</p> <pre><code>from collections.abc import Set as AbstractSet\nfrom typing import Protocol, runtime_checkable\n\n@runtime_checkable\nclass StepProtocol(Protocol):\n    requires: AbstractSet[str]\n    provides: AbstractSet[str]\n\n    def __call__(self, ctx: StepContext) -&gt; StepContext: ...\n</code></pre> <p><code>AbstractSet[str]</code> accepts both <code>set</code> and <code>frozenset</code> \u2014 steps declare plain set literals; the pipeline normalizes them to <code>frozenset</code> at construction time before doing any contract validation. <code>Pipeline</code> and <code>Branch</code> both satisfy this protocol, so they can be nested wherever a <code>Step</code> is expected without extra annotation. <code>@runtime_checkable</code> lets the pipeline validator use <code>isinstance(step, StepProtocol)</code> at construction time to give a clear error if a step is missing required attributes, rather than failing at call time.</p>"},{"location":"PIPELINE_DESIGN/#stepcontext-immutability-contract","title":"StepContext \u2014 immutability contract","text":"<p><code>StepContext</code> is a frozen dataclass. Steps never mutate the incoming context \u2014 they return a new one via <code>.replace()</code>.</p> <p>The pipeline engine defines a minimal base with only two fields:</p> <pre><code>from types import MappingProxyType\n\n@dataclass(frozen=True)\nclass StepContext:\n    sample: Any\n    metadata: MappingProxyType = field(default_factory=lambda: MappingProxyType({}))\n\n    def __post_init__(self):\n        # Ensures mutation is a hard runtime error even if caller passes a plain dict\n        if not isinstance(self.metadata, MappingProxyType):\n            object.__setattr__(self, \"metadata\", MappingProxyType(self.metadata))\n\n    def replace(self, **changes) -&gt; \"StepContext\":\n        return dataclasses.replace(self, **changes)\n</code></pre> <p>The engine never reads anything beyond <code>sample</code> and <code>metadata</code>. All domain-specific fields are added by subclassing.</p>"},{"location":"PIPELINE_DESIGN/#subclassing-for-domain-fields","title":"Subclassing for domain fields","text":"<p>Consuming applications subclass <code>StepContext</code> to add named fields for concepts shared across their pipelines:</p> <pre><code>@dataclass(frozen=True)\nclass ACEContext(StepContext):\n    # Shared across all ACE pipelines\n    skillbook: Skillbook | None = None\n    environment: TaskEnvironment | None = None\n\n    # Produced by steps (None until the providing step runs)\n    agent_output: AgentOutput | None = None\n    environment_result: EnvironmentResult | None = None\n    reflection: ReflectorOutput | None = None\n    skill_manager_output: UpdateBatch | None = None\n\n    # Runner bookkeeping\n    epoch: int = 1\n    total_epochs: int = 1\n    step_index: int = 0\n    total_steps: int = 0\n</code></pre> <p>The <code>requires</code>/<code>provides</code> validation works on attribute names (strings) \u2014 it checks that the field exists on the context object at runtime, so it is subclass-agnostic. A step that declares <code>requires = {\"skillbook\"}</code> works whether the context is <code>ACEContext</code> or any other subclass that has a <code>skillbook</code> attribute.</p> <p>Data that is specific to a single integration or step goes in <code>metadata</code> to prevent field accumulation on the subclass. For example, <code>metadata[\"browser_history\"]</code> for browser-use or <code>metadata[\"transcript_path\"]</code> for Claude Code.</p>"},{"location":"PIPELINE_DESIGN/#immutable-update-patterns","title":"Immutable update patterns","text":"<p>Updating metadata follows the same immutable pattern as any other field:</p> <pre><code>return ctx.replace(metadata=MappingProxyType({**ctx.metadata, \"key\": value}))\n</code></pre> <p>Steps follow this pattern:</p> <pre><code>def __call__(self, ctx: StepContext) -&gt; StepContext:\n    result = do_work(ctx.sample)\n    return ctx.replace(result=result)\n</code></pre> <p><code>frozen=True</code> makes mutation a hard error at runtime rather than a subtle bug. It also makes <code>Branch</code> safe by default \u2014 since <code>StepContext</code> is immutable, all branches can receive the same object without risk; no deep copy is needed.</p>"},{"location":"PIPELINE_DESIGN/#pipeline","title":"Pipeline","text":"<p>A <code>Pipeline</code> is an ordered list of steps that runs sequentially for a single input. It also satisfies the <code>Step</code> protocol, so it can be embedded inside another pipeline.</p> <pre><code>pipe = Pipeline([\n    AgentStep(),\n    EvaluateStep(),\n    ReflectStep(),\n    UpdateStep(),\n])\n</code></pre> <p>Fluent builder API (preferred):</p> <pre><code>pipe = (\n    Pipeline()\n    .then(AgentStep())\n    .then(EvaluateStep())\n    .then(ReflectStep())\n    .then(UpdateStep())\n)\n</code></pre> <p>Fan-out across contexts:</p> <pre><code>pipe.run(contexts, workers=4)   # same pipeline, N contexts in parallel\n</code></pre>"},{"location":"PIPELINE_DESIGN/#inner-pipeline-as-a-fan-out-step","title":"Inner pipeline as a fan-out step","text":"<p>A <code>Pipeline</code>-as-<code>Step</code> receives one context and must return one context \u2014 but nothing prevents it from internally expanding to multiple sub-inputs. This is the map-reduce step pattern:</p> <pre><code>class MultiSearchStep:\n    \"\"\"Generates N queries from one context, runs them in parallel, merges.\"\"\"\n    def __call__(self, ctx: StepContext) -&gt; StepContext:\n        queries = generate_queries(ctx.sample)                          # 1 \u2192 N\n        sub_ctxs = [StepContext(sample=q) for q in queries]\n        sub_pipe = Pipeline().then(FetchStep())\n        results = sub_pipe.run(sub_ctxs, workers=len(queries))         # parallel\n        return ctx.replace(agent_output=merge(results))                 # N \u2192 1\n</code></pre> <p><code>sub_pipe.run()</code> is a top-level runner call, so <code>async_boundary</code> and <code>workers</code> on its inner steps fire normally. From the outer pipeline's perspective, <code>MultiSearchStep</code> is a black box that takes one context and returns one context \u2014 the fan-out is an internal implementation detail.</p>"},{"location":"PIPELINE_DESIGN/#requiresprovides-for-nested-pipelines","title":"requires/provides for nested pipelines","text":"<p>When a <code>Pipeline</code> is used as a <code>Step</code> inside another pipeline, its <code>requires</code> and <code>provides</code> are computed automatically at construction time from its inner steps \u2014 no manual annotation needed.</p> <pre><code>class Pipeline:\n    def __init__(self, steps):\n        self.steps = steps\n        self.requires, self.provides = self._infer_contracts(steps)\n\n    @staticmethod\n    def _infer_contracts(steps):\n        provided_so_far = set()\n        external_requires = set()\n        for step in steps:\n            external_requires |= step.requires - provided_so_far\n            provided_so_far |= step.provides\n        return frozenset(external_requires), frozenset(provided_so_far)\n</code></pre> <ul> <li><code>requires</code> = everything the pipeline needs from the outside (what its first steps need that no earlier inner step provides)</li> <li><code>provides</code> = union of everything any inner step writes</li> </ul> <p>The outer pipeline validates against these aggregated values at construction time, so nesting never breaks the contract.</p> <p>Deliberate constraint: <code>_infer_contracts</code> assumes all <code>Branch</code> children always run. It has no concept of conditional branches where only some children execute. If one branch provided a field that a later step required but other branches did not, static validation would pass while the pipeline could fail at runtime. Conditional branching \u2014 where a branch may or may not run depending on context \u2014 is out of scope; all branches in a <code>Branch</code> are always executed.</p>"},{"location":"PIPELINE_DESIGN/#branch","title":"Branch","text":"<p>A <code>Branch</code> is a step that runs multiple pipelines in parallel and joins before returning. It is just a <code>Step</code> \u2014 no special pipeline mode needed.</p> <pre><code>pipe = (\n    Pipeline()\n    .then(AgentStep())\n    .then(EvaluateStep())\n    .branch(\n        Pipeline().then(ReflectStep()),\n        Pipeline().then(LogStep()),\n    )\n    .then(UpdateStep())   # only runs after both branches complete\n)\n</code></pre> <p><code>wait</code> is implicit \u2014 any step after a <code>Branch</code> waits for all branches to finish.</p>"},{"location":"PIPELINE_DESIGN/#context-merging","title":"Context merging","text":"<p>Each branch receives the same context reference. Since <code>StepContext</code> is frozen, no copy is needed \u2014 branches cannot mutate what they receive. When all branches complete, their output contexts are merged back into one before the next step runs.</p> <p>The merge function receives the list of output contexts and returns a single context:</p> <pre><code>Branch(\n    Pipeline().then(ReflectStep()),\n    Pipeline().then(LogStep()),\n    merge=lambda ctxs: dataclasses.replace(\n        ctxs[0],\n        metadata={**ctxs[0].metadata, **ctxs[1].metadata}\n    )\n)\n</code></pre> <p>Built-in merge strategies:</p> Strategy Behaviour <code>raise_on_conflict</code> raises if two branches write the same field \u2014 safe default, no silent data loss <code>last_write_wins</code> last branch's value wins on conflict \u2014 simple but lossy <code>namespaced</code> branches write to <code>ctx.metadata[\"branch_0\"]</code> etc., no conflict possible custom <code>merge=fn</code> <code>fn(ctxs: list[StepContext]) -&gt; StepContext</code> \u2014 full control <p>The actual default when no <code>merge=</code> argument is passed is <code>raise_on_conflict</code>. The constructor signature makes this explicit:</p> <pre><code>def __init__(self, *pipelines, merge=MergeStrategy.RAISE_ON_CONFLICT):\n    ...\n</code></pre> <p>In practice, branches that write disjoint fields (e.g. Reflect writes <code>reflection</code>, Log writes <code>metadata[\"log\"]</code>) never conflict and the merge is a no-op \u2014 <code>raise_on_conflict</code> passes through without raising.</p>"},{"location":"PIPELINE_DESIGN/#async-behavior","title":"Async Behavior","text":"<p>\"Async\" means three different things in this framework, operating at different levels. It is important to keep them separate \u2014 they solve different problems.</p> Type Level Problem it solves Async step single step don't block the thread during I/O <code>async_boundary</code> across samples start the next sample before the current one finishes Branch parallelism within one sample run independent work simultaneously on the same data"},{"location":"PIPELINE_DESIGN/#1-async-steps-non-blocking-io","title":"1. Async steps \u2014 non-blocking I/O","text":"<p>Problem: A step makes a network call (LLM API, HTTP, subprocess). It should not block the thread while waiting for a response.</p> <p>Solution: Define the step as a coroutine. The pipeline detects this automatically and awaits it. Sync steps get wrapped with <code>asyncio.to_thread()</code> so they are safe in an async context too.</p> <pre><code># Sync step \u2014 no changes needed\nclass AgentStep:\n    def __call__(self, ctx: StepContext) -&gt; StepContext: ...\n\n# Async step \u2014 native coroutine, awaited by the pipeline\nclass BrowserExecuteStep:\n    async def __call__(self, ctx: StepContext) -&gt; StepContext: ...\n</code></pre> <pre><code># Pipeline runner \u2014 handles both transparently\nfor step in self.steps:\n    if asyncio.iscoroutinefunction(step.__call__):\n        ctx = await step(ctx)\n    else:\n        ctx = await asyncio.to_thread(step, ctx)\n</code></pre> <p>Pipeline entry points: <code>pipe.run(contexts)</code> for sync callers, <code>await pipe.run_async(contexts)</code> for async callers (e.g. inside browser-use).</p> <p>This type is about not blocking. Nothing runs in parallel \u2014 the pipeline is still sequential, it just yields the thread during waits.</p>"},{"location":"PIPELINE_DESIGN/#2-async_boundary-pipeline-across-samples","title":"2. async_boundary \u2014 pipeline across samples","text":"<p>Problem: Reflect and Update are slow (LLM calls). If we wait for them before starting the next sample, throughput is poor. We want to fire them off and immediately move to sample N+1.</p> <p>Solution: A step declares <code>async_boundary = True</code>. Everything from that step onwards runs in a background executor. The pipeline loop does not wait \u2014 it moves straight to the next sample.</p> <pre><code>class ReflectStep:\n    async_boundary = True   # hand off to background from here\n    max_workers = 3         # up to 3 reflections running in parallel\n\nclass UpdateStep:\n    max_workers = 1         # must serialize \u2014 writes to shared skillbook\n</code></pre> <pre><code>sample 1:  [Agent] [Evaluate] \u2500\u2500fire\u2500\u2500\u25ba [Reflect] [Update]  (background)\nsample 2:  [Agent] [Evaluate] \u2500\u2500fire\u2500\u2500\u25ba [Reflect] [Update]  (background)\nsample 3:  [Agent] [Evaluate] ...\n                              \u2191\n                        async_boundary\n</code></pre> <p>This type is about throughput. Multiple samples are in-flight simultaneously, at different stages of the pipeline. The caller only waits for steps before the boundary.</p> <p>Note: <code>max_workers</code> controls how many background instances of a step run concurrently. Steps that write shared state (like <code>UpdateStep</code>) must use <code>max_workers = 1</code> to avoid races.</p> <p>Background pool is per step class, shared across pipeline instances. <code>ReflectStep.max_workers = 3</code> means a single pool of 3 threads for all <code>ReflectStep</code> instances. This avoids pool proliferation and makes <code>max_workers</code> a straightforward capacity knob independent of how many pipelines are running.</p> <p>Pool lifecycle: The <code>ThreadPoolExecutor</code> for each step class is created lazily at first use (not at class definition or pipeline construction) and persists for the process lifetime. Callers that need explicit cleanup can call <code>StepClass._executor.shutdown(wait=True)</code>. If two users of the same step class need different concurrency limits (e.g. different LLM backends behind the same step type), they should subclass rather than share the class attribute.</p> <p>Boundary rules: - The first step with <code>async_boundary = True</code> is the handoff point. Only one boundary per pipeline. - If multiple steps in the same pipeline declare <code>async_boundary = True</code>, the pipeline raises <code>PipelineConfigError</code> at construction time. A duplicate boundary is almost always a copy-paste mistake, not a deliberate choice. - <code>async_boundary</code> inside a <code>Branch</code> child pipeline raises <code>PipelineConfigError</code> at construction time. Branch children always block until joined; detaching mid-branch is incoherent and there is no valid interpretation. - <code>async_boundary</code> inside a <code>Pipeline</code>-as-<code>Step</code> raises a warning at construction time (not an error). When a pipeline is used as a step inside another pipeline, there is no \"next sample\" to move to \u2014 the outer pipeline is blocked waiting for the inner one to return a context. The boundary is ignored and the inner pipeline runs fully synchronously. The warning surfaces this declared intent being ignored so callers can investigate. The same pipeline definition works both as a top-level runner (where <code>async_boundary</code> fires) and as a nested step (where it warns and is ignored) \u2014 no reconfiguration needed.</p>"},{"location":"PIPELINE_DESIGN/#3-branch-parallelism-concurrent-work-on-the-same-sample","title":"3. Branch parallelism \u2014 concurrent work on the same sample","text":"<p>Problem: Two independent steps could run at the same time on the same sample (e.g. reflect and log), but a linear pipeline forces them to be sequential.</p> <p>Solution: <code>Branch</code> forks the context, runs each sub-pipeline in parallel, then joins before the next step. In sync mode it uses <code>ThreadPoolExecutor</code>; in async mode it uses <code>asyncio.gather()</code>.</p> <pre><code>pipe = (\n    Pipeline()\n    .then(EvaluateStep())\n    .branch(\n        Pipeline().then(ReflectStep()),   # runs in parallel\n        Pipeline().then(LogStep()),       # runs in parallel\n    )\n    .then(UpdateStep())   # waits for both branches\n)\n</code></pre> <pre><code># Branch internals (async mode)\nasync def __call__(self, ctx: StepContext) -&gt; StepContext:\n    results = await asyncio.gather(\n        *[p(ctx) for p in self.pipelines],\n        return_exceptions=True,   # all branches run to completion even if one fails\n    )\n    failures = [r for r in results if isinstance(r, BaseException)]\n    if failures:\n        raise BranchError(failures)   # caller sees all branch failures, not just the first\n    return self.merge(results)\n</code></pre> <p><code>return_exceptions=True</code> is required for consistent error handling: without it, the first branch failure cancels all remaining branches and the <code>SampleResult</code> would silently drop their work. With it, all branches complete and the runner captures the full failure set.</p> <p>This type is about latency within a single sample. Nothing moves to the next sample \u2014 the pipeline waits for the join before continuing.</p>"},{"location":"PIPELINE_DESIGN/#rule-of-thumb","title":"Rule of thumb","text":"Question Answer Does the step wait on I/O? <code>async def __call__</code> Do I want to process more samples while previous ones are still learning? <code>async_boundary</code> on the step where the handoff happens Can two steps on the same sample run simultaneously? <code>Branch</code> Do I want N samples going through the pipeline at the same time? <code>workers=N</code> on <code>run()</code> <p>Each mechanism is independent. They compose freely \u2014 you can have async steps inside branches, behind an <code>async_boundary</code>, run with multiple workers.</p>"},{"location":"PIPELINE_DESIGN/#concurrency-model","title":"Concurrency Model","text":"<p>Parallelism is declared on the step, not the pipeline. The pipeline executor reads these at runtime:</p> <pre><code>class ReflectStep:\n    async_boundary = True   # hand off to background threads from here\n    max_workers = 3         # up to 3 running in parallel\n\nclass UpdateStep:\n    max_workers = 1         # must serialize (writes to shared skillbook)\n</code></pre> <p>Fan-out (same step, different samples): Controlled by <code>max_workers</code> on the step. Each step class has a single shared <code>ThreadPoolExecutor</code> \u2014 <code>ReflectStep.max_workers = 3</code> means one pool of 3 threads regardless of how many pipeline instances are running.</p> <p>Pipeline split (pipelining across samples): <code>async_boundary = True</code> on a step tells the runner to hand off everything from that step onwards to background threads, freeing the caller to start the next sample immediately.</p> <pre><code>sample 1:  [AgentStep] [EvaluateStep] \u2500\u2500\u25ba [ReflectStep] [UpdateStep]\nsample 2:  [AgentStep] [EvaluateStep] \u2500\u2500\u25ba ...             (background)\n                                      \u2191\n                               async_boundary\n</code></pre> <p>This replaces the hardcoded <code>steps[:2]</code> / <code>steps[2:]</code> split that existed in the old <code>AsyncLearningPipeline</code>.</p>"},{"location":"PIPELINE_DESIGN/#workers-vs-max_workers-independent-pools","title":"workers vs max_workers \u2014 independent pools","text":"<p>These two knobs control different thread pools and do not interact:</p> Knob Pool Controls <code>pipe.run(contexts, workers=N)</code> foreground pool how many contexts run through pre-boundary steps simultaneously <code>step.max_workers = K</code> background pool per step class how many instances of that step run in the background simultaneously <p>A sample leaves the foreground pool when it crosses the <code>async_boundary</code> point and enters the background step's pool. With <code>workers=4</code> and <code>ReflectStep.max_workers=3</code>, you can have 4 samples in Agent/Evaluate and 3 reflections running concurrently \u2014 two separate pools, no multiplication.</p> <p>Mental model: <code>workers</code> controls throughput into the pipeline; <code>max_workers</code> controls throughput through each slow background step.</p> <p>LLM rate limits: <code>workers</code> and <code>max_workers</code> are independent pools, but total concurrent outbound LLM calls = foreground calls + background calls. With <code>workers=4</code> and <code>ReflectStep.max_workers=3</code>, up to 7 LLM requests may be in-flight simultaneously. Account for this when configuring per-provider rate limits.</p>"},{"location":"PIPELINE_DESIGN/#error-handling","title":"Error Handling","text":"<p>Failure semantics differ depending on which side of the <code>async_boundary</code> a step is on.</p> <p>Foreground steps (before the boundary): the runner catches exceptions per sample and records them in a <code>SampleResult</code>. The pipeline then moves to the next sample.</p> <pre><code># Pipeline runner (foreground loop)\nfor ctx in contexts:\n    try:\n        for step in self.foreground_steps:\n            ctx = step(ctx)\n        self._submit_to_background(ctx)\n        results.append(SampleResult(sample=ctx.sample, output=ctx, error=None, failed_at=None))\n    except Exception as e:\n        results.append(SampleResult(sample=ctx.sample, output=None, error=e, failed_at=type(step).__name__))\n</code></pre> <p>Background steps (after the boundary): the caller has already moved on, so exceptions cannot propagate. Background failures are captured and attached to the <code>SampleResult</code> \u2014 nothing is dropped silently.</p> <pre><code>@dataclass\nclass SampleResult:\n    sample: Any\n    output: StepContext | None     # None if a step failed\n    error: Exception | None        # set if any step failed\n    failed_at: str | None          # name of the step class that failed\n    cause: Exception | None = None # for BranchError: the inner step exception\n</code></pre> <p>Every sample produces a result \u2014 either successful with <code>output</code> set, or failed with <code>error</code> and <code>failed_at</code> set. After <code>run()</code> completes (or after <code>wait_for_learning()</code>), callers can inspect results for failures.</p> <p>When a <code>Branch</code> step fails, <code>failed_at</code> is <code>\"Branch\"</code> and <code>error</code> is a <code>BranchError</code>. <code>cause</code> carries the inner exception from the failing branch so callers can see which inner step actually failed, not just the outer wrapper.</p> <p>Retry logic is the responsibility of individual steps, not the pipeline.</p> <p>Shutdown: <code>wait_for_background(timeout=N)</code> raises <code>TimeoutError</code> if background steps have not drained within <code>N</code> seconds. Individual step implementations are responsible for their own per-call timeouts (e.g. LLM API call timeouts).</p> <p>Monitoring: <code>background_stats()</code> returns a <code>dict</code> with <code>active</code> and <code>completed</code> counts for background threads. Thread-safe \u2014 can be called from any thread while the pipeline is running. This is the public API for monitoring background progress; callers should not access <code>_bg_lock</code> or <code>_bg_threads</code> directly.</p>"},{"location":"PIPELINE_DESIGN/#summary-table","title":"Summary Table","text":"Concept Unit Threading Communication <code>Step</code> single unit of work always sync via <code>StepContext</code> <code>Pipeline</code> ordered step list for one input <code>workers=N</code> across inputs via <code>StepContext</code> <code>Branch</code> parallel pipeline list always parallel internally copy + merge of <code>StepContext</code> <code>Pipeline</code> as a <code>Step</code> reuse / nesting inherits parent context via <code>StepContext</code>"},{"location":"PIPELINE_DESIGN/#what-was-rejected-and-why","title":"What Was Rejected and Why","text":"<p><code>PipelineProcess</code> (external wrapper): Adding a separate class to wrap pipelines with executor/queue machinery was considered. Rejected \u2014 it adds an indirection layer without benefit for this project's use case. Concurrency is declared on steps instead.</p> <p>Special async pipeline subclass: Having an <code>AsyncPipeline</code> type was considered. Rejected \u2014 it mixes sequential logic with concurrency concerns in the same class. The <code>async_boundary</code> marker on steps is data-driven and doesn't require subclassing.</p> <p>Full DAG executor (auto-inferred parallelism): The <code>requires</code>/<code>provides</code> graph already contains enough information to infer which steps can run in parallel. Deferred \u2014 <code>Branch</code> covers the explicit fork/join case; automatic DAG inference can be added later if needed.</p> <p>Alternative <code>requires</code>/<code>provides</code> declaration styles: Four alternatives to plain set class attributes were considered:</p> <ul> <li><code>__init_subclass__</code> keyword args (<code>class MyStep(Step, requires={\"agent_output\"})</code>): moves the declaration to the class header but requires inheriting from a base <code>Step</code> class, eliminating the structural Protocol advantage \u2014 any object with the right attributes is a step without needing to inherit anything.</li> <li><code>ClassVar</code> annotations (<code>requires: ClassVar[frozenset[str]] = ...</code>): more type-checker friendly but adds verbosity with no semantic change.</li> <li>Function decorator wrapping <code>__call__</code>: removes class boilerplate for stateless steps but introduces two styles (decorated functions vs classes with collaborators like <code>self.reflector</code>), inconsistency not worth the reduction.</li> <li>Decomposed signature / Hamilton-style (steps receive named fields as parameters instead of <code>StepContext</code>): elegant zero-annotation contracts \u2014 <code>requires</code> and <code>provides</code> are inferred from function signature at zero cost. Rejected because it loses explicit ordering control (order is inferred from data dependencies, not declared; independent steps have undefined order), collapses the two-tier <code>StepContext</code>/<code>metadata</code> structure into a flat dict (integration-specific data collides with shared fields), and makes side-effect steps with no consumed output impossible to anchor in the sequence.</li> </ul> <p>Plain set class attributes with pipeline normalization to <code>frozenset</code> at construction time is the right balance: explicit, readable, no inheritance required, and the ordering and context model stay intact.</p>"},{"location":"PIPELINE_DESIGN/#external-libraries-considered","title":"External Libraries Considered","text":"<p>This pattern is known as Pipes and Filters. Several open source libraries implement variants of it. None were adopted \u2014 reasons below.</p> <p>Kedro \u2014 closest to the <code>requires</code>/<code>provides</code> model. Nodes declare explicit named inputs and outputs; pipelines are composable. The gap: requires a \"data catalog\" abstraction for named datasets, has no <code>async_boundary</code> concept, and is oriented toward ML/ETL rather than agentic loops. Fighting the data catalog to pass a <code>StepContext</code> would cost more than writing the primitives cleanly.</p> <p>Hamilton \u2014 lightest-weight equivalent. Functions declare inputs as parameters and outputs as return types; the framework infers the DAG. No server, no UI. The gap: no built-in async boundary, no fork/join <code>Branch</code>, no per-step <code>max_workers</code>. Gets contract validation for free but requires building all concurrency from scratch anyway.</p> <p>Pypeln \u2014 designed for exactly the \"process N samples through concurrent stages\" problem. Has sync, thread, and async modes. The gap: no typed contracts, no <code>Branch</code>, no nested pipelines. Gets the <code>async_boundary</code>-style throughput but not the structural guarantees.</p> <p>Dagster \u2014 closest overall feature set. Ops (\u2248 Steps) with typed inputs/outputs, jobs (\u2248 Pipelines), graph-based branching. The gap: it is a platform, not a library. Brings a scheduler, UI, asset catalog, and significant operational overhead. Too heavy to embed inside ACE.</p> <p>Conclusion: The specific combination of <code>async_boundary</code>, per-step <code>max_workers</code>, <code>Pipeline</code>-as-<code>Step</code> nesting, and <code>SampleResult</code> error wrapping is not provided by any of the above out of the box. Adapting any of them would cost as much as writing the ~300-line core cleanly.</p> <p>What is borrowed rather than written: <code>concurrent.futures.ThreadPoolExecutor</code> for the background step pools, and <code>asyncio.gather</code> (or <code>anyio</code> task groups) for <code>Branch</code> internals. These are well-tested primitives that are not reinvented.</p>"},{"location":"PROMPTS/","title":"ACE Framework - Prompt Template Guide","text":"<p>This guide explains the different prompt versions available in ACE, their use cases, and how to migrate between versions.</p>"},{"location":"PROMPTS/#table-of-contents","title":"Table of Contents","text":"<ul> <li>Prompt Versions</li> <li>Template Variables</li> <li>Version Comparison</li> <li>Migration Guide</li> <li>Custom Prompts</li> </ul>"},{"location":"PROMPTS/#prompt-versions","title":"Prompt Versions","text":""},{"location":"PROMPTS/#v10-simple-acepromptspy","title":"v1.0 (Simple) - <code>ace/prompts.py</code>","text":"<p>Status: \u2705 Maintained Use Case: Quick starts, tutorials, minimal examples Lines of Code: 149</p> <p>Characteristics: - Simple, straightforward templates - Minimal formatting and structure - Best for understanding ACE fundamentals - Lower token usage - Suitable for weaker models (GPT-3.5, etc.)</p> <p>Example: <pre><code>from ace.prompts import AGENT_PROMPT\nfrom ace import Agent, LiteLLMClient\n\nllm = LiteLLMClient(model=\"gpt-3.5-turbo\")\nagent = Agent(llm, prompt_template=AGENT_PROMPT)\n</code></pre></p>"},{"location":"PROMPTS/#v20-advanced-aceprompts_v2py","title":"v2.0 (Advanced) - <code>ace/prompts_v2.py</code>","text":"<p>Status: \u26a0\ufe0f DEPRECATED (use v2.1 instead) Use Case: None - superseded by v2.1 Lines of Code: 984</p> <p>Deprecation Notice: <pre><code># Will emit DeprecationWarning\nfrom ace.prompts_v2 import AGENT_V2_PROMPT\n</code></pre></p> <p>Why Deprecated: v2.1 includes all v2.0 features plus MCP enhancements and better error handling. There's no reason to use v2.0 over v2.1.</p>"},{"location":"PROMPTS/#v21-recommended-aceprompts_v2_1py","title":"v2.1 (Recommended) - <code>ace/prompts_v2_1.py</code>","text":"<p>Status: \u2705 Recommended for production Use Case: Production systems, advanced applications, best performance Lines of Code: 1469</p> <p>Characteristics: - State-of-the-art prompt engineering - MCP (Model Context Protocol) techniques - Identity headers with metadata - Hierarchical organization - Meta-cognitive instructions - Enhanced error handling - Optimized for Claude 3.5, GPT-4, and similar models</p> <p>Example: <pre><code>from ace.prompts_v2_1 import PromptManager\n\n# Use the recommended prompts\nprompt_mgr = PromptManager()\nagent = Agent(llm, prompt_template=prompt_mgr.get_agent_prompt())\nreflector = Reflector(llm, prompt_template=prompt_mgr.get_reflector_prompt())\nskill_manager = SkillManager(llm, prompt_template=prompt_mgr.get_skill_manager_prompt())\n</code></pre></p>"},{"location":"PROMPTS/#version-comparison","title":"Version Comparison","text":"<p>Quick guide to choosing the right prompt version:</p> Feature v1.0 v2.0 (Deprecated) v2.1 (Recommended) Status Stable Deprecated Recommended Performance Baseline +12% vs v1 +17% vs v1 Lines of Code 146 969 1,469 Use Case Tutorials, simple tasks N/A (use v2.1) Production systems MCP Support \u274c No \u274c No \u2705 Yes Error Handling Basic Enhanced Advanced Meta-Cognition None Basic Advanced Examples Included \u274c No \u2705 Yes \u2705 Yes Identity Headers \u274c No \u2705 Yes \u2705 Enhanced Validation Basic Strict Strict + Recovery Best For Learning ACE basics Don't use All production use <p>Recommendation: Use v2.1 for all new projects. Only use v1 for educational/tutorial purposes where simplicity is more important than performance.</p> <p>Migration: Switching from v1 \u2192 v2.1 is straightforward: <pre><code># Before (v1)\nfrom ace.prompts import AGENT_PROMPT\nagent = Agent(llm, prompt_template=AGENT_PROMPT)\n\n# After (v2.1)\nfrom ace.prompts_v2_1 import PromptManager\nmgr = PromptManager()\nagent = Agent(llm, prompt_template=mgr.get_agent_prompt())\n</code></pre></p>"},{"location":"PROMPTS/#template-variables","title":"Template Variables","text":"<p>All ACE prompts use Python's <code>.format()</code> syntax with these variables:</p>"},{"location":"PROMPTS/#agent-prompts","title":"Agent Prompts","text":"Variable Type Description Required <code>{skillbook}</code> str Formatted skillbook skills \u2705 Yes <code>{question}</code> str The question to answer \u2705 Yes <code>{context}</code> str Additional context (optional) \u274c No <code>{reflection}</code> str Prior reflection (optional) \u274c No <p>Output Format (JSON): <pre><code>{\n  \"reasoning\": \"Step-by-step thinking process\",\n  \"final_answer\": \"The actual answer\",\n  \"skill_ids\": [\"skill1\", \"skill2\"]\n}\n</code></pre></p>"},{"location":"PROMPTS/#reflector-prompts","title":"Reflector Prompts","text":"Variable Type Description Required <code>{skillbook}</code> str Formatted skillbook skills \u2705 Yes <code>{question}</code> str Original question \u2705 Yes <code>{context}</code> str Additional context \u274c No <code>{agent_output}</code> str Agent's JSON output \u2705 Yes <code>{feedback}</code> str Environment feedback \u2705 Yes <code>{ground_truth}</code> str Expected answer (optional) \u274c No <p>Output Format (JSON): <pre><code>{\n  \"analysis\": \"Analysis of what went wrong/right\",\n  \"skill_tags\": [\n    {\"id\": \"skill1\", \"tag\": \"helpful\"},\n    {\"id\": \"skill2\", \"tag\": \"harmful\"}\n  ]\n}\n</code></pre></p>"},{"location":"PROMPTS/#skillmanager-prompts","title":"SkillManager Prompts","text":"Variable Type Description Required <code>{skillbook}</code> str Current skillbook state \u2705 Yes <code>{reflection}</code> str Reflector's analysis \u2705 Yes <code>{recent_reflections}</code> str Past N reflections \u274c No <p>Output Format (JSON): <pre><code>{\n  \"updates\": [\n    {\"operation\": \"ADD\", \"section\": \"Math\", \"content\": \"Always check units\"},\n    {\"operation\": \"UPDATE\", \"skill_id\": \"s1\", \"content\": \"Revised strategy\"},\n    {\"operation\": \"TAG\", \"skill_id\": \"s2\", \"tag\": \"helpful\", \"increment\": 1},\n    {\"operation\": \"REMOVE\", \"skill_id\": \"s3\"}\n  ]\n}\n</code></pre></p>"},{"location":"PROMPTS/#version-comparison_1","title":"Version Comparison","text":"Feature v1.0 v2.0 v2.1 Token Usage Low High High Complexity Simple Complex Complex Performance Good Better Best Error Handling Basic Good Excellent MCP Techniques \u274c \u274c \u2705 Meta-Cognitive \u274c \u2705 \u2705 Production Ready \u2705 \u26a0\ufe0f \u2705 Status Maintained Deprecated Recommended"},{"location":"PROMPTS/#performance-benchmarks","title":"Performance Benchmarks","text":"<p>Based on internal testing (200 samples, Claude Sonnet 4.5):</p> Metric v1.0 v2.1 Improvement Success Rate 72% 89% +17% JSON Parse Errors 8% 1% -7% Avg Tokens/Call 850 1200 +41% Quality Score 7.2/10 9.1/10 +26% <p>Recommendation: Use v2.1 for production. The token increase is worth the quality gain.</p>"},{"location":"PROMPTS/#migration-guide","title":"Migration Guide","text":""},{"location":"PROMPTS/#v10-v21","title":"v1.0 \u2192 v2.1","text":"<p>Step 1: Update imports <pre><code># Before\nfrom ace.prompts import AGENT_PROMPT, REFLECTOR_PROMPT, SKILL_MANAGER_PROMPT\n\n# After\nfrom ace.prompts_v2_1 import PromptManager\nprompt_mgr = PromptManager()\n</code></pre></p> <p>Step 2: Update role initialization <pre><code># Before\nagent = Agent(llm)  # Uses v1.0 default\n\n# After\nagent = Agent(llm, prompt_template=prompt_mgr.get_agent_prompt())\nreflector = Reflector(llm, prompt_template=prompt_mgr.get_reflector_prompt())\nskill_manager = SkillManager(llm, prompt_template=prompt_mgr.get_skill_manager_prompt())\n</code></pre></p> <p>Step 3: Test and validate - Run your test suite - Monitor JSON parse success rates - Check output quality - Adjust <code>max_retries</code> if needed (v2.1 is more reliable)</p>"},{"location":"PROMPTS/#v20-v21","title":"v2.0 \u2192 v2.1","text":"<p>Minimal changes required:</p> <pre><code># Before (emits DeprecationWarning)\nfrom ace.prompts_v2 import AGENT_V2_PROMPT, REFLECTOR_V2_PROMPT, SKILL_MANAGER_V2_PROMPT\n\n# After\nfrom ace.prompts_v2_1 import PromptManager\nprompt_mgr = PromptManager()\n\nagent = Agent(llm, prompt_template=prompt_mgr.get_agent_prompt())\nreflector = Reflector(llm, prompt_template=prompt_mgr.get_reflector_prompt())\nskill_manager = SkillManager(llm, prompt_template=prompt_mgr.get_skill_manager_prompt())\n</code></pre> <p>Benefits of Upgrading: - +12% fewer JSON parse errors - Better handling of edge cases - MCP-enhanced reasoning - No deprecation warnings</p>"},{"location":"PROMPTS/#custom-prompts","title":"Custom Prompts","text":""},{"location":"PROMPTS/#creating-custom-prompts","title":"Creating Custom Prompts","text":"<p>You can create domain-specific prompts while maintaining ACE compatibility:</p> <pre><code>CUSTOM_AGENT_PROMPT = \"\"\"\nYou are a medical diagnosis assistant using ACE strategies.\n\n# Available Strategies\n{skillbook}\n\n# Patient Question\n{question}\n\n# Medical Context\n{context}\n\n# Previous Reflection\n{reflection}\n\nIMPORTANT: Return JSON with:\n- \"reasoning\": Your diagnostic reasoning\n- \"final_answer\": Your diagnosis and recommendations\n- \"skill_ids\": Strategy IDs you used (e.g., [\"med_01\", \"diag_03\"])\n\nRespond ONLY with valid JSON, no other text.\n\"\"\"\n\n# Use it\nagent = Agent(llm, prompt_template=CUSTOM_AGENT_PROMPT)\n</code></pre>"},{"location":"PROMPTS/#template-requirements","title":"Template Requirements","text":"<p>\u2705 Required: - Include all 4 variables: <code>{skillbook}</code>, <code>{question}</code>, <code>{context}</code>, <code>{reflection}</code> - Specify JSON output format clearly - List required fields: <code>reasoning</code>, <code>final_answer</code>, <code>skill_ids</code></p> <p>\u274c Avoid: - Hardcoding language-specific instructions in prompts - Overly complex nested instructions - Ambiguous output format requirements</p>"},{"location":"PROMPTS/#testing-custom-prompts","title":"Testing Custom Prompts","text":"<pre><code>import json\nfrom ace import Agent, Skillbook, Sample\n\n# Create test setup\nllm = LiteLLMClient(model=\"claude-sonnet-4-5-20250929\")\nskillbook = Skillbook()\nskillbook.add_skill(section=\"Testing\", content=\"Always validate output format\")\n\nagent = Agent(llm, prompt_template=CUSTOM_AGENT_PROMPT)\n\n# Test\noutput = agent.generate(\n    question=\"Test question\",\n    context=\"Test context\",\n    skillbook=skillbook,\n    reflection=None\n)\n\n# Validate\nassert output.reasoning\nassert output.final_answer\nassert isinstance(output.skill_ids, list)\nprint(\"\u2713 Custom prompt works!\")\n</code></pre>"},{"location":"PROMPTS/#advanced-topics","title":"Advanced Topics","text":""},{"location":"PROMPTS/#domain-specific-sections","title":"Domain-Specific Sections","text":"<p>Organize skillbook skills by domain:</p> <pre><code>skillbook.add_skill(\n    section=\"Medical/Diagnosis\",\n    content=\"Check for common symptoms first\"\n)\n\nskillbook.add_skill(\n    section=\"Medical/Treatment\",\n    content=\"Consider contraindications\"\n)\n\nskillbook.add_skill(\n    section=\"Legal/Compliance\",\n    content=\"Verify HIPAA requirements\"\n)\n</code></pre>"},{"location":"PROMPTS/#multi-language-support","title":"Multi-Language Support","text":"<p>v2.1 prompts work with non-English content:</p> <pre><code># Question and context can be in any language\noutput = agent.generate(\n    question=\"\u00bfCu\u00e1l es la capital de Francia?\",\n    context=\"Responde en espa\u00f1ol\",\n    skillbook=skillbook\n)\n# Output will be in Spanish\n</code></pre>"},{"location":"PROMPTS/#troubleshooting","title":"Troubleshooting","text":""},{"location":"PROMPTS/#high-json-parse-failure-rate","title":"High JSON Parse Failure Rate","text":"<p>Symptom: Frequent <code>RuntimeError: Agent failed to produce valid JSON</code></p> <p>Solutions: 1. Upgrade to v2.1 prompts 2. Increase <code>max_retries</code>: <code>Agent(llm, max_retries=5)</code> 3. Use a more capable model (Claude 3.5 Sonnet, GPT-4 Turbo) 4. Add custom retry prompt if using non-English</p>"},{"location":"PROMPTS/#empty-skill-ids","title":"Empty Skill IDs","text":"<p>Symptom: <code>skill_ids</code> is always <code>[]</code></p> <p>Cause: Skillbook is empty or skills not referenced</p> <p>Solution: <pre><code># Ensure skillbook has skills\nprint(f\"Skillbook has {len(skillbook.skills())} skills\")\n\n# Check skillbook format\nprint(skillbook.as_prompt())\n</code></pre></p>"},{"location":"PROMPTS/#poor-quality-answers","title":"Poor Quality Answers","text":"<p>Symptom: Agent produces generic/unhelpful answers</p> <p>Solutions: 1. Add more specific skills to skillbook 2. Provide richer context 3. Upgrade to v2.1 for better reasoning 4. Increase model temperature for creativity: <code>LiteLLMClient(model=\"...\", temperature=0.7)</code></p>"},{"location":"PROMPTS/#best-practices","title":"Best Practices","text":"<p>\u2705 Do: - Use v2.1 for production systems - Provide rich context in <code>context</code> parameter - Test prompts with your specific domain - Monitor JSON parse success rates</p> <p>\u274c Don't: - Use v2.0 (deprecated) - Hardcode language-specific instructions in templates - Skip testing custom prompts thoroughly - Ignore JSON parse errors silently</p>"},{"location":"PROMPTS/#references","title":"References","text":"<ul> <li>Research Paper: Agentic Context Engineering</li> <li>API Documentation: See <code>ace/roles.py</code> docstrings</li> <li>Examples: <code>examples/</code> directory</li> <li>Changelog: See <code>CHANGELOG.md</code></li> </ul> <p>Questions or Issues?</p> <ul> <li>GitHub Issues: https://github.com/kayba-ai/agentic-context-engine/issues</li> <li>Documentation: https://github.com/kayba-ai/agentic-context-engine#readme</li> </ul>"},{"location":"PROMPT_ENGINEERING/","title":"ACE Prompt Engineering Guide","text":""},{"location":"PROMPT_ENGINEERING/#state-of-the-art-prompt-design-principles","title":"State-of-the-Art Prompt Design Principles","text":"<p>This guide documents best practices for creating and optimizing ACE prompts, based on analysis of 80+ production prompts from leading AI systems (GPT-5, Claude 3.5, and others).</p>"},{"location":"PROMPT_ENGINEERING/#table-of-contents","title":"Table of Contents","text":"<ol> <li>Core Principles</li> <li>Prompt Structure</li> <li>Writing Effective Instructions</li> <li>Domain-Specific Optimizations</li> <li>Common Anti-Patterns</li> <li>Testing and Iteration</li> <li>Migration from v1 to v2</li> </ol>"},{"location":"PROMPT_ENGINEERING/#core-principles","title":"Core Principles","text":""},{"location":"PROMPT_ENGINEERING/#1-identity-headers-are-essential","title":"1. Identity Headers Are Essential","text":"<p>Always start prompts with clear identity and metadata:</p> <pre><code># GOOD - Clear identity with version and capabilities\n\"\"\"\n# Identity and Metadata\nYou are ACE Agent v2.0, an expert problem-solving agent.\nPrompt Version: 2.0.0\nCurrent Date: {current_date}\nMode: Strategic Problem Solving\nConfidence Threshold: 0.7\n\"\"\"\n\n# BAD - Vague introduction\n\"\"\"\nYou are an assistant that helps solve problems.\n\"\"\"\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#2-hierarchical-organization","title":"2. Hierarchical Organization","text":"<p>Structure prompts with clear sections and subsections:</p> <pre><code># GOOD - Hierarchical structure\n\"\"\"\n## Core Responsibilities\n1. Analyze questions using skillbook\n2. Apply relevant strategies\n\n### Strategy Selection Protocol\n- ONLY use skills with confidence &gt; 0.7\n- NEVER apply conflicting strategies\n\n### Output Requirements\nReturn JSON with exact schema...\n\"\"\"\n\n# BAD - Flat structure\n\"\"\"\nAnalyze questions and apply strategies. Use skills with high confidence.\nReturn JSON with your answer.\n\"\"\"\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#3-concrete-examples-over-abstract-principles","title":"3. Concrete Examples Over Abstract Principles","text":"<p>Show exactly what you want with good/bad examples:</p> <pre><code># GOOD - Concrete examples\n\"\"\"\n### Good Example:\n{\n  \"reasoning\": \"1. Breaking down 15 \u00d7 24: This is multiplication. 2. Using decomposition: 15 \u00d7 (20 + 4)...\",\n  \"skill_ids\": [\"skill_023\"],\n  \"final_answer\": \"360\"\n}\n\n### Bad Example (DO NOT DO THIS):\n{\n  \"reasoning\": \"Using the skillbook strategies, the answer is clear.\",\n  \"final_answer\": \"360\"\n}\n\"\"\"\n\n# BAD - Abstract guidance\n\"\"\"\nProvide clear reasoning and cite relevant skills.\n\"\"\"\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#4-emphatic-capitalization-for-critical-requirements","title":"4. Emphatic Capitalization for Critical Requirements","text":"<p>Use MUST/NEVER/ALWAYS only for frequently violated rules:</p> <pre><code># GOOD - Emphatic for critical requirements\n\"\"\"\n**MUST** include step-by-step reasoning\n**NEVER** skip intermediate calculations\n**ALWAYS** cite specific skill IDs\n\"\"\"\n\n# BAD - Overuse dilutes effectiveness\n\"\"\"\nALWAYS be helpful. NEVER make mistakes. MUST think carefully.\n\"\"\"\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#prompt-structure","title":"Prompt Structure","text":""},{"location":"PROMPT_ENGINEERING/#standard-v2-template","title":"Standard v2 Template","text":"<pre><code>TEMPLATE = \"\"\"\n# Identity and Metadata\n[Role, version, capabilities, date]\n\n## Core Mission\n[One-sentence primary objective]\n\n## Input Context\n[Structured presentation of inputs]\n\n## Processing Protocol\n[Numbered steps or decision tree]\n\n## Critical Requirements\n**MUST**: [Absolute requirements]\n**NEVER**: [Explicit prohibitions]\n\n## Output Format\n[Exact schema with examples]\n\n## Error Recovery\n[Fallback procedures]\n\"\"\"\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#decision-trees-for-complex-logic","title":"Decision Trees for Complex Logic","text":"<pre><code># GOOD - Clear conditional logic\n\"\"\"\nExecute in order - use FIRST that applies:\n\n### 1. SUCCESS_CASE_DETECTED\nIF prediction matches ground truth:\n   \u2192 Tag strategies as helpful\n   \u2192 Extract reusable patterns\n\n### 2. CALCULATION_ERROR_DETECTED\nIF mathematical error in reasoning:\n   \u2192 Pinpoint error location\n   \u2192 Identify root cause\n\"\"\"\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#writing-effective-instructions","title":"Writing Effective Instructions","text":""},{"location":"PROMPT_ENGINEERING/#1-specify-anti-patterns-explicitly","title":"1. Specify Anti-Patterns Explicitly","text":"<pre><code># GOOD - Explicit anti-patterns\n\"\"\"\n**NEVER** use these phrases:\n- \"Based on the skillbook\"\n- \"The model was wrong\"\n- \"Should have known better\"\n- \"Obviously incorrect\"\n\"\"\"\n\n# BAD - Vague guidance\n\"\"\"\nAvoid unhelpful phrases.\n\"\"\"\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#2-include-meta-cognitive-instructions","title":"2. Include Meta-Cognitive Instructions","text":"<pre><code># GOOD - Self-assessment thresholds\n\"\"\"\nONLY use strategies if confidence &gt; 0.7\nIf uncertain about approach, state \"low_confidence\" in output\nVerify each reasoning step before proceeding\n\"\"\"\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#3-procedural-workflows","title":"3. Procedural Workflows","text":"<pre><code># GOOD - Numbered procedures\n\"\"\"\n## Solution Process\n1. Classify problem type (arithmetic/algebra/geometry)\n2. Select appropriate method based on classification\n3. Apply method with ALL intermediate steps shown\n4. Verify answer using alternative approach\n5. Format output according to schema\n\"\"\"\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#4-completeness-requirements","title":"4. Completeness Requirements","text":"<pre><code># GOOD - Prevent partial outputs\n\"\"\"\n**MUST** output COMPLETE code even if lengthy\n**NEVER** use \"...\" or \"rest remains the same\"\n**ALWAYS** include ALL import statements\n\"\"\"\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#domain-specific-optimizations","title":"Domain-Specific Optimizations","text":""},{"location":"PROMPT_ENGINEERING/#mathematics-prompts","title":"Mathematics Prompts","text":"<pre><code># Key additions for math domain:\n\"\"\"\n## Mathematical Protocols\n- ALWAYS show intermediate steps\n- VERIFY calculations twice\n- Use standard order of operations (PEMDAS)\n- State units in final answer\n- Round only final result, not intermediate values\n\"\"\"\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#code-generation-prompts","title":"Code Generation Prompts","text":"<pre><code># Key additions for code domain:\n\"\"\"\n## Code Requirements\n- Write COMPLETE, runnable code\n- Include error handling for edge cases\n- Follow language idioms and style guides\n- Add type hints where applicable\n- Include basic test cases\n\"\"\"\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#reasoninganalysis-prompts","title":"Reasoning/Analysis Prompts","text":"<pre><code># Key additions for reasoning:\n\"\"\"\n## Analytical Framework\n- Identify assumptions explicitly\n- Consider multiple perspectives\n- Acknowledge uncertainty ranges\n- Distinguish correlation from causation\n- Cite evidence for claims\n\"\"\"\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#common-anti-patterns","title":"Common Anti-Patterns","text":""},{"location":"PROMPT_ENGINEERING/#1-vague-instructions","title":"1. Vague Instructions","text":"<pre><code># BAD\n\"Be careful with your analysis\"\n\"Think step by step\"\n\"Consider all aspects\"\n\n# GOOD\n\"Verify arithmetic at each step using reverse operations\"\n\"Follow this 5-step analysis procedure: [specific steps]\"\n\"Address these specific aspects: [enumerated list]\"\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#2-over-reliance-on-training","title":"2. Over-Reliance on Training","text":"<pre><code># BAD\n\"Use your knowledge to solve this\"\n\"Apply appropriate methods\"\n\n# GOOD\n\"Use methods from the skillbook section 'algebra'\"\n\"Apply the quadratic formula: x = (-b \u00b1 \u221a(b\u00b2-4ac))/2a\"\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#3-missing-error-recovery","title":"3. Missing Error Recovery","text":"<pre><code># BAD\n\"Return JSON with the answer\"\n\n# GOOD\n\"\"\"\nReturn JSON with exact schema.\nIf JSON generation fails:\n1. Check all required fields present\n2. Escape special characters\n3. Validate number formats\nMaximum retries: 3\n\"\"\"\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#4-ambiguous-output-format","title":"4. Ambiguous Output Format","text":"<pre><code># BAD\n\"Respond with your analysis and conclusion\"\n\n# GOOD\n\"\"\"\nReturn ONLY valid JSON:\n{\n  \"analysis\": \"&lt;numbered points&gt;\",\n  \"conclusion\": \"&lt;one sentence&gt;\",\n  \"confidence\": 0.0-1.0\n}\n\"\"\"\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#testing-and-iteration","title":"Testing and Iteration","text":""},{"location":"PROMPT_ENGINEERING/#1-ab-testing-framework","title":"1. A/B Testing Framework","text":"<pre><code>from ace.prompts_v2_1 import PromptManager\n\n# Test different versions\nmanager = PromptManager()\n\n# Version A - Standard v2.1\nprompt_a = manager.get_agent_prompt(version=\"2.1\")\n\n# Version B - Custom variant\nprompt_b = custom_prompt_with_modifications\n\n# Track performance\nresults_a = run_tests_with_prompt(prompt_a)\nresults_b = run_tests_with_prompt(prompt_b)\n\n# Compare metrics\ncompare_accuracy(results_a, results_b)\ncompare_confidence_calibration(results_a, results_b)\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#2-validation-utilities","title":"2. Validation Utilities","text":"<pre><code>from ace.prompts_v2_1 import validate_prompt_output\n\n# Test output compliance\noutput = llm.generate(prompt)\nis_valid, errors = validate_prompt_output(output, role=\"agent\")\n\nif not is_valid:\n    print(f\"Output validation failed: {errors}\")\n    # Iterate on prompt to fix common failures\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#3-performance-metrics","title":"3. Performance Metrics","text":"<p>Track these metrics to evaluate prompt effectiveness:</p> <ul> <li>Accuracy: Correct answers / total</li> <li>Compliance: Valid JSON outputs / total</li> <li>Confidence Calibration: Correlation between confidence and accuracy</li> <li>Retry Rate: Failed attempts requiring retry</li> <li>Token Efficiency: Average tokens per response</li> </ul>"},{"location":"PROMPT_ENGINEERING/#4-iterative-refinement-process","title":"4. Iterative Refinement Process","text":"<ol> <li>Baseline: Start with v2.1 template</li> <li>Observe: Identify failure patterns</li> <li>Hypothesize: Form specific improvements</li> <li>Test: A/B test modifications</li> <li>Adopt: Integrate successful changes</li> <li>Document: Record what worked and why</li> </ol>"},{"location":"PROMPT_ENGINEERING/#migration-to-v21","title":"Migration to v2.1","text":""},{"location":"PROMPT_ENGINEERING/#quick-start","title":"Quick Start","text":"<pre><code># Old approach (v1)\nfrom ace.prompts import AGENT_PROMPT\nagent = Agent(llm, prompt_template=AGENT_PROMPT)\n\n# Recommended approach (v2.1) - +17% success rate vs v1\nfrom ace.prompts_v2_1 import PromptManager\nmanager = PromptManager(default_version=\"2.1\")\nagent = Agent(llm, prompt_template=manager.get_agent_prompt())\n</code></pre> <p>Note: v2.0 prompts are deprecated. Use v2.1 for best performance.</p>"},{"location":"PROMPT_ENGINEERING/#key-improvements","title":"Key Improvements","text":"Feature v1 v2.1 Performance Baseline +17% success rate Structure Basic sections Hierarchical with metadata Examples None Good/bad examples included Error Handling Basic JSON check Detailed recovery procedures Requirements General guidance MUST/NEVER with specifics Output Loose schema Strict schema with validation Domains One-size-fits-all Specialized variants Anti-patterns Not specified Explicitly prohibited Confidence Not tracked Built-in confidence scores MCP Support No Yes (v2.1 enhancement)"},{"location":"PROMPT_ENGINEERING/#gradual-migration-strategy","title":"Gradual Migration Strategy","text":"<ol> <li>Phase 1: Test v2 prompts with small sample</li> <li>Phase 2: A/B test v1 vs v2 on real tasks</li> <li>Phase 3: Migrate best-performing roles first</li> <li>Phase 4: Customize v2 based on your needs</li> <li>Phase 5: Fully migrate to v2 framework</li> </ol>"},{"location":"PROMPT_ENGINEERING/#best-practices-summary","title":"Best Practices Summary","text":""},{"location":"PROMPT_ENGINEERING/#do","title":"DO:","text":"<ul> <li>\u2705 Start with identity headers and metadata</li> <li>\u2705 Use hierarchical organization with clear sections</li> <li>\u2705 Provide concrete good/bad examples</li> <li>\u2705 Specify exact output schemas</li> <li>\u2705 Include error recovery procedures</li> <li>\u2705 Add domain-specific optimizations</li> <li>\u2705 List explicit anti-patterns to avoid</li> <li>\u2705 Use emphatic caps sparingly for critical rules</li> <li>\u2705 Include meta-cognitive assessment instructions</li> <li>\u2705 Test and iterate based on failure patterns</li> </ul>"},{"location":"PROMPT_ENGINEERING/#dont","title":"DON'T:","text":"<ul> <li>\u274c Write vague, abstract instructions</li> <li>\u274c Rely solely on model training</li> <li>\u274c Overuse emphatic capitalization</li> <li>\u274c Skip error handling</li> <li>\u274c Use ambiguous output formats</li> <li>\u274c Ignore domain-specific needs</li> <li>\u274c Assume one prompt fits all cases</li> <li>\u274c Deploy without validation testing</li> <li>\u274c Mix multiple concerns in one section</li> <li>\u274c Forget to version your prompts</li> </ul>"},{"location":"PROMPT_ENGINEERING/#advanced-techniques","title":"Advanced Techniques","text":""},{"location":"PROMPT_ENGINEERING/#1-prompt-chaining","title":"1. Prompt Chaining","text":"<pre><code># Break complex tasks into stages\nstage1_prompt = manager.get_agent_prompt(domain=\"analysis\")\nstage2_prompt = manager.get_agent_prompt(domain=\"synthesis\")\n\n# Chain outputs\nanalysis = agent_stage1.generate(prompt=stage1_prompt, ...)\nsynthesis = agent_stage2.generate(\n    prompt=stage2_prompt,\n    context=analysis.output,\n    ...\n)\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#2-dynamic-prompt-selection","title":"2. Dynamic Prompt Selection","text":"<pre><code>def select_prompt_by_difficulty(question: str) -&gt; str:\n    \"\"\"Select prompt variant based on problem complexity.\"\"\"\n    difficulty = assess_difficulty(question)\n\n    if difficulty &gt; 0.8:\n        return manager.get_agent_prompt(variant=\"expert\")\n    elif difficulty &gt; 0.5:\n        return manager.get_agent_prompt(variant=\"standard\")\n    else:\n        return manager.get_agent_prompt(variant=\"simple\")\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#3-prompt-versioning","title":"3. Prompt Versioning","text":"<pre><code>class VersionedPromptManager:\n    \"\"\"Track prompt performance across versions.\"\"\"\n\n    def __init__(self):\n        self.versions = {}\n        self.performance = {}\n\n    def register_version(self, version: str, prompt: str):\n        self.versions[version] = prompt\n        self.performance[version] = {\"uses\": 0, \"success\": 0}\n\n    def get_best_performing(self) -&gt; str:\n        \"\"\"Return prompt with highest success rate.\"\"\"\n        best = max(\n            self.performance.items(),\n            key=lambda x: x[1][\"success\"] / max(x[1][\"uses\"], 1)\n        )\n        return self.versions[best[0]]\n</code></pre>"},{"location":"PROMPT_ENGINEERING/#resources","title":"Resources","text":"<ul> <li>Original ACE Paper</li> <li>Prompt Engineering Best Practices</li> <li>LiteLLM Documentation</li> <li>Example Implementations - See examples directory for prompt engineering patterns</li> </ul>"},{"location":"PROMPT_ENGINEERING/#contributing","title":"Contributing","text":"<p>When contributing new prompts:</p> <ol> <li>Follow the v2 template structure</li> <li>Include at least 2 good/bad examples</li> <li>Add domain-specific optimizations if applicable</li> <li>Test with validation utilities</li> <li>Document performance improvements</li> <li>Submit with A/B test results if available</li> </ol> <p>This guide is based on analysis of production prompts from GPT-5, Claude 3.5, Grok, and 80+ other systems. It will be updated as new patterns emerge.</p>"},{"location":"QUICK_START/","title":"ACE Framework Quick Start","text":"<p>Get your first self-learning AI agent running!</p>"},{"location":"QUICK_START/#installation","title":"Installation","text":"<pre><code>pip install ace-framework\n</code></pre> <p>Set your API key:</p> <pre><code>export OPENAI_API_KEY=\"your-key-here\"\n# Or: ANTHROPIC_API_KEY, GOOGLE_API_KEY, etc.\n</code></pre>"},{"location":"QUICK_START/#integration-examples","title":"Integration Examples","text":""},{"location":"QUICK_START/#acelitellm-simple-self-improving-agent","title":"ACELiteLLM - Simple Self-Improving Agent","text":"<pre><code>from ace import ACELiteLLM\n\n# Create self-improving agent\nagent = ACELiteLLM(model=\"gpt-4o-mini\")\n\n# Ask related questions - agent learns patterns\nanswer1 = agent.ask(\"If all cats are animals, is Felix (a cat) an animal?\")\nanswer2 = agent.ask(\"If all birds fly, can penguins (birds) fly?\")  # Learns to check assumptions!\nanswer3 = agent.ask(\"If all metals conduct electricity, does copper conduct electricity?\")\n\n# View learned strategies\nprint(f\"Learned {len(agent.skillbook.skills())} reasoning skills\")\n\n# Save for reuse\nagent.save_skillbook(\"my_agent.json\")\n\n# Load and continue\nagent2 = ACELiteLLM(model=\"gpt-4o-mini\", skillbook_path=\"my_agent.json\")\n</code></pre>"},{"location":"QUICK_START/#acelangchain-wrap-langchain-chainsagents","title":"ACELangChain - Wrap LangChain Chains/Agents","text":"<p>Best for multi-step workflows and tool-using agents.</p> <pre><code>from ace import ACELangChain\n\nace_chain = ACELangChain(runnable=your_langchain_chain)\nresult = ace_chain.invoke({\"question\": \"Your task\"})  # Learns automatically\n</code></pre>"},{"location":"QUICK_START/#aceagent-browser-automation-browser-use","title":"ACEAgent - Browser Automation (browser-use)","text":"<p>Drop-in replacement for <code>browser_use.Agent</code> with automatic learning.</p> <pre><code>pip install ace-framework[browser-use]\n</code></pre> <pre><code>from ace import ACEAgent\nfrom browser_use import ChatBrowserUse\n\n# Two LLMs: ChatBrowserUse for browser, gpt-4o-mini for ACE learning\nagent = ACEAgent(\n    llm=ChatBrowserUse(),      # Browser execution\n    ace_model=\"gpt-4o-mini\"    # ACE learning\n)\n\nawait agent.run(task=\"Find top Hacker News post\")\nagent.save_skillbook(\"hn_expert.json\")\n\n# Reuse learned knowledge\nagent = ACEAgent(llm=ChatBrowserUse(), skillbook_path=\"hn_expert.json\")\nawait agent.run(task=\"New task\")  # Starts smart!\n</code></pre> <p>\u2192 Browser Use Guide</p>"},{"location":"QUICK_START/#aceclaudecode-claude-code-cli","title":"ACEClaudeCode - Claude Code CLI","text":"<p>Self-improving coding agent using Claude Code.</p> <pre><code>from ace import ACEClaudeCode\n\nagent = ACEClaudeCode(\n    working_dir=\"./my_project\",\n    ace_model=\"claude-sonnet-4-5-20250929\"  # Any LiteLLM-supported model works\n)\n\n# Execute coding tasks - agent learns from each\nresult = agent.run(task=\"Add unit tests for utils.py\")\nagent.save_skillbook(\"coding_expert.json\")\n\n# Reuse learned knowledge\nagent = ACEClaudeCode(working_dir=\"./project\", skillbook_path=\"coding_expert.json\")\n</code></pre> <p>\u2192 Claude Code Loop Example</p>"},{"location":"QUICK_START/#advanced-tutorial-understanding-ace-internals","title":"Advanced Tutorial: Understanding ACE Internals","text":"<p>Want to understand how ACE works under the hood? This section shows the full architecture with Agent, Reflector, and SkillManager roles.</p>"},{"location":"QUICK_START/#full-pipeline-example","title":"Full Pipeline Example","text":"<pre><code>from ace import OfflineACE, Agent, Reflector, SkillManager\nfrom ace import LiteLLMClient, Sample, TaskEnvironment, EnvironmentResult\n\n\n# Simple environment that checks if answer contains the ground truth\nclass SimpleEnvironment(TaskEnvironment):\n    def evaluate(self, sample, agent_output):\n        correct = str(sample.ground_truth).lower() in str(agent_output.final_answer).lower()\n        return EnvironmentResult(\n            feedback=\"Correct!\" if correct else \"Incorrect\",\n            ground_truth=sample.ground_truth\n        )\n\n\n# Initialize LLM client\nclient = LiteLLMClient(model=\"gpt-4o-mini\")\n\n# Create ACE components (three roles)\nagent = Agent(client)              # Produces answers\nreflector = Reflector(client)      # Analyzes performance\nskill_manager = SkillManager(client)  # Updates skillbook\n\n# Create adapter to orchestrate everything\nadapter = OfflineACE(agent=agent, reflector=reflector, skill_manager=skill_manager)\n\n# Create training samples\nsamples = [\n    Sample(question=\"What is the capital of France?\", context=\"\", ground_truth=\"Paris\"),\n    Sample(question=\"What is 2 + 2?\", context=\"\", ground_truth=\"4\"),\n    Sample(question=\"Who wrote Romeo and Juliet?\", context=\"\", ground_truth=\"Shakespeare\")\n]\n\n# Train the agent\nprint(\"Training agent...\")\nresults = adapter.run(samples, SimpleEnvironment(), epochs=2)\n\n# Save learned strategies\nadapter.skillbook.save_to_file(\"my_agent.json\")\nprint(f\"\u2705 Agent trained! Learned {len(adapter.skillbook.skills())} strategies\")\n\n# Test with new question\ntest_output = agent.generate(\n    question=\"What is 5 + 3?\",\n    context=\"\",\n    skillbook=adapter.skillbook\n)\nprint(f\"\\nTest question: What is 5 + 3?\")\nprint(f\"Answer: {test_output.final_answer}\")\n</code></pre> <p>Expected output: <pre><code>Training agent...\n\u2705 Agent trained! Learned 3 strategies\n\nTest question: What is 5 + 3?\nAnswer: 8\n</code></pre></p>"},{"location":"QUICK_START/#understanding-the-architecture","title":"Understanding the Architecture","text":"<p>Three ACE Roles: 1. Agent - Executes tasks using skillbook strategies 2. Reflector - Analyzes what worked/didn't work 3. SkillManager - Updates skillbook with new strategies</p> <p>Two Adaptation Modes: - OfflineACE - Train on batch of samples (shown above) - OnlineACE - Learn from each task in real-time</p>"},{"location":"QUICK_START/#next-steps","title":"Next Steps","text":""},{"location":"QUICK_START/#load-saved-agent","title":"Load Saved Agent","text":"<pre><code>from ace import ACELiteLLM\n\n# Load previously trained agent\nagent = ACELiteLLM(model=\"gpt-4o-mini\", skillbook_path=\"my_agent.json\")\n\n# Use it immediately\nanswer = agent.ask(\"New question\")\n</code></pre> <p>Or with full pipeline:</p> <pre><code>from ace import Skillbook, Agent, LiteLLMClient\n\n# Load skillbook\nskillbook = Skillbook.load_from_file(\"my_agent.json\")\n\n# Use with agent\nclient = LiteLLMClient(model=\"gpt-4o-mini\")\nagent = Agent(client)\noutput = agent.generate(\n    question=\"New question\",\n    context=\"\",\n    skillbook=skillbook\n)\n</code></pre>"},{"location":"QUICK_START/#try-different-models","title":"Try Different Models","text":"<pre><code># Anthropic Claude\nagent = ACELiteLLM(model=\"claude-3-5-sonnet-20241022\")\n\n# Google Gemini\nagent = ACELiteLLM(model=\"gemini-pro\")\n\n# Local Ollama\nagent = ACELiteLLM(model=\"ollama/llama2\")\n</code></pre>"},{"location":"QUICK_START/#common-patterns","title":"Common Patterns","text":""},{"location":"QUICK_START/#online-learning-learn-while-running","title":"Online Learning (Learn While Running)","text":"<pre><code>from ace import OnlineACE\n\nadapter = OnlineACE(\n    skillbook=skillbook,\n    agent=agent,\n    reflector=reflector,\n    skill_manager=skill_manager\n)\n\n# Process tasks one by one, learning from each\nfor task in tasks:\n    result = adapter.process(task, environment)\n</code></pre>"},{"location":"QUICK_START/#custom-evaluation","title":"Custom Evaluation","text":"<pre><code>class MathEnvironment(TaskEnvironment):\n    def evaluate(self, sample, output):\n        try:\n            result = eval(output.final_answer)\n            correct = result == sample.ground_truth\n            return EnvironmentResult(\n                feedback=f\"Result: {result}. {'\u2713' if correct else '\u2717'}\",\n                ground_truth=sample.ground_truth\n            )\n        except:\n            return EnvironmentResult(\n                feedback=\"Invalid math expression\",\n                ground_truth=sample.ground_truth\n            )\n</code></pre>"},{"location":"QUICK_START/#learn-more","title":"Learn More","text":"<ul> <li>Integration Guide - Add ACE to existing agents</li> <li>Complete Guide - Deep dive into ACE concepts</li> <li>Examples - Real-world examples</li> <li>Browser Automation - Self-improving browser agents</li> <li>LangChain Integration - Wrap chains with learning</li> <li>Custom Integration - Any agent pattern</li> </ul>"},{"location":"QUICK_START/#troubleshooting","title":"Troubleshooting","text":"<p>Import errors? <pre><code>pip install --upgrade ace-framework\n</code></pre></p> <p>API key not working? - Verify key is correct: <code>echo $OPENAI_API_KEY</code> - Try different model: <code>ACELiteLLM(model=\"gpt-3.5-turbo\")</code></p> <p>Need help? - GitHub Issues - Discord Community</p> <p>Ready to build production agents? Check out the Integration Guide for browser automation, LangChain, and custom agent patterns.</p>"},{"location":"SETUP_GUIDE/","title":"\u2699\ufe0f ACE Framework Setup Guide","text":"<p>Quick setup and configuration guide for ACE Framework.</p>"},{"location":"SETUP_GUIDE/#requirements","title":"Requirements","text":"<ul> <li>Python 3.12</li> <li>API key for your LLM provider (OpenAI, Anthropic, Google, etc.)</li> </ul> <p>Check Python version: <pre><code>python --version  # Should show 3.12\n</code></pre></p>"},{"location":"SETUP_GUIDE/#installation","title":"Installation","text":""},{"location":"SETUP_GUIDE/#for-users","title":"For Users","text":"<pre><code># Basic installation\npip install ace-framework\n\n# With optional features\npip install ace-framework[instructor]     # Structured outputs (Instructor)\npip install ace-framework[langchain]      # LangChain integration\npip install ace-framework[browser-use]    # Browser automation\npip install ace-framework[claude-code]    # Claude Code CLI integration\npip install ace-framework[observability]  # Opik monitoring + cost tracking\npip install ace-framework[deduplication]  # Skill deduplication (embeddings)\npip install ace-framework[transformers]   # Local model support\npip install ace-framework[all]            # All features\n</code></pre>"},{"location":"SETUP_GUIDE/#for-contributors","title":"For Contributors","text":"<pre><code>git clone https://github.com/kayba-ai/agentic-context-engine\ncd agentic-context-engine\nuv sync  # Installs everything automatically (10-100x faster than pip)\n</code></pre>"},{"location":"SETUP_GUIDE/#api-key-setup","title":"API Key Setup","text":""},{"location":"SETUP_GUIDE/#option-1-environment-variable-recommended","title":"Option 1: Environment Variable (Recommended)","text":"<pre><code># Set in your shell\nexport OPENAI_API_KEY=\"sk-...\"\n\n# Or create .env file\necho \"OPENAI_API_KEY=sk-...\" &gt; .env\n</code></pre> <p>Load in Python: <pre><code>from dotenv import load_dotenv\nload_dotenv()  # Loads from .env file\n</code></pre></p>"},{"location":"SETUP_GUIDE/#option-2-direct-in-code","title":"Option 2: Direct in Code","text":"<pre><code>from ace import LiteLLMClient\n\nclient = LiteLLMClient(\n    model=\"gpt-4o-mini\",\n    api_key=\"your-key-here\"  # Not recommended for production\n)\n</code></pre>"},{"location":"SETUP_GUIDE/#provider-examples","title":"Provider Examples","text":""},{"location":"SETUP_GUIDE/#openai","title":"OpenAI","text":"<ol> <li>Get API key: platform.openai.com</li> <li>Set key: <code>export OPENAI_API_KEY=\"sk-...\"</code></li> <li>Use it: <pre><code>from ace import LiteLLMClient\nclient = LiteLLMClient(model=\"gpt-4o-mini\")\n</code></pre></li> </ol>"},{"location":"SETUP_GUIDE/#anthropic-claude","title":"Anthropic Claude","text":"<ol> <li>Get API key: console.anthropic.com</li> <li>Set key: <code>export ANTHROPIC_API_KEY=\"sk-ant-...\"</code></li> <li>Use it: <pre><code>client = LiteLLMClient(model=\"claude-3-5-sonnet-20241022\")\n</code></pre></li> </ol>"},{"location":"SETUP_GUIDE/#google-gemini","title":"Google Gemini","text":"<ol> <li>Get API key: makersuite.google.com</li> <li>Set key: <code>export GOOGLE_API_KEY=\"AIza...\"</code></li> <li>Use it: <pre><code>client = LiteLLMClient(model=\"gemini-pro\")\n</code></pre></li> </ol>"},{"location":"SETUP_GUIDE/#local-models-ollama","title":"Local Models (Ollama)","text":"<ol> <li>Install Ollama: ollama.ai</li> <li>Pull model: <code>ollama pull llama2</code></li> <li>Use it: <pre><code>client = LiteLLMClient(model=\"ollama/llama2\")\n</code></pre></li> </ol> <p>Supported Providers: 100+ via LiteLLM (AWS Bedrock, Azure, Cohere, Hugging Face, etc.)</p>"},{"location":"SETUP_GUIDE/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"SETUP_GUIDE/#custom-llm-parameters","title":"Custom LLM Parameters","text":"<pre><code>from ace import LiteLLMClient\n\nclient = LiteLLMClient(\n    model=\"gpt-4o-mini\",\n    temperature=0.7,\n    max_tokens=2048,\n    timeout=60  # seconds\n)\n</code></pre>"},{"location":"SETUP_GUIDE/#production-monitoring-opik","title":"Production Monitoring (Opik)","text":"<pre><code>pip install ace-framework[observability]\n</code></pre> <p>Opik automatically tracks: - Token usage per LLM call - Cost per operation - Agent/Reflector/SkillManager performance - Skillbook evolution over time</p> <p>View dashboard: comet.com/opik</p>"},{"location":"SETUP_GUIDE/#skillbook-storage","title":"Skillbook Storage","text":"<pre><code>from ace import Skillbook\n\n# Save skillbook\nskillbook.save_to_file(\"my_skillbook.json\")\n\n# Load skillbook\nskillbook = Skillbook.load_from_file(\"my_skillbook.json\")\n\n# For production: Use database storage\n# PostgreSQL, SQLite, or vector stores supported\n</code></pre>"},{"location":"SETUP_GUIDE/#checkpoint-saving","title":"Checkpoint Saving","text":"<pre><code>from ace import OfflineACE\n\nadapter = OfflineACE(\n    skillbook=skillbook,\n    agent=agent,\n    reflector=reflector,\n    skill_manager=skill_manager\n)\n\n# Save skillbook every 10 samples during training\nresults = adapter.run(\n    samples,\n    environment,\n    checkpoint_interval=10,\n    checkpoint_dir=\"./checkpoints\"\n)\n</code></pre>"},{"location":"SETUP_GUIDE/#troubleshooting","title":"Troubleshooting","text":""},{"location":"SETUP_GUIDE/#import-errors","title":"Import Errors","text":"<pre><code># Upgrade to latest version\npip install --upgrade ace-framework\n\n# Check installation\npip show ace-framework\n</code></pre>"},{"location":"SETUP_GUIDE/#api-key-not-working","title":"API Key Not Working","text":"<pre><code># Verify key is set\necho $OPENAI_API_KEY\n\n# Test different model\nfrom ace import LiteLLMClient\nclient = LiteLLMClient(model=\"gpt-3.5-turbo\")  # Cheaper for testing\n</code></pre>"},{"location":"SETUP_GUIDE/#rate-limits","title":"Rate Limits","text":"<pre><code>from ace import LiteLLMClient\n\n# Add delays between calls\nimport time\ntime.sleep(1)  # 1 second between calls\n\n# Or use a cheaper/faster model\nclient = LiteLLMClient(model=\"gpt-3.5-turbo\")\n</code></pre>"},{"location":"SETUP_GUIDE/#json-parse-failures","title":"JSON Parse Failures","text":"<pre><code># Increase max_tokens for SkillManager/Reflector\nfrom ace import SkillManager, Reflector\n\nllm = LiteLLMClient(model=\"gpt-4o-mini\", max_tokens=2048)  # Higher limit\nskill_manager = SkillManager(llm)\nreflector = Reflector(llm)\n</code></pre>"},{"location":"SETUP_GUIDE/#need-more-help","title":"Need More Help?","text":"<ul> <li>GitHub Issues: github.com/kayba-ai/agentic-context-engine/issues</li> <li>Discord Community: discord.gg/mqCqH7sTyK</li> <li>Documentation: Complete Guide, Quick Start, Integration Guide</li> </ul> <p>Next Steps: Check out the Quick Start Guide to build your first self-learning agent!</p>"},{"location":"TESTING_GUIDE/","title":"\ud83e\uddea ACE Framework Testing Guide","text":"<p>Complete guide for testing ACE agents and validating performance.</p>"},{"location":"TESTING_GUIDE/#testing-philosophy","title":"Testing Philosophy","text":"<p>ACE testing focuses on three key areas: 1. Correctness: Does the agent produce accurate answers? 2. Learning: Does the skillbook improve over time? 3. Robustness: Does the system handle edge cases?</p>"},{"location":"TESTING_GUIDE/#running-tests","title":"Running Tests","text":""},{"location":"TESTING_GUIDE/#quick-start","title":"Quick Start","text":"<pre><code># Run all tests\nuv run pytest\n\n# Run with verbose output\nuv run pytest -v\n\n# Run specific test file\nuv run pytest tests/test_adaptation.py\n\n# Run integration tests\nuv run pytest tests/test_integration.py\n</code></pre>"},{"location":"TESTING_GUIDE/#using-unittest","title":"Using unittest","text":"<pre><code># Run all tests\npython -m unittest discover -s tests\n\n# Run specific test file\npython -m unittest tests.test_adaptation\n\n# Verbose output\npython -m unittest discover -s tests -v\n</code></pre>"},{"location":"TESTING_GUIDE/#unit-testing","title":"Unit Testing","text":""},{"location":"TESTING_GUIDE/#testing-skillbook-operations","title":"Testing Skillbook Operations","text":"<pre><code>import unittest\nfrom ace import Skillbook\n\nclass TestSkillbook(unittest.TestCase):\n    def test_add_and_retrieve_skill(self):\n        skillbook = Skillbook()\n\n        skill = skillbook.add_skill(\n            section=\"Test\",\n            content=\"Test strategy\"\n        )\n\n        retrieved = skillbook.get_skill(skill.id)\n        self.assertEqual(retrieved.content, \"Test strategy\")\n\n    def test_save_and_load(self):\n        skillbook = Skillbook()\n        skillbook.add_skill(\"Section\", \"Content\")\n\n        skillbook.save_to_file(\"test.json\")\n        loaded = Skillbook.load_from_file(\"test.json\")\n\n        self.assertEqual(len(loaded.skills()), 1)\n</code></pre>"},{"location":"TESTING_GUIDE/#testing-agent","title":"Testing Agent","text":"<pre><code>from ace import Agent, DummyLLMClient, Skillbook\n\nclass TestAgent(unittest.TestCase):\n    def setUp(self):\n        self.client = DummyLLMClient()\n        self.agent = Agent(self.client)\n        self.skillbook = Skillbook()\n\n    def test_generate_with_empty_skillbook(self):\n        output = self.agent.generate(\n            question=\"What is 2+2?\",\n            context=\"\",\n            skillbook=self.skillbook\n        )\n\n        self.assertIsNotNone(output.final_answer)\n        self.assertIsNotNone(output.reasoning)\n\n    def test_generate_uses_skillbook(self):\n        skill = self.skillbook.add_skill(\n            section=\"Math\",\n            content=\"Show step-by-step work\"\n        )\n\n        output = self.agent.generate(\n            question=\"What is 10*5?\",\n            context=\"\",\n            skillbook=self.skillbook\n        )\n\n        # Agent should cite the skill\n        self.assertIn(skill.id, output.skill_ids)\n</code></pre>"},{"location":"TESTING_GUIDE/#testing-reflector-skillmanager","title":"Testing Reflector &amp; SkillManager","text":"<pre><code>from ace import Reflector, SkillManager, AgentOutput\n\nclass TestReflectorSkillManager(unittest.TestCase):\n    def setUp(self):\n        self.client = DummyLLMClient()\n        self.reflector = Reflector(self.client)\n        self.skill_manager = SkillManager(self.client)\n        self.skillbook = Skillbook()\n\n    def test_reflection(self):\n        agent_output = AgentOutput(\n            reasoning=\"Solved math problem\",\n            final_answer=\"4\",\n            skill_ids=[],\n            raw={}\n        )\n\n        reflection = self.reflector.reflect(\n            question=\"What is 2+2?\",\n            agent_output=agent_output,\n            skillbook=self.skillbook,\n            feedback=\"Correct answer\"\n        )\n\n        self.assertIsNotNone(reflection.reasoning)\n\n    def test_update_skills(self):\n        reflection = self.reflector.reflect(...)\n\n        skill_manager_output = self.skill_manager.update_skills(\n            reflection=reflection,\n            skillbook=self.skillbook,\n            question_context=\"Math problem\",\n            progress=\"Task 1/10\"\n        )\n\n        self.assertIsNotNone(skill_manager_output.update)\n</code></pre>"},{"location":"TESTING_GUIDE/#integration-testing","title":"Integration Testing","text":""},{"location":"TESTING_GUIDE/#end-to-end-learning-cycle","title":"End-to-End Learning Cycle","text":"<pre><code>from ace import OfflineACE, Sample, TaskEnvironment, EnvironmentResult\n\nclass SimpleEnvironment(TaskEnvironment):\n    def evaluate(self, sample, output):\n        correct = sample.ground_truth in output.final_answer\n        return EnvironmentResult(\n            feedback=\"Correct\" if correct else \"Wrong\",\n            ground_truth=sample.ground_truth\n        )\n\nclass TestLearningCycle(unittest.TestCase):\n    def test_offline_adaptation(self):\n        # Setup\n        client = DummyLLMClient()\n        agent = Agent(client)\n        reflector = Reflector(client)\n        skill_manager = SkillManager(client)\n        adapter = OfflineACE(\n            agent=agent,\n            reflector=reflector,\n            skill_manager=skill_manager\n        )\n\n        # Training samples\n        samples = [\n            Sample(\"What is 2+2?\", \"\", \"4\"),\n            Sample(\"What is 3+3?\", \"\", \"6\")\n        ]\n\n        # Run adaptation\n        results = adapter.run(samples, SimpleEnvironment(), epochs=2)\n\n        # Verify learning occurred\n        self.assertGreater(len(adapter.skillbook.skills()), 0)\n</code></pre>"},{"location":"TESTING_GUIDE/#testing-checkpoints","title":"Testing Checkpoints","text":"<pre><code>def test_checkpoint_saving(self):\n    import tempfile\n    import os\n\n    with tempfile.TemporaryDirectory() as tmpdir:\n        adapter = OfflineACE(\n            agent=agent,\n            reflector=reflector,\n            skill_manager=skill_manager\n        )\n\n        results = adapter.run(\n            samples,\n            environment,\n            checkpoint_interval=2,\n            checkpoint_dir=tmpdir\n        )\n\n        # Verify checkpoints exist\n        checkpoints = os.listdir(tmpdir)\n        self.assertGreater(len(checkpoints), 0)\n        self.assertIn(\"ace_latest.json\", checkpoints)\n</code></pre>"},{"location":"TESTING_GUIDE/#testing-without-api-calls","title":"Testing Without API Calls","text":"<p>Use <code>DummyLLMClient</code> to avoid real API calls during tests:</p> <pre><code>from ace import DummyLLMClient\n\n# Returns predefined responses\nclient = DummyLLMClient()\n\n# Use in tests\nagent = Agent(client)\noutput = agent.generate(question=\"test question\", context=\"\", skillbook=skillbook)\n</code></pre> <p>Benefits: - No API costs - Deterministic test results - Fast execution - No rate limits</p>"},{"location":"TESTING_GUIDE/#performance-testing","title":"Performance Testing","text":""},{"location":"TESTING_GUIDE/#benchmark-learning-speed","title":"Benchmark Learning Speed","text":"<pre><code>import time\n\ndef test_learning_performance(self):\n    start = time.time()\n\n    results = adapter.run(samples, environment, epochs=3)\n\n    duration = time.time() - start\n    avg_per_sample = duration / len(samples)\n\n    print(f\"Processed {len(samples)} samples in {duration:.2f}s\")\n    print(f\"Average: {avg_per_sample:.2f}s per sample\")\n\n    # Assert reasonable performance\n    self.assertLess(avg_per_sample, 5.0)  # Less than 5s per sample\n</code></pre>"},{"location":"TESTING_GUIDE/#measure-skillbook-growth","title":"Measure Skillbook Growth","text":"<pre><code>def test_skillbook_growth(self):\n    initial_skills = len(adapter.skillbook.skills())\n\n    results = adapter.run(samples, environment)\n\n    final_skills = len(adapter.skillbook.skills())\n    growth = final_skills - initial_skills\n\n    print(f\"Skillbook grew by {growth} skills\")\n\n    # Verify learning occurred\n    self.assertGreater(growth, 0)\n</code></pre>"},{"location":"TESTING_GUIDE/#common-test-patterns","title":"Common Test Patterns","text":""},{"location":"TESTING_GUIDE/#fixture-setup","title":"Fixture Setup","text":"<pre><code>class TestACEComponents(unittest.TestCase):\n    @classmethod\n    def setUpClass(cls):\n        \"\"\"Run once for all tests\"\"\"\n        cls.client = DummyLLMClient()\n\n    def setUp(self):\n        \"\"\"Run before each test\"\"\"\n        self.skillbook = Skillbook()\n        self.agent = Agent(self.client)\n        self.reflector = Reflector(self.client)\n        self.skill_manager = SkillManager(self.client)\n\n    def tearDown(self):\n        \"\"\"Run after each test\"\"\"\n        # Clean up temporary files\n        import os\n        if os.path.exists(\"test.json\"):\n            os.remove(\"test.json\")\n</code></pre>"},{"location":"TESTING_GUIDE/#mocking-external-services","title":"Mocking External Services","text":"<pre><code>from unittest.mock import Mock, patch\n\ndef test_with_mock_llm(self):\n    mock_client = Mock()\n    mock_client.complete.return_value = Mock(text='{\"answer\": \"42\"}')\n\n    agent = Agent(mock_client)\n    output = agent.generate(question=\"test\", context=\"\", skillbook=skillbook)\n\n    mock_client.complete.assert_called_once()\n</code></pre>"},{"location":"TESTING_GUIDE/#continuous-integration","title":"Continuous Integration","text":"<p>ACE includes pytest configuration and GitHub Actions workflow.</p>"},{"location":"TESTING_GUIDE/#local-ci-simulation","title":"Local CI Simulation","text":"<pre><code># Run full test suite\nuv run pytest\n\n# With coverage\nuv run pytest --cov=ace --cov-report=html\n\n# Type checking\nuv run mypy ace/\n\n# Code formatting check\nuv run black --check ace/ tests/\n</code></pre>"},{"location":"TESTING_GUIDE/#github-actions","title":"GitHub Actions","text":"<p>Tests run automatically on: - Push to main - Pull requests - Release tags</p> <p>See <code>.github/workflows/test.yml</code> for configuration.</p>"},{"location":"TESTING_GUIDE/#troubleshooting-tests","title":"Troubleshooting Tests","text":""},{"location":"TESTING_GUIDE/#tests-timeout","title":"Tests Timeout","text":"<pre><code># Reduce epochs/samples\nresults = adapter.run(samples[:5], environment, epochs=1)\n\n# Use DummyLLMClient\nclient = DummyLLMClient()  # Instead of real LLM\n</code></pre>"},{"location":"TESTING_GUIDE/#flaky-tests","title":"Flaky Tests","text":"<pre><code># Use deterministic data\nsamples = [Sample(\"2+2\", \"\", \"4\")]  # Not random\n\n# Set random seeds\nimport random\nrandom.seed(42)\n</code></pre>"},{"location":"TESTING_GUIDE/#import-errors","title":"Import Errors","text":"<pre><code># Install test dependencies\nuv sync  # Installs dev dependencies\n\n# Or manually\npip install pytest pytest-cov\n</code></pre>"},{"location":"TESTING_GUIDE/#best-practices","title":"Best Practices","text":"<ol> <li>Use DummyLLMClient for unit tests (fast, no API costs)</li> <li>Test one thing per test (easier to debug failures)</li> <li>Clean up temp files in tearDown()</li> <li>Mock external services (databases, APIs)</li> <li>Set timeouts for long-running tests</li> <li>Run tests frequently during development</li> </ol>"},{"location":"TESTING_GUIDE/#resources","title":"Resources","text":"<ul> <li>Test Suite: <code>tests/</code> directory</li> <li>Integration Tests: <code>tests/test_integration.py</code> (10 comprehensive tests)</li> <li>CI Configuration: <code>.github/workflows/test.yml</code></li> <li>Coverage Reports: Run <code>uv run pytest --cov=ace</code></li> </ul> <p>Need help with testing? Join our Discord or open a GitHub issue.</p>"},{"location":"api/","title":"API Reference","text":"<p>Quick reference for the most-used classes and functions in <code>ace_next</code>.</p>"},{"location":"api/#runners","title":"Runners","text":""},{"location":"api/#acelitellm","title":"ACELiteLLM","text":"<p>Simple self-improving conversational agent.</p> <pre><code>from ace_next import ACELiteLLM\n\nagent = ACELiteLLM.from_model(\"gpt-4o-mini\")\n</code></pre> Method Description <code>ask(question, context=\"\")</code> Generate an answer using the current skillbook <code>learn(samples, environment, epochs=1, *, wait=True)</code> Run the full ACE learning pipeline <code>learn_from_feedback(feedback, ground_truth=None)</code> Learn from the last <code>ask()</code> interaction <code>learn_from_traces(traces, epochs=1, *, wait=True)</code> Learn from pre-recorded execution traces <code>save(path)</code> Save skillbook to JSON <code>load(path)</code> Load skillbook from JSON <code>enable_learning()</code> / <code>disable_learning()</code> Toggle learning on/off <code>wait_for_background(timeout=None)</code> Wait for async learning to finish <code>learning_stats</code> Dict with background learning progress <code>get_strategies()</code> Formatted string of current strategies <p>See LiteLLM Integration for full details.</p>"},{"location":"api/#ace","title":"ACE","text":"<p>Full adaptive pipeline (Agent + Reflector + SkillManager + Environment).</p> <pre><code>from ace_next import ACE, Agent, Reflector, SkillManager, Skillbook, LiteLLMClient, SimpleEnvironment\n\nclient = LiteLLMClient(model=\"gpt-4o-mini\")\nrunner = ACE.from_roles(\n    agent=Agent(client),\n    reflector=Reflector(client),\n    skill_manager=SkillManager(client),\n    environment=SimpleEnvironment(),\n    skillbook=Skillbook(),\n)\n\nresults = runner.run(samples, epochs=3)\n</code></pre> Method Description <code>run(samples, epochs=1, wait=True)</code> Run adaptation loop, return <code>list[SampleResult]</code> <code>save(path)</code> Save skillbook <code>wait_for_background(timeout=None)</code> Wait for async learning <code>learning_stats</code> Background learning progress <p>See Full Pipeline Guide.</p>"},{"location":"api/#browseruse","title":"BrowserUse","text":"<p>Browser automation with learning.</p> <pre><code>from ace_next import BrowserUse\n\nrunner = BrowserUse.from_model(browser_llm=my_llm, ace_model=\"gpt-4o-mini\")\nresults = runner.run(\"Find the top post on Hacker News\")\n</code></pre> <p>See Browser-Use Integration.</p>"},{"location":"api/#langchain","title":"LangChain","text":"<p>Wrap LangChain Runnables with learning.</p> <pre><code>from ace_next import LangChain\n\nrunner = LangChain.from_model(my_chain, ace_model=\"gpt-4o-mini\")\nresults = runner.run([{\"input\": \"Summarize this document\"}])\n</code></pre> <p>See LangChain Integration.</p>"},{"location":"api/#claudecode","title":"ClaudeCode","text":"<p>Claude Code CLI with learning.</p> <pre><code>from ace_next import ClaudeCode\n\nrunner = ClaudeCode.from_model(working_dir=\"./project\", ace_model=\"gpt-4o-mini\")\nresults = runner.run(\"Add unit tests for utils.py\")\n</code></pre> <p>See Claude Code Integration.</p>"},{"location":"api/#roles","title":"Roles","text":""},{"location":"api/#agent","title":"Agent","text":"<p>Produces answers using the current skillbook.</p> <pre><code>from ace_next import Agent\n\nagent = Agent(llm)\noutput = agent.generate(\n    question=\"What is 2+2?\",\n    context=\"\",\n    skillbook=skillbook,\n    reflection=None,  # optional\n)\n</code></pre> <p>AgentOutput fields:</p> Field Type Description <code>final_answer</code> <code>str</code> The generated answer <code>reasoning</code> <code>str</code> Step-by-step reasoning <code>skill_ids</code> <code>list[str]</code> Skillbook strategies cited <code>raw</code> <code>dict</code> Raw LLM response"},{"location":"api/#reflector","title":"Reflector","text":"<p>Analyzes what worked and what failed.</p> <pre><code>from ace_next import Reflector\n\nreflector = Reflector(llm)\nreflection = reflector.reflect(\n    question=\"What is 2+2?\",\n    agent_output=output,\n    skillbook=skillbook,\n    ground_truth=\"4\",\n    feedback=\"Correct!\",\n)\n</code></pre> <p>ReflectorOutput fields:</p> Field Type Description <code>reasoning</code> <code>str</code> Analysis of the outcome <code>error_identification</code> <code>str</code> What went wrong <code>root_cause_analysis</code> <code>str</code> Why it went wrong <code>correct_approach</code> <code>str</code> What should have been done <code>key_insight</code> <code>str</code> Main lesson learned <code>extracted_learnings</code> <code>list[ExtractedLearning]</code> Learnings with evidence and justification <code>skill_tags</code> <code>list[SkillTag]</code> <code>(skill_id, tag)</code> pairs <code>raw</code> <code>dict</code> Raw LLM response"},{"location":"api/#skillmanager","title":"SkillManager","text":"<p>Transforms reflections into skillbook updates.</p> <pre><code>from ace_next import SkillManager\n\nskill_manager = SkillManager(llm)\nsm_output = skill_manager.update_skills(\n    reflection=reflection,\n    skillbook=skillbook,\n    question_context=\"Math problems\",\n    progress=\"3/5 correct\",\n)\n# Apply the updates\nskillbook.apply_update(sm_output.update)\n</code></pre> <p>Returns a <code>SkillManagerOutput</code> with an <code>.update</code> field (<code>UpdateBatch</code>) and <code>.raw</code> field.</p> <p>See Roles for full details.</p>"},{"location":"api/#skillbook","title":"Skillbook","text":"<pre><code>from ace_next import Skillbook\n\nskillbook = Skillbook()\n</code></pre> Method / Property Description <code>add_skill(section, content, metadata=None)</code> Add a strategy <code>apply_update(update_batch)</code> Apply update operations <code>as_prompt()</code> TOON format for LLM consumption <code>save_to_file(path)</code> Save to JSON <code>Skillbook.load_from_file(path)</code> Load from JSON <code>stats()</code> Section count, skill count, tag totals <code>skills()</code> List of all skills <p>See The Skillbook.</p>"},{"location":"api/#data-types","title":"Data Types","text":""},{"location":"api/#sample","title":"Sample","text":"<pre><code>from ace_next import Sample\n\nsample = Sample(\n    question=\"What is 2+2?\",\n    context=\"Show your work\",\n    ground_truth=\"4\",\n)\n</code></pre>"},{"location":"api/#environmentresult","title":"EnvironmentResult","text":"<pre><code>from ace_next import EnvironmentResult\n\nresult = EnvironmentResult(\n    feedback=\"Correct!\",\n    ground_truth=\"4\",\n    metrics={\"accuracy\": 1.0},\n)\n</code></pre>"},{"location":"api/#updateoperation","title":"UpdateOperation","text":"<pre><code>from ace_next import UpdateOperation\n\nop = UpdateOperation(\n    type=\"ADD\",\n    section=\"Math\",\n    content=\"Break problems into smaller steps\",\n    skill_id=\"math-00001\",\n)\n</code></pre> <p>Operations: <code>ADD</code>, <code>UPDATE</code>, <code>TAG</code>, <code>REMOVE</code>. See Update Operations.</p>"},{"location":"api/#deduplicationconfig","title":"DeduplicationConfig","text":"<p>Requires: <code>pip install ace-framework[deduplication]</code></p> <pre><code>from ace_next import DeduplicationConfig\n\nconfig = DeduplicationConfig(\n    enabled=True,\n    embedding_model=\"text-embedding-3-small\",\n    similarity_threshold=0.85,\n)\n</code></pre>"},{"location":"api/#environments","title":"Environments","text":"<p>Extend <code>TaskEnvironment</code> to provide evaluation feedback:</p> <pre><code>from ace_next import TaskEnvironment, EnvironmentResult\n\nclass MyEnvironment(TaskEnvironment):\n    def evaluate(self, sample, agent_output):\n        correct = sample.ground_truth.lower() in agent_output.final_answer.lower()\n        return EnvironmentResult(\n            feedback=\"Correct!\" if correct else \"Incorrect\",\n            ground_truth=sample.ground_truth,\n        )\n</code></pre> <p>A built-in <code>SimpleEnvironment</code> uses substring matching and is included for quick testing.</p>"},{"location":"api/#llm-clients","title":"LLM Clients","text":""},{"location":"api/#litellmclient","title":"LiteLLMClient","text":"<pre><code>from ace_next import LiteLLMClient\n\nclient = LiteLLMClient(model=\"gpt-4o-mini\", temperature=0.0, max_tokens=2048)\nresponse = client.complete(\"Hello\")\n</code></pre> <p>Supports all LiteLLM providers (OpenAI, Anthropic, Google, Ollama, etc.).</p>"},{"location":"api/#instructorclient","title":"InstructorClient","text":"<p>Wraps any LLM client with Pydantic validation for more reliable structured outputs.</p> <p>Requires: <code>pip install ace-framework[instructor]</code></p> <pre><code>from ace_next import InstructorClient, LiteLLMClient\n\nclient = InstructorClient(LiteLLMClient(model=\"ollama/gemma3:1b\"))\n</code></pre>"},{"location":"api/#patterns","title":"Patterns","text":""},{"location":"api/#subrunner","title":"SubRunner","text":"<p>Abstract base class for steps that run an internal <code>Pipeline</code> in a loop. Satisfies <code>StepProtocol</code> \u2014 can be placed directly in any pipeline.</p> <pre><code>from ace_next.core import SubRunner\n</code></pre> <p>Subclasses implement five template methods plus <code>__call__</code>:</p> Method Signature Description <code>_build_inner_pipeline</code> <code>(**kwargs) -&gt; Pipeline</code> Return the step sequence for one iteration <code>_build_initial_context</code> <code>(**kwargs) -&gt; StepContext</code> Return the context for the first iteration <code>_is_done</code> <code>(ctx) -&gt; bool</code> Return <code>True</code> when the loop should stop <code>_extract_result</code> <code>(ctx) -&gt; Any</code> Pull the final result from the terminal context <code>_accumulate</code> <code>(ctx) -&gt; StepContext</code> Build the next iteration's context from the current one <code>_on_timeout</code> <code>(last_ctx, iteration, **kwargs) -&gt; Any</code> Called when <code>max_iterations</code> is reached. Default raises <code>RuntimeError</code>. <code>run_loop</code> <code>(**kwargs) -&gt; Any</code> Execute the loop (called by <code>__call__</code>, also usable standalone) <code>__call__</code> <code>(ctx) -&gt; StepContext</code> <code>StepProtocol</code> entry point \u2014 map outer context to <code>run_loop</code> result Parameter Type Default Description <code>max_iterations</code> <code>int</code> <code>20</code> Maximum loop iterations before <code>_on_timeout</code> fires <p>Example:</p> <pre><code>from ace_next.core import SubRunner\nfrom pipeline import Pipeline, StepContext\n\nclass RefineRunner(SubRunner):\n    requires = frozenset({\"draft\"})\n    provides = frozenset({\"refined\"})\n\n    def __init__(self, scorer, improver, threshold=0.9):\n        super().__init__(max_iterations=10)\n        self.scorer = scorer\n        self.improver = improver\n        self.threshold = threshold\n\n    def _build_inner_pipeline(self, **kw):\n        return Pipeline([ScoreStep(self.scorer), ImproveStep(self.improver)])\n\n    def _build_initial_context(self, **kw):\n        return RefineContext(text=kw[\"draft\"])\n\n    def _is_done(self, ctx):\n        return ctx.score &gt;= self.threshold\n\n    def _extract_result(self, ctx):\n        return ctx.text\n\n    def _accumulate(self, ctx):\n        return ctx.replace(iteration=ctx.iteration + 1)\n\n    def _on_timeout(self, last_ctx, iteration, **kwargs):\n        return last_ctx.text  # best effort\n\n    def __call__(self, ctx):\n        result = self.run_loop(draft=ctx.metadata[\"draft\"])\n        return ctx.replace(metadata=MappingProxyType({**ctx.metadata, \"refined\": result}))\n</code></pre> <p>Canonical implementation: <code>RRStep</code> in <code>ace_next/rr/</code> \u2014 the Recursive Reflector's REPL loop.</p> <p>See Building Custom Steps for the full guide.</p>"},{"location":"api/#observability","title":"Observability","text":""},{"location":"api/#opikstep","title":"OpikStep","text":"<p>Append to any pipeline for automatic tracing and cost tracking:</p> <pre><code>from ace_next import OpikStep\n\nOpikStep(project_name=\"my-experiment\", tags=[\"training\"])\n</code></pre>"},{"location":"api/#register_opik_litellm_callback","title":"register_opik_litellm_callback","text":"<p>Standalone LLM cost tracking without pipeline traces:</p> <pre><code>from ace_next import register_opik_litellm_callback\n\nregister_opik_litellm_callback(project_name=\"my-experiment\")\n</code></pre> <p>See Opik Observability.</p>"},{"location":"api/#prompts","title":"Prompts","text":"<p>The default prompts are v2.1 (built into <code>ace_next</code>). Pass a custom template via <code>prompt_template</code>:</p> <pre><code>agent = Agent(llm, prompt_template=\"Custom prompt with {skillbook}, {question}, {context}\")\nreflector = Reflector(llm, prompt_template=\"Custom reflector prompt ...\")\nskill_manager = SkillManager(llm, prompt_template=\"Custom skill manager prompt ...\")\n</code></pre> <p>See Prompt Engineering.</p>"},{"location":"concepts/insight-levels/","title":"Insight Levels","text":"<p>The ACE framework operates at three insight levels depending on what scope the Reflector analyzes.</p>"},{"location":"concepts/insight-levels/#overview","title":"Overview","text":"Level Reflector Scope Feedback Source Implementation Micro Single interaction Environment (ground truth) <code>ACE</code> runner with <code>TaskEnvironment</code> Meso Full agent run Execution trace (no ground truth) Integration runners (<code>BrowserUse</code>, <code>LangChain</code>, <code>ClaudeCode</code>) Macro Cross-run analysis Pattern comparison across runs Future enhancement"},{"location":"concepts/insight-levels/#micro-level","title":"Micro-Level","text":"<p>The Reflector receives the agent's output and environment feedback (ground truth, correctness). This is the most precise learning signal.</p> <pre><code>graph LR\n    Q[Question] --&gt; R[Reflector]\n    A[Agent Answer] --&gt; R\n    GT[Ground Truth] --&gt; R\n    F[Feedback] --&gt; R</code></pre> <p>Use when you have labeled data or a reliable evaluation function.</p> <pre><code>from ace_next import ACE, Sample, SimpleEnvironment\n\nrunner = ACE.from_roles(\n    agent=agent,\n    reflector=reflector,\n    skill_manager=skill_manager,\n    environment=SimpleEnvironment(),\n)\n\nsamples = [\n    Sample(question=\"What is 2+2?\", context=\"\", ground_truth=\"4\"),\n]\nrunner.run(samples, epochs=3)\n</code></pre>"},{"location":"concepts/insight-levels/#meso-level","title":"Meso-Level","text":"<p>The Reflector receives the full execution trace \u2014 the agent's reasoning steps, tool calls, actions, and outcomes \u2014 but no external ground truth. It learns from execution patterns rather than correctness evaluation.</p> <pre><code>graph LR\n    T[Task] --&gt; R[Reflector]\n    ET[\"Execution Trace (thoughts, actions, results)\"] --&gt; R</code></pre> <p>Use when wrapping external agents where you don't have labeled answers.</p> <pre><code>from ace_next import BrowserUse\n\n# The browser-use agent produces a rich trace of actions\nrunner = BrowserUse.from_model(\n    browser_llm=ChatOpenAI(model=\"gpt-4o\"),\n    ace_model=\"gpt-4o-mini\",\n)\nrunner.run(\"Find the top post on Hacker News\")\n</code></pre> <p>The extracted trace includes:</p> <ul> <li>Agent reasoning at each step</li> <li>Browser actions (click, type, navigate)</li> <li>Page observations</li> <li>Success/failure of each action</li> </ul>"},{"location":"concepts/insight-levels/#macro-level","title":"Macro-Level","text":"<p>Cross-run pattern analysis \u2014 comparing strategies across multiple execution histories. Not yet implemented.</p>"},{"location":"concepts/insight-levels/#what-to-read-next","title":"What to Read Next","text":"<ul> <li>Three Roles \u2014 the roles involved at each level</li> <li>Integration Pattern \u2014 meso-level integrations in practice</li> <li>Full Pipeline Guide \u2014 micro-level pipelines in practice</li> </ul>"},{"location":"concepts/overview/","title":"How ACE Works","text":"<p>Agentic Context Engineering (ACE) enables AI agents to learn from their own execution feedback. Instead of updating model weights (expensive, slow, opaque), ACE evolves a skillbook of strategies based on what actually works.</p> <p>Research</p> <p>ACE was introduced in Agentic Context Engineering: Evolving Contexts for Self-Improving Language Models by researchers at Stanford University and SambaNova Systems.</p>"},{"location":"concepts/overview/#the-learning-loop","title":"The Learning Loop","text":"<p>Three collaborative roles share the same base LLM:</p> <pre><code>graph LR\n    S[Sample] --&gt; A[Agent]\n    A --&gt; E[Environment]\n    E --&gt;|feedback| R[Reflector]\n    R --&gt;|analyzes| SM[SkillManager]\n    SM --&gt;|updates| SK[Skillbook]\n    SK -.-&gt;|context| A</code></pre> <ol> <li>The Agent executes a task using strategies from the skillbook</li> <li>The Environment evaluates the result (correct/incorrect, feedback)</li> <li>The Reflector analyzes what worked and what failed</li> <li>The SkillManager updates the skillbook with new strategies</li> </ol> <p>The Skillbook accumulates strategies across runs, making every subsequent agent call smarter.</p>"},{"location":"concepts/overview/#three-roles","title":"Three Roles","text":"Role Responsibility Key Class Agent Executes tasks using skillbook strategies <code>Agent</code> Reflector Analyzes execution results (what worked, what failed) <code>Reflector</code> SkillManager Transforms reflections into skillbook updates <code>SkillManager</code> <p>All three roles use the same LLM \u2014 the intelligence comes from the specialized prompts each role receives.</p> <p>See Three Roles for details on each role's inputs and outputs.</p>"},{"location":"concepts/overview/#two-architecture-patterns","title":"Two Architecture Patterns","text":""},{"location":"concepts/overview/#full-ace-pipeline","title":"Full ACE Pipeline","text":"<p>Use when building a new agent from scratch.</p> <pre><code>graph LR\n    S[Sample] --&gt; A[Agent]\n    A --&gt; E[Environment]\n    E --&gt; R[Reflector]\n    R --&gt; SM[SkillManager]\n    SM --&gt; SK[Skillbook]</code></pre> <p>All three roles participate. The Agent produces answers, the Environment evaluates them, and the learning pipeline updates the skillbook.</p> <pre><code>from ace_next import ACE, Agent, Reflector, SkillManager, LiteLLMClient, SimpleEnvironment\n\nllm = LiteLLMClient(model=\"gpt-4o-mini\")\nrunner = ACE.from_roles(\n    agent=Agent(llm),\n    reflector=Reflector(llm),\n    skill_manager=SkillManager(llm),\n    environment=SimpleEnvironment(),\n)\nresults = runner.run(samples, epochs=3)\n</code></pre>"},{"location":"concepts/overview/#integration-pattern","title":"Integration Pattern","text":"<p>Use when wrapping an existing agent (browser-use, LangChain, Claude Code).</p> <pre><code>graph LR\n    EA[External Agent] --&gt;|executes| R[Reflector]\n    R --&gt;|analyzes trace| SM[SkillManager]\n    SM --&gt;|updates| SK[Skillbook]</code></pre> <p>No ACE Agent \u2014 the external framework handles execution. ACE only learns from the results.</p> <p>Three steps: INJECT skillbook context, EXECUTE with external agent, LEARN from results.</p> <pre><code>from ace_next import BrowserUse\n\nrunner = BrowserUse.from_model(\n    browser_llm=ChatOpenAI(model=\"gpt-4o\"),\n    ace_model=\"gpt-4o-mini\",\n)\nresults = runner.run(\"Find the top post on Hacker News\")\n</code></pre> <p>See Integration Pattern for building custom integrations.</p>"},{"location":"concepts/overview/#how-it-compares","title":"How It Compares","text":"Approach Updates Speed Interpretability Fine-tuning Model weights Slow (hours) Low (opaque) RAG External documents Medium Medium ACE Skillbook context Fast (real-time) High (readable strategies) <p>ACE strategies are human-readable, auditable, and transferable between models.</p>"},{"location":"concepts/overview/#performance","title":"Performance","text":"Benchmark Improvement Notes AppWorld Agent +17.1 pp Complex multi-step tasks with tool use FiNER (Finance) +8.6 pp Financial reasoning tasks Adaptation Latency -86.9% vs. existing context-adaptation methods"},{"location":"concepts/overview/#what-to-read-next","title":"What to Read Next","text":"<ul> <li>The Skillbook \u2014 how strategies are stored and evolve</li> <li>Three Roles \u2014 Agent, Reflector, and SkillManager in detail</li> <li>Quick Start \u2014 run your first agent</li> </ul>"},{"location":"concepts/roles/","title":"Three Roles","text":"<p>ACE uses three collaborative roles that share the same base LLM. Each role has a specialized prompt that focuses it on a specific part of the learning loop.</p> <pre><code>graph LR\n    A[Agent] --&gt;|execute| E[Environment]\n    E --&gt;|evaluate| R[Reflector]\n    R --&gt;|analyze| SM[SkillManager]\n    SM --&gt;|update| SK[Skillbook]</code></pre>"},{"location":"concepts/roles/#agent","title":"Agent","text":"<p>Produces answers using the current skillbook.</p> <p>The Agent receives a question, context, and the skillbook's strategies, then generates a reasoned answer citing which skills it used.</p> <pre><code>from ace_next import Agent, LiteLLMClient\n\nllm = LiteLLMClient(model=\"gpt-4o-mini\")\nagent = Agent(llm)\n\noutput = agent.generate(\n    question=\"What is 2+2?\",\n    context=\"Show your work\",\n    skillbook=skillbook,\n    reflection=None,  # Optional: reflection from a previous attempt\n)\n</code></pre>"},{"location":"concepts/roles/#agentoutput","title":"AgentOutput","text":"Field Type Description <code>final_answer</code> <code>str</code> The generated answer <code>reasoning</code> <code>str</code> Step-by-step reasoning <code>skill_ids</code> <code>List[str]</code> Skillbook strategies cited <code>raw</code> <code>Dict</code> Raw LLM response"},{"location":"concepts/roles/#reflector","title":"Reflector","text":"<p>Analyzes execution outcomes \u2014 what worked, what failed, and why.</p> <p>The Reflector receives the agent's output, the environment's feedback, and the skillbook. It produces an analysis of the outcome and tags each cited skill as helpful, harmful, or neutral.</p> <pre><code>from ace_next import Reflector\n\nreflector = Reflector(llm)\n\nreflection = reflector.reflect(\n    question=\"What is 2+2?\",\n    agent_output=output,\n    skillbook=skillbook,\n    ground_truth=\"4\",\n    feedback=\"Correct!\",\n)\n</code></pre>"},{"location":"concepts/roles/#reflectoroutput","title":"ReflectorOutput","text":"Field Type Description <code>reasoning</code> <code>str</code> Analysis of the outcome <code>error_identification</code> <code>str</code> What went wrong (if anything) <code>root_cause_analysis</code> <code>str</code> Why it went wrong <code>correct_approach</code> <code>str</code> What should have been done <code>key_insight</code> <code>str</code> Main lesson learned <code>skill_tags</code> <code>List[SkillTag]</code> <code>(skill_id, tag)</code> pairs"},{"location":"concepts/roles/#reflector-modes","title":"Reflector Modes","text":"Mode Description <code>SIMPLE</code> Single-pass analysis (default) <code>RECURSIVE</code> Multi-pass with code execution in a REPL loop"},{"location":"concepts/roles/#skillmanager","title":"SkillManager","text":"<p>Transforms reflections into skillbook updates.</p> <p>The SkillManager takes the Reflector's analysis and decides which operations to apply to the skillbook \u2014 adding new strategies, updating existing ones, or removing harmful ones.</p> <pre><code>from ace_next import SkillManager\n\nskill_manager = SkillManager(llm)\n\nsm_output = skill_manager.update_skills(\n    reflection=reflection,\n    skillbook=skillbook,\n    question_context=\"Math problems\",\n    progress=\"3/5 correct\",\n)\n\n# Apply the updates\nskillbook.apply_update(sm_output.update)\n</code></pre>"},{"location":"concepts/roles/#skillmanageroutput","title":"SkillManagerOutput","text":"Field Type Description <code>update</code> <code>UpdateBatch</code> Batch of update operations to apply <code>consolidation_ops</code> <code>List</code> Deduplication operations (if enabled)"},{"location":"concepts/roles/#shared-llm","title":"Shared LLM","text":"<p>All three roles share the same LLM instance. The intelligence comes from the specialized prompts, not from using different models:</p> <pre><code>from ace_next import Agent, Reflector, SkillManager, LiteLLMClient\n\nllm = LiteLLMClient(model=\"gpt-4o-mini\")\n\nagent = Agent(llm)              # Same LLM\nreflector = Reflector(llm)      # Same LLM\nskill_manager = SkillManager(llm)  # Same LLM\n</code></pre> <p>You can optionally use a cheaper model for the learning roles (Reflector + SkillManager) while keeping a stronger model for the Agent:</p> <pre><code>agent_llm = LiteLLMClient(model=\"gpt-4o\")\nlearning_llm = LiteLLMClient(model=\"gpt-4o-mini\")\n\nagent = Agent(agent_llm)\nreflector = Reflector(learning_llm)\nskill_manager = SkillManager(learning_llm)\n</code></pre>"},{"location":"concepts/roles/#what-to-read-next","title":"What to Read Next","text":"<ul> <li>Insight Levels \u2014 micro, meso, and macro analysis scopes</li> <li>Update Operations \u2014 the operations the SkillManager emits</li> <li>Full Pipeline Guide \u2014 wire the roles together</li> </ul>"},{"location":"concepts/skillbook/","title":"The Skillbook","text":"<p>The Skillbook is ACE's knowledge store \u2014 a structured collection of strategies that the agent has learned from experience. Each strategy is called a skill.</p>"},{"location":"concepts/skillbook/#what-is-a-skill","title":"What Is a Skill?","text":"<p>A skill is a single strategy entry with:</p> Field Description <code>id</code> Unique identifier (e.g., <code>math-00001</code>) <code>section</code> Category grouping (e.g., <code>\"Math Strategies\"</code>) <code>content</code> The strategy text <code>helpful</code> Times this skill contributed to a correct answer <code>harmful</code> Times this skill led to an incorrect answer <code>neutral</code> Times cited but had no clear effect <p>Example skill:</p> <pre><code>{\n  \"id\": \"math-00001\",\n  \"section\": \"Math Strategies\",\n  \"content\": \"Break complex problems into smaller steps before computing\",\n  \"helpful\": 5,\n  \"harmful\": 0,\n  \"neutral\": 1\n}\n</code></pre>"},{"location":"concepts/skillbook/#skill-lifecycle","title":"Skill Lifecycle","text":"<p>Skills go through four stages:</p> <ol> <li>Created \u2014 the SkillManager adds a new skill after a reflection</li> <li>Tagged \u2014 each time the Agent cites a skill, the Reflector tags it as helpful, harmful, or neutral</li> <li>Updated \u2014 the SkillManager may refine a skill's content based on new learnings</li> <li>Removed \u2014 skills that are consistently harmful get pruned</li> </ol> <p>These correspond to four update operations: <code>ADD</code>, <code>TAG</code>, <code>UPDATE</code>, <code>REMOVE</code>.</p>"},{"location":"concepts/skillbook/#toon-format","title":"TOON Format","text":"<p>When the skillbook is included in LLM prompts, it uses TOON (Token-Oriented Object Notation) \u2014 a compact format that saves 16-62% tokens compared to JSON:</p> <pre><code>skillbook.as_prompt()  # TOON format for LLM consumption\n</code></pre> <pre><code>skills[3]{id    section content helpful harmful neutral}:\n  math-00001    Math Strategies Break complex problems into smaller steps   5   0   1\n  math-00002    Math Strategies Verify answers by working backwards 3   1   0\n  logic-00001   Logic   Check edge cases before concluding  2   0   0\n</code></pre> <p>For human debugging, use the string representation:</p> <pre><code>str(skillbook)  # Markdown format for readability\n</code></pre>"},{"location":"concepts/skillbook/#sections","title":"Sections","text":"<p>Skills are organized into sections. Sections emerge naturally from the SkillManager's categorization during learning:</p> <pre><code>from ace_next import Skillbook\n\nskillbook = Skillbook()\n\n# Add skills to specific sections\nskillbook.add_skill(\n    section=\"Math Strategies\",\n    content=\"Break complex problems into smaller steps\",\n    metadata={\"helpful\": 5, \"harmful\": 0, \"neutral\": 1},\n)\n</code></pre>"},{"location":"concepts/skillbook/#persistence","title":"Persistence","text":"<pre><code># Save\nskillbook.save_to_file(\"strategies.json\")\n\n# Load\nskillbook = Skillbook.load_from_file(\"strategies.json\")\n</code></pre>"},{"location":"concepts/skillbook/#statistics","title":"Statistics","text":"<pre><code>stats = skillbook.stats()\n# {\"sections\": 3, \"skills\": 15, \"tags\": {\"helpful\": 45, \"harmful\": 5, \"neutral\": 10}}\n</code></pre>"},{"location":"concepts/skillbook/#deduplication","title":"Deduplication","text":"<p>As the skillbook grows, similar skills can accumulate. The <code>DeduplicationManager</code> detects and consolidates them using embedding similarity:</p> <pre><code>from ace_next import DeduplicationConfig, DeduplicationManager\n\nconfig = DeduplicationConfig(\n    enabled=True,\n    embedding_model=\"text-embedding-3-small\",\n    similarity_threshold=0.85,\n    within_section_only=True,\n)\ndedup = DeduplicationManager(config)\n</code></pre> <p>When used with a runner, deduplication runs automatically at a configurable interval:</p> <pre><code>runner = ACE.from_roles(\n    ...,\n    dedup_manager=dedup,\n    dedup_interval=10,  # Every 10 samples\n)\n</code></pre>"},{"location":"concepts/skillbook/#insight-source-tracing","title":"Insight Source Tracing","text":"<p>Each skill tracks where it came from \u2014 which sample, epoch, and step produced it. Query this with:</p> <pre><code>sources = skillbook.source_map()     # skill_id -&gt; source info\nsummary = skillbook.source_summary() # Aggregated statistics\n</code></pre>"},{"location":"concepts/skillbook/#what-to-read-next","title":"What to Read Next","text":"<ul> <li>Update Operations \u2014 how ADD, UPDATE, TAG, REMOVE work</li> <li>Three Roles \u2014 which role creates, tags, and updates skills</li> <li>Full Pipeline Guide \u2014 see the skillbook in action</li> </ul>"},{"location":"concepts/updates/","title":"Update Operations","text":"<p>The SkillManager communicates changes to the skillbook through update operations. Each operation is a structured instruction to modify the skillbook in a specific way.</p>"},{"location":"concepts/updates/#operation-types","title":"Operation Types","text":"Type Description Required Fields <code>ADD</code> Create a new skill <code>section</code>, <code>content</code> <code>UPDATE</code> Modify an existing skill's content <code>skill_id</code>, <code>content</code> <code>TAG</code> Record a skill as helpful, harmful, or neutral <code>skill_id</code>, <code>tag</code> <code>REMOVE</code> Delete a skill from the skillbook <code>skill_id</code>"},{"location":"concepts/updates/#examples","title":"Examples","text":""},{"location":"concepts/updates/#add","title":"ADD","text":"<p>Adds a new strategy learned from experience:</p> <pre><code>{\n  \"type\": \"ADD\",\n  \"section\": \"Math Strategies\",\n  \"content\": \"Break complex problems into smaller steps before computing\"\n}\n</code></pre>"},{"location":"concepts/updates/#update","title":"UPDATE","text":"<p>Refines an existing strategy:</p> <pre><code>{\n  \"type\": \"UPDATE\",\n  \"skill_id\": \"math-00001\",\n  \"content\": \"Break complex problems into smaller steps. Verify each step before proceeding.\"\n}\n</code></pre>"},{"location":"concepts/updates/#tag","title":"TAG","text":"<p>Records whether a strategy helped or hurt:</p> <pre><code>{\n  \"type\": \"TAG\",\n  \"skill_id\": \"math-00001\",\n  \"tag\": \"helpful\"\n}\n</code></pre> <p>Tags are one of: <code>helpful</code>, <code>harmful</code>, <code>neutral</code>.</p>"},{"location":"concepts/updates/#remove","title":"REMOVE","text":"<p>Prunes a strategy that is consistently harmful:</p> <pre><code>{\n  \"type\": \"REMOVE\",\n  \"skill_id\": \"math-00003\"\n}\n</code></pre>"},{"location":"concepts/updates/#update-batches","title":"Update Batches","text":"<p>The SkillManager emits operations as an <code>UpdateBatch</code> \u2014 one or more operations applied atomically:</p> <pre><code>from ace_next import UpdateOperation, UpdateBatch\n\nbatch = UpdateBatch(operations=[\n    UpdateOperation(type=\"ADD\", section=\"Debugging\", content=\"Log inputs before errors\"),\n    UpdateOperation(type=\"TAG\", skill_id=\"debug-00001\", tag=\"helpful\"),\n])\n\nskillbook.apply_update(batch)\n</code></pre>"},{"location":"concepts/updates/#how-updates-flow","title":"How Updates Flow","text":"<pre><code>Agent cites skill_ids --&gt; Reflector tags them --&gt; SkillManager emits ADD/UPDATE/REMOVE\n</code></pre> <ol> <li>The Agent cites skill IDs it used in its reasoning</li> <li>The Reflector classifies each cited skill as helpful/harmful/neutral (TAG operations)</li> <li>The SkillManager may also ADD new strategies or UPDATE/REMOVE existing ones based on the reflection</li> </ol>"},{"location":"concepts/updates/#what-to-read-next","title":"What to Read Next","text":"<ul> <li>The Skillbook \u2014 where operations are applied</li> <li>Three Roles \u2014 which role emits which operations</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#for-users","title":"For Users","text":"pipWith extras <pre><code>pip install ace-framework\n</code></pre> <pre><code>pip install ace-framework[all]            # All optional features\npip install ace-framework[instructor]     # Structured outputs (Instructor)\npip install ace-framework[langchain]      # LangChain integration\npip install ace-framework[browser-use]    # Browser automation\npip install ace-framework[claude-code]    # Claude Code CLI integration\npip install ace-framework[observability]  # Opik monitoring + cost tracking\npip install ace-framework[deduplication]  # Skill deduplication (embeddings)\npip install ace-framework[transformers]   # Local model support\n</code></pre>"},{"location":"getting-started/installation/#for-contributors","title":"For Contributors","text":"UV (Recommended)pip <pre><code>git clone https://github.com/kayba-ai/agentic-context-engine\ncd agentic-context-engine\nuv sync  # Installs everything (10-100x faster than pip)\n</code></pre> <pre><code>git clone https://github.com/kayba-ai/agentic-context-engine\ncd agentic-context-engine\npip install -e .\n</code></pre>"},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.12</li> <li>An API key for your LLM provider</li> </ul>"},{"location":"getting-started/installation/#api-key-setup","title":"API Key Setup","text":"<p>Set one of these environment variables depending on your provider:</p> <pre><code>export OPENAI_API_KEY=\"sk-...\"\nexport ANTHROPIC_API_KEY=\"sk-ant-...\"\nexport GOOGLE_API_KEY=\"AIza...\"\n</code></pre> <p>Or create a <code>.env</code> file:</p> <pre><code>echo \"OPENAI_API_KEY=sk-...\" &gt; .env\n</code></pre> <pre><code>from dotenv import load_dotenv\nload_dotenv()  # Loads from .env\n</code></pre>"},{"location":"getting-started/installation/#supported-providers","title":"Supported Providers","text":"<p>ACE uses LiteLLM for model access, supporting 100+ providers:</p> Provider Model Example Env Variable OpenAI <code>gpt-4o-mini</code> <code>OPENAI_API_KEY</code> Anthropic <code>claude-sonnet-4-5-20250929</code> <code>ANTHROPIC_API_KEY</code> Google <code>gemini-pro</code> <code>GOOGLE_API_KEY</code> Ollama (local) <code>ollama/llama2</code> \u2014 AWS Bedrock <code>bedrock/anthropic.claude-v2</code> AWS credentials Azure <code>azure/gpt-4</code> <code>AZURE_API_KEY</code>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code>from ace_next import ACELiteLLM\n\nagent = ACELiteLLM.from_model(\"gpt-4o-mini\")\nprint(agent.ask(\"Hello!\"))\n</code></pre>"},{"location":"getting-started/installation/#what-to-read-next","title":"What to Read Next","text":"<ul> <li>Quick Start \u2014 build your first self-learning agent</li> <li>How ACE Works \u2014 understand the architecture</li> </ul>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>Get a self-learning agent running in under a minute.</p>"},{"location":"getting-started/quick-start/#simplest-example","title":"Simplest Example","text":"<pre><code>from ace_next import ACELiteLLM\n\nagent = ACELiteLLM.from_model(\"gpt-4o-mini\")\n\n# Ask related questions \u2014 the agent learns patterns across them\nanswer1 = agent.ask(\"If all cats are animals, is Felix (a cat) an animal?\")\nanswer2 = agent.ask(\"If all birds fly, can penguins (birds) fly?\")\n\nprint(f\"Learned {len(agent.skillbook.skills())} strategies\")\n\n# Save and reload later\nagent.save(\"my_agent.json\")\n</code></pre>"},{"location":"getting-started/quick-start/#choose-your-integration","title":"Choose Your Integration","text":"LiteLLMLangChainBrowser-UseClaude Code <p>The simplest path. Supports 100+ LLM providers.</p> <pre><code>from ace_next import ACELiteLLM\n\nagent = ACELiteLLM.from_model(\"gpt-4o-mini\")\nanswer = agent.ask(\"Your question\")\nagent.save(\"learned.json\")\n</code></pre> <p>Wrap any LangChain Runnable (chains, agents, graphs) with learning.</p> <pre><code>from ace_next import LangChain\n\nrunner = LangChain.from_model(your_chain, ace_model=\"gpt-4o-mini\")\nresults = runner.run([{\"input\": \"Your task\"}])\nrunner.save(\"chain_expert.json\")\n</code></pre> <p>Browser automation that learns navigation patterns.</p> <pre><code>from ace_next import BrowserUse\nfrom langchain_openai import ChatOpenAI\n\nrunner = BrowserUse.from_model(\n    browser_llm=ChatOpenAI(model=\"gpt-4o\"),\n    ace_model=\"gpt-4o-mini\",\n)\nresults = runner.run(\"Find the top post on Hacker News\")\nrunner.save(\"browser_expert.json\")\n</code></pre> <p>Self-improving coding agent using the Claude Code CLI.</p> <pre><code>from ace_next import ClaudeCode\n\nrunner = ClaudeCode.from_model(working_dir=\"./my_project\")\nresults = runner.run(\"Add unit tests for utils.py\")\nrunner.save(\"coding_expert.json\")\n</code></pre>"},{"location":"getting-started/quick-start/#full-pipeline-example","title":"Full Pipeline Example","text":"<p>For full control, use the three ACE roles directly:</p> <pre><code>from ace_next import (\n    ACE, Agent, Reflector, SkillManager,\n    LiteLLMClient, Sample, SimpleEnvironment,\n)\n\n# Create LLM and roles\nllm = LiteLLMClient(model=\"gpt-4o-mini\")\nagent = Agent(llm)\nreflector = Reflector(llm)\nskill_manager = SkillManager(llm)\n\n# Build the adaptive pipeline\nrunner = ACE.from_roles(\n    agent=agent,\n    reflector=reflector,\n    skill_manager=skill_manager,\n    environment=SimpleEnvironment(),\n)\n\n# Train on samples\nsamples = [\n    Sample(question=\"What is the capital of France?\", context=\"\", ground_truth=\"Paris\"),\n    Sample(question=\"What is 2 + 2?\", context=\"\", ground_truth=\"4\"),\n]\n\nresults = runner.run(samples, epochs=2)\nprint(f\"Learned {len(runner.skillbook.skills())} strategies\")\nrunner.save(\"trained.json\")\n</code></pre>"},{"location":"getting-started/quick-start/#loading-saved-agents","title":"Loading Saved Agents","text":"<pre><code>from ace_next import ACELiteLLM\n\n# Resume from a saved skillbook\nagent = ACELiteLLM.from_model(\"gpt-4o-mini\", skillbook_path=\"my_agent.json\")\nanswer = agent.ask(\"New question\")  # Uses previously learned strategies\n</code></pre>"},{"location":"getting-started/quick-start/#trying-different-models","title":"Trying Different Models","text":"<pre><code>from ace_next import ACELiteLLM\n\n# OpenAI\nagent = ACELiteLLM.from_model(\"gpt-4o-mini\")\n\n# Anthropic\nagent = ACELiteLLM.from_model(\"claude-sonnet-4-5-20250929\")\n\n# Google\nagent = ACELiteLLM.from_model(\"gemini-pro\")\n\n# Local (Ollama)\nagent = ACELiteLLM.from_model(\"ollama/llama2\")\n</code></pre>"},{"location":"getting-started/quick-start/#what-to-read-next","title":"What to Read Next","text":"<ul> <li>How ACE Works \u2014 understand the three-role architecture</li> <li>The Skillbook \u2014 how strategies are stored and evolve</li> <li>Full Pipeline Guide \u2014 build custom ACE pipelines</li> <li>Integrations \u2014 LangChain, Browser-Use, Claude Code</li> </ul>"},{"location":"getting-started/setup/","title":"ACE Framework Setup Guide","text":"<p>Quick setup and configuration guide for ACE Framework.</p>"},{"location":"getting-started/setup/#requirements","title":"Requirements","text":"<ul> <li>Python 3.12</li> <li>API key for your LLM provider (OpenAI, Anthropic, Google, etc.)</li> </ul> <p>Check Python version: <pre><code>python --version  # Should show 3.12\n</code></pre></p>"},{"location":"getting-started/setup/#installation","title":"Installation","text":""},{"location":"getting-started/setup/#for-users","title":"For Users","text":"<pre><code># Basic installation\npip install ace-framework\n\n# With optional features\npip install ace-framework[instructor]     # Structured outputs (Instructor)\npip install ace-framework[langchain]      # LangChain integration\npip install ace-framework[browser-use]    # Browser automation\npip install ace-framework[claude-code]    # Claude Code CLI integration\npip install ace-framework[observability]  # Opik monitoring + cost tracking\npip install ace-framework[deduplication]  # Skill deduplication (embeddings)\npip install ace-framework[transformers]   # Local model support\npip install ace-framework[all]            # All features\n</code></pre>"},{"location":"getting-started/setup/#for-contributors","title":"For Contributors","text":"<pre><code>git clone https://github.com/kayba-ai/agentic-context-engine\ncd agentic-context-engine\nuv sync  # Installs everything automatically (10-100x faster than pip)\n</code></pre>"},{"location":"getting-started/setup/#api-key-setup","title":"API Key Setup","text":""},{"location":"getting-started/setup/#option-1-environment-variable-recommended","title":"Option 1: Environment Variable (Recommended)","text":"<pre><code># Set in your shell\nexport OPENAI_API_KEY=\"sk-...\"\n\n# Or create .env file\necho \"OPENAI_API_KEY=sk-...\" &gt; .env\n</code></pre> <p>Load in Python: <pre><code>from dotenv import load_dotenv\nload_dotenv()  # Loads from .env file\n</code></pre></p>"},{"location":"getting-started/setup/#option-2-direct-in-code","title":"Option 2: Direct in Code","text":"<pre><code>from ace import LiteLLMClient\n\nclient = LiteLLMClient(\n    model=\"gpt-4o-mini\",\n    api_key=\"your-key-here\"  # Not recommended for production\n)\n</code></pre>"},{"location":"getting-started/setup/#provider-examples","title":"Provider Examples","text":""},{"location":"getting-started/setup/#openai","title":"OpenAI","text":"<ol> <li>Get API key: platform.openai.com</li> <li>Set key: <code>export OPENAI_API_KEY=\"sk-...\"</code></li> <li>Use it: <pre><code>from ace import LiteLLMClient\nclient = LiteLLMClient(model=\"gpt-4o-mini\")\n</code></pre></li> </ol>"},{"location":"getting-started/setup/#anthropic-claude","title":"Anthropic Claude","text":"<ol> <li>Get API key: console.anthropic.com</li> <li>Set key: <code>export ANTHROPIC_API_KEY=\"sk-ant-...\"</code></li> <li>Use it: <pre><code>client = LiteLLMClient(model=\"claude-3-5-sonnet-20241022\")\n</code></pre></li> </ol>"},{"location":"getting-started/setup/#google-gemini","title":"Google Gemini","text":"<ol> <li>Get API key: makersuite.google.com</li> <li>Set key: <code>export GOOGLE_API_KEY=\"AIza...\"</code></li> <li>Use it: <pre><code>client = LiteLLMClient(model=\"gemini-pro\")\n</code></pre></li> </ol>"},{"location":"getting-started/setup/#local-models-ollama","title":"Local Models (Ollama)","text":"<ol> <li>Install Ollama: ollama.ai</li> <li>Pull model: <code>ollama pull llama2</code></li> <li>Use it: <pre><code>client = LiteLLMClient(model=\"ollama/llama2\")\n</code></pre></li> </ol> <p>Supported Providers: 100+ via LiteLLM (AWS Bedrock, Azure, Cohere, Hugging Face, etc.)</p>"},{"location":"getting-started/setup/#advanced-configuration","title":"Advanced Configuration","text":""},{"location":"getting-started/setup/#custom-llm-parameters","title":"Custom LLM Parameters","text":"<pre><code>from ace import LiteLLMClient\n\nclient = LiteLLMClient(\n    model=\"gpt-4o-mini\",\n    temperature=0.7,\n    max_tokens=2048,\n    timeout=60  # seconds\n)\n</code></pre>"},{"location":"getting-started/setup/#production-monitoring-opik","title":"Production Monitoring (Opik)","text":"<pre><code>pip install ace-framework[observability]\n</code></pre> <p>Opik automatically tracks: - Token usage per LLM call - Cost per operation - Agent/Reflector/SkillManager performance - Skillbook evolution over time</p> <p>View dashboard: comet.com/opik</p>"},{"location":"getting-started/setup/#skillbook-storage","title":"Skillbook Storage","text":"<pre><code>from ace import Skillbook\n\n# Save skillbook\nskillbook.save_to_file(\"my_skillbook.json\")\n\n# Load skillbook\nskillbook = Skillbook.load_from_file(\"my_skillbook.json\")\n\n# For production: Use database storage\n# PostgreSQL, SQLite, or vector stores supported\n</code></pre>"},{"location":"getting-started/setup/#checkpoint-saving","title":"Checkpoint Saving","text":"<pre><code>from ace import OfflineACE\n\nadapter = OfflineACE(\n    skillbook=skillbook,\n    agent=agent,\n    reflector=reflector,\n    skill_manager=skill_manager\n)\n\n# Save skillbook every 10 samples during training\nresults = adapter.run(\n    samples,\n    environment,\n    checkpoint_interval=10,\n    checkpoint_dir=\"./checkpoints\"\n)\n</code></pre>"},{"location":"getting-started/setup/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/setup/#import-errors","title":"Import Errors","text":"<pre><code># Upgrade to latest version\npip install --upgrade ace-framework\n\n# Check installation\npip show ace-framework\n</code></pre>"},{"location":"getting-started/setup/#api-key-not-working","title":"API Key Not Working","text":"<pre><code># Verify key is set\necho $OPENAI_API_KEY\n\n# Test different model\nfrom ace import LiteLLMClient\nclient = LiteLLMClient(model=\"gpt-3.5-turbo\")  # Cheaper for testing\n</code></pre>"},{"location":"getting-started/setup/#rate-limits","title":"Rate Limits","text":"<pre><code>from ace import LiteLLMClient\n\n# Add delays between calls\nimport time\ntime.sleep(1)  # 1 second between calls\n\n# Or use a cheaper/faster model\nclient = LiteLLMClient(model=\"gpt-3.5-turbo\")\n</code></pre>"},{"location":"getting-started/setup/#json-parse-failures","title":"JSON Parse Failures","text":"<pre><code># Increase max_tokens for SkillManager/Reflector\nfrom ace import SkillManager, Reflector\n\nllm = LiteLLMClient(model=\"gpt-4o-mini\", max_tokens=2048)  # Higher limit\nskill_manager = SkillManager(llm)\nreflector = Reflector(llm)\n</code></pre>"},{"location":"getting-started/setup/#need-more-help","title":"Need More Help?","text":"<ul> <li>GitHub Issues: github.com/kayba-ai/agentic-context-engine/issues</li> <li>Discord Community: discord.gg/mqCqH7sTyK</li> <li>Documentation: Complete Guide, Quick Start, Integration Guide</li> </ul> <p>Next Steps: Check out the Quick Start Guide to build your first self-learning agent!</p>"},{"location":"guides/async-learning/","title":"Async Learning","text":"<p>By default, learning (Reflect, Tag, Update, Apply) runs synchronously after each sample. With async learning, the Agent returns immediately while learning continues in the background.</p>"},{"location":"guides/async-learning/#architecture","title":"Architecture","text":"<pre><code>graph LR\n    S1[Sample 1] --&gt; A[Agent]\n    S2[Sample 2] --&gt; A\n    S3[Sample 3] --&gt; A\n    A --&gt;|foreground| E[Environment]\n    E --&gt;|background| R1[Reflector 1]\n    E --&gt; R2[Reflector 2]\n    E --&gt; R3[Reflector 3]\n    R1 --&gt; Q[Queue]\n    R2 --&gt; Q\n    R3 --&gt; Q\n    Q --&gt;|serialized| SM[SkillManager]\n    SM --&gt; SK[Skillbook]</code></pre> <ul> <li>Reflectors run concurrently (safe \u2014 they only read the skillbook)</li> <li>SkillManager runs sequentially (required \u2014 it writes to the skillbook)</li> <li>The Agent uses whatever skillbook state is available (eventual consistency)</li> </ul>"},{"location":"guides/async-learning/#basic-usage","title":"Basic Usage","text":"<p>Pass <code>wait=False</code> to <code>run()</code>:</p> <pre><code>from ace_next import ACE\n\nrunner = ACE.from_roles(\n    agent=agent,\n    reflector=reflector,\n    skill_manager=skill_manager,\n    environment=environment,\n)\n\n# Agent returns fast \u2014 learning continues in background\nresults = runner.run(samples, epochs=3, wait=False)\n\n# Use results immediately\nfor r in results:\n    print(r)\n\n# Wait before saving\nrunner.wait_for_background()\nrunner.save(\"learned.json\")\n</code></pre>"},{"location":"guides/async-learning/#monitoring-progress","title":"Monitoring Progress","text":"<pre><code>stats = runner.learning_stats\n# {'active': 5, 'completed': 25}\n</code></pre>"},{"location":"guides/async-learning/#with-acelitellm","title":"With ACELiteLLM","text":"<pre><code>from ace_next import ACELiteLLM, Sample, SimpleEnvironment\n\nagent = ACELiteLLM.from_model(\"gpt-4o-mini\")\n\nsamples = [Sample(question=\"...\", context=\"\", ground_truth=\"...\")]\nresults = agent.learn(samples, environment=SimpleEnvironment(), wait=False)\n\n# Agent is immediately available\nanswer = agent.ask(\"New question\")\n\n# Wait when you need to save\nagent.wait_for_background()\nagent.save(\"learned.json\")\n</code></pre>"},{"location":"guides/async-learning/#why-this-architecture","title":"Why This Architecture","text":"Component Parallelizable? Reason Reflector Yes Only reads the skillbook, produces independent analysis SkillManager No Writes to the skillbook, handles deduplication <p>This gives ~3x faster learning when the Reflector LLM calls run concurrently.</p>"},{"location":"guides/async-learning/#what-to-read-next","title":"What to Read Next","text":"<ul> <li>Full Pipeline Guide \u2014 synchronous pipeline setup</li> <li>Testing \u2014 test async learning with MagicMock</li> </ul>"},{"location":"guides/complete-guide/","title":"Complete ACE Guide","text":"<p>See also</p> <p>The full content for this page will be migrated from COMPLETE_GUIDE_TO_ACE.md.</p>"},{"location":"guides/full-pipeline/","title":"Full Pipeline Guide","text":"<p>This guide walks through building a complete ACE pipeline from scratch \u2014 choosing components, defining an environment, running training, and saving results.</p>"},{"location":"guides/full-pipeline/#components","title":"Components","text":"<p>A full pipeline needs four things:</p> <ol> <li>LLM Client \u2014 the language model powering all three roles</li> <li>Three Roles \u2014 Agent, Reflector, SkillManager</li> <li>Environment \u2014 evaluates agent outputs</li> <li>Samples \u2014 training data with questions and ground truth</li> </ol>"},{"location":"guides/full-pipeline/#step-1-create-the-llm-client","title":"Step 1: Create the LLM Client","text":"<pre><code>from ace_next import LiteLLMClient\n\nllm = LiteLLMClient(model=\"gpt-4o-mini\")\n</code></pre> <p>For robust JSON parsing with small models, wrap with Instructor (requires <code>pip install ace-framework[instructor]</code>):</p> <pre><code>from ace_next import LiteLLMClient, wrap_with_instructor\n\nllm = wrap_with_instructor(LiteLLMClient(model=\"ollama/gemma3:1b\"))\n</code></pre>"},{"location":"guides/full-pipeline/#step-2-create-the-roles","title":"Step 2: Create the Roles","text":"<pre><code>from ace_next import Agent, Reflector, SkillManager\n\nagent = Agent(llm)\nreflector = Reflector(llm)\nskill_manager = SkillManager(llm)\n</code></pre> <p>Optionally use a cheaper model for learning:</p> <pre><code>agent_llm = LiteLLMClient(model=\"gpt-4o\")\nlearning_llm = LiteLLMClient(model=\"gpt-4o-mini\")\n\nagent = Agent(agent_llm)\nreflector = Reflector(learning_llm)\nskill_manager = SkillManager(learning_llm)\n</code></pre>"},{"location":"guides/full-pipeline/#step-3-define-an-environment","title":"Step 3: Define an Environment","text":"<p>The environment evaluates agent outputs. Extend <code>TaskEnvironment</code> and implement <code>evaluate()</code>:</p> <pre><code>from ace_next import TaskEnvironment, EnvironmentResult\n\nclass MathEnvironment(TaskEnvironment):\n    def evaluate(self, sample, agent_output):\n        correct = str(sample.ground_truth).lower() in str(agent_output.final_answer).lower()\n        return EnvironmentResult(\n            feedback=\"Correct!\" if correct else f\"Incorrect. Expected: {sample.ground_truth}\",\n            ground_truth=sample.ground_truth,\n            metrics={\"accuracy\": 1.0 if correct else 0.0},\n        )\n</code></pre> <p>Or use the built-in <code>SimpleEnvironment</code> for basic ground-truth matching:</p> <pre><code>from ace_next import SimpleEnvironment\n\nenvironment = SimpleEnvironment()\n</code></pre>"},{"location":"guides/full-pipeline/#step-4-prepare-samples","title":"Step 4: Prepare Samples","text":"<pre><code>from ace_next import Sample\n\nsamples = [\n    Sample(question=\"What is 2+2?\", context=\"\", ground_truth=\"4\"),\n    Sample(question=\"Capital of France?\", context=\"\", ground_truth=\"Paris\"),\n    Sample(question=\"Who wrote Hamlet?\", context=\"\", ground_truth=\"Shakespeare\"),\n]\n</code></pre>"},{"location":"guides/full-pipeline/#step-5-build-and-run-the-pipeline","title":"Step 5: Build and Run the Pipeline","text":"<pre><code>from ace_next import ACE\n\nrunner = ACE.from_roles(\n    agent=agent,\n    reflector=reflector,\n    skill_manager=skill_manager,\n    environment=environment,\n)\n\nresults = runner.run(samples, epochs=3)\n</code></pre>"},{"location":"guides/full-pipeline/#step-6-save-the-skillbook","title":"Step 6: Save the Skillbook","text":"<pre><code>runner.save(\"trained.json\")\nprint(f\"Learned {len(runner.skillbook.skills())} strategies\")\n</code></pre>"},{"location":"guides/full-pipeline/#complete-example","title":"Complete Example","text":"<pre><code>from ace_next import (\n    ACE, Agent, Reflector, SkillManager,\n    LiteLLMClient, Sample, SimpleEnvironment,\n)\n\n# LLM and roles\nllm = LiteLLMClient(model=\"gpt-4o-mini\")\nagent = Agent(llm)\nreflector = Reflector(llm)\nskill_manager = SkillManager(llm)\n\n# Pipeline\nrunner = ACE.from_roles(\n    agent=agent,\n    reflector=reflector,\n    skill_manager=skill_manager,\n    environment=SimpleEnvironment(),\n)\n\n# Training data\nsamples = [\n    Sample(question=\"What is 2+2?\", context=\"\", ground_truth=\"4\"),\n    Sample(question=\"Capital of France?\", context=\"\", ground_truth=\"Paris\"),\n]\n\n# Train and save\nresults = runner.run(samples, epochs=3)\nrunner.save(\"trained.json\")\n</code></pre>"},{"location":"guides/full-pipeline/#checkpoints","title":"Checkpoints","text":"<p>Save the skillbook automatically during long training runs:</p> <pre><code>runner = ACE.from_roles(\n    agent=agent,\n    reflector=reflector,\n    skill_manager=skill_manager,\n    environment=environment,\n    checkpoint_dir=\"./checkpoints\",\n    checkpoint_interval=10,  # Save every 10 samples\n)\n</code></pre> <p>This creates:</p> <ul> <li><code>ace_checkpoint_10.json</code>, <code>ace_checkpoint_20.json</code>, etc.</li> <li><code>ace_latest.json</code> (always the most recent)</li> </ul>"},{"location":"guides/full-pipeline/#deduplication","title":"Deduplication","text":"<p>Prevent duplicate skills from accumulating (requires <code>pip install ace-framework[deduplication]</code>):</p> <pre><code>from ace_next import DeduplicationConfig, DeduplicationManager\n\ndedup = DeduplicationManager(DeduplicationConfig(\n    enabled=True,\n    embedding_model=\"text-embedding-3-small\",\n    similarity_threshold=0.85,\n))\n\nrunner = ACE.from_roles(\n    ...,\n    dedup_manager=dedup,\n    dedup_interval=10,\n)\n</code></pre>"},{"location":"guides/full-pipeline/#custom-prompts","title":"Custom Prompts","text":"<p>The default prompts are v2.1 and work well out of the box. You can pass your own templates via the <code>prompt_template</code> parameter:</p> <pre><code>agent = Agent(llm, prompt_template=\"Your custom agent prompt with {skillbook}, {question}, {context}\")\nreflector = Reflector(llm, prompt_template=\"Your custom reflector prompt ...\")\nskill_manager = SkillManager(llm, prompt_template=\"Your custom skill manager prompt ...\")\n</code></pre> <p>See Prompt Engineering for template variables and more examples.</p>"},{"location":"guides/full-pipeline/#testing-without-api-calls","title":"Testing Without API Calls","text":"<p>Use a mock to test pipeline wiring without making real LLM calls. Any object satisfying the <code>LLMClientLike</code> protocol (with <code>complete()</code> and <code>complete_structured()</code> methods) works:</p> <pre><code>from unittest.mock import MagicMock\n\nmock_llm = MagicMock()\nmock_llm.complete.return_value = '{\"reasoning\": \"test\", \"final_answer\": \"4\", \"skill_ids\": []}'\n\nagent = Agent(mock_llm)\nreflector = Reflector(mock_llm)\nskill_manager = SkillManager(mock_llm)\n</code></pre>"},{"location":"guides/full-pipeline/#observability","title":"Observability","text":"<p>Add Opik tracing to any pipeline via <code>extra_steps</code> (requires <code>pip install ace-framework[observability]</code>):</p> <pre><code>from ace_next import ACE, OpikStep, register_opik_litellm_callback\n\nrunner = ACE.from_roles(\n    agent=agent,\n    reflector=reflector,\n    skill_manager=skill_manager,\n    environment=environment,\n    extra_steps=[OpikStep(project_name=\"my-experiment\")],\n)\n\n# Optionally add per-LLM-call cost tracking\nregister_opik_litellm_callback(project_name=\"my-experiment\")\n</code></pre> <p>See Opik Observability for full details.</p>"},{"location":"guides/full-pipeline/#what-to-read-next","title":"What to Read Next","text":"<ul> <li>Async Learning \u2014 parallel Reflector execution</li> <li>Prompt Engineering \u2014 customize prompt templates</li> <li>Integration Pattern \u2014 wrap existing agents instead</li> <li>Opik Observability \u2014 monitor costs and traces</li> </ul>"},{"location":"guides/integration/","title":"Integration Pattern","text":"<p>Use the integration pattern when you have an existing agent (browser-use, LangChain, Claude Code, or a custom framework) and want to add ACE learning on top.</p> <p>Full Pipeline vs Integration</p> <p>The Full Pipeline uses all three ACE roles. The integration pattern skips the ACE Agent \u2014 your external agent handles execution, and ACE only learns from the results.</p>"},{"location":"guides/integration/#three-steps","title":"Three Steps","text":"<p>Every integration follows the same pattern:</p> <pre><code>1. INJECT   \u2014 Add skillbook strategies to the agent's context\n2. EXECUTE  \u2014 Run the external agent normally\n3. LEARN    \u2014 Reflector + SkillManager update the skillbook\n</code></pre>"},{"location":"guides/integration/#using-built-in-runners","title":"Using Built-In Runners","text":"<p>ACE provides runners for popular frameworks. Each uses <code>from_model()</code> for quick setup or <code>from_roles()</code> for full control:</p> Browser-UseLangChainClaude Code <pre><code>from ace_next import BrowserUse\nfrom langchain_openai import ChatOpenAI\n\nrunner = BrowserUse.from_model(\n    browser_llm=ChatOpenAI(model=\"gpt-4o\"),\n    ace_model=\"gpt-4o-mini\",\n)\nresults = runner.run([\"Find top HN post\", \"Check weather in NYC\"])\nrunner.save(\"browser_expert.json\")\n</code></pre> <pre><code>from ace_next import LangChain\n\nrunner = LangChain.from_model(your_chain, ace_model=\"gpt-4o-mini\")\nresults = runner.run([{\"input\": \"Summarize this document\"}])\nrunner.save(\"chain_expert.json\")\n</code></pre> <pre><code>from ace_next import ClaudeCode\n\nrunner = ClaudeCode.from_model(working_dir=\"./my_project\")\nresults = runner.run([\"Add tests for utils.py\", \"Fix the login bug\"])\nrunner.save(\"code_expert.json\")\n</code></pre>"},{"location":"guides/integration/#construction-patterns","title":"Construction Patterns","text":"<p>All integration runners offer two construction paths:</p>"},{"location":"guides/integration/#from_model-quick-setup","title":"from_model() \u2014 Quick Setup","text":"<p>Builds ACE roles automatically from a model string:</p> <pre><code>runner = BrowserUse.from_model(\n    browser_llm=ChatOpenAI(model=\"gpt-4o\"),\n    ace_model=\"gpt-4o-mini\",       # Model for Reflector + SkillManager\n    skillbook_path=\"saved.json\",   # Optional: resume from saved skillbook\n)\n</code></pre>"},{"location":"guides/integration/#from_roles-full-control","title":"from_roles() \u2014 Full Control","text":"<p>Provide pre-built role instances:</p> <pre><code>from ace_next import Reflector, SkillManager, LiteLLMClient\n\nlearning_llm = LiteLLMClient(model=\"gpt-4o-mini\")\n\nrunner = BrowserUse.from_roles(\n    browser_llm=ChatOpenAI(model=\"gpt-4o\"),\n    reflector=Reflector(learning_llm),\n    skill_manager=SkillManager(learning_llm),\n    skillbook_path=\"saved.json\",\n    dedup_config=my_dedup_config,\n    checkpoint_dir=\"./checkpoints\",\n)\n</code></pre>"},{"location":"guides/integration/#common-options","title":"Common Options","text":"<p>All integration runners share these parameters:</p> Parameter Description Default <code>skillbook</code> Existing <code>Skillbook</code> instance <code>None</code> (creates empty) <code>skillbook_path</code> Path to load skillbook from <code>None</code> <code>dedup_config</code> Deduplication configuration <code>None</code> <code>dedup_interval</code> Samples between dedup runs <code>10</code> <code>checkpoint_dir</code> Directory for checkpoint files <code>None</code> <code>checkpoint_interval</code> Samples between checkpoints <code>10</code>"},{"location":"guides/integration/#lifecycle-methods","title":"Lifecycle Methods","text":"<p>All runners expose:</p> <pre><code>runner.save(\"path.json\")              # Save skillbook\nrunner.wait_for_background()          # Wait for async learning\nrunner.learning_stats                 # Background progress dict\nrunner.skillbook                      # Current Skillbook instance\nrunner.get_strategies()               # Formatted strategies string\n</code></pre>"},{"location":"guides/integration/#building-a-custom-integration","title":"Building a Custom Integration","text":"<p>For frameworks not covered by the built-in runners, you can compose a custom pipeline using steps.</p> <p>The pattern: Execute Step (runs your agent) + ToTrace Step (extracts learning signal) + learning_tail() (standard learning pipeline).</p> <pre><code>from pipeline import Pipeline\nfrom ace_next import Skillbook, Reflector, SkillManager, LiteLLMClient\nfrom ace_next.steps import learning_tail\nfrom ace_next.runners import ACERunner\n\n# Your custom execute step would implement the Step protocol\n# See the Pipeline Engine docs for details on building custom steps\n\nlearning_llm = LiteLLMClient(model=\"gpt-4o-mini\")\nskillbook = Skillbook()\n\nsteps = [\n    MyCustomExecuteStep(...),\n    MyCustomToTraceStep(),\n    *learning_tail(\n        Reflector(learning_llm),\n        SkillManager(learning_llm),\n        skillbook,\n    ),\n]\n\nrunner = ACERunner(pipeline=Pipeline(steps), skillbook=skillbook)\n</code></pre> <p>See Pipeline Engine: Building Custom Steps for the Step protocol.</p>"},{"location":"guides/integration/#what-to-read-next","title":"What to Read Next","text":"<ul> <li>LiteLLM Integration \u2014 simplest self-improving agent</li> <li>Browser-Use Integration \u2014 browser automation details</li> <li>LangChain Integration \u2014 chain/agent wrapping</li> <li>Claude Code Integration \u2014 coding tasks</li> </ul>"},{"location":"guides/prompts/","title":"Prompt Engineering","text":"<p>ACE uses specialized prompt templates for each role. The framework includes multiple prompt versions with different trade-offs.</p>"},{"location":"guides/prompts/#default-prompts","title":"Default Prompts","text":"<p><code>ace_next</code> ships with v2.1 prompts built in. All three roles (<code>Agent</code>, <code>Reflector</code>, <code>SkillManager</code>) use them by default \u2014 no extra imports needed.</p> <p>Recommendation</p> <p>The built-in v2.1 prompts work well out of the box. Only provide custom prompts when you need domain-specific instructions.</p>"},{"location":"guides/prompts/#overriding-prompts","title":"Overriding Prompts","text":"<p>Pass a <code>prompt_template</code> string to any role constructor:</p> <pre><code>from ace_next import Agent, Reflector, SkillManager, LiteLLMClient\n\nllm = LiteLLMClient(model=\"gpt-4o-mini\")\n\nagent = Agent(llm, prompt_template=\"Your custom agent prompt ...\")\nreflector = Reflector(llm, prompt_template=\"Your custom reflector prompt ...\")\nskill_manager = SkillManager(llm, prompt_template=\"Your custom skill manager prompt ...\")\n</code></pre>"},{"location":"guides/prompts/#template-variables","title":"Template Variables","text":""},{"location":"guides/prompts/#agent-prompt","title":"Agent Prompt","text":"Variable Description <code>{skillbook}</code> Current skillbook in TOON format <code>{question}</code> The input question <code>{context}</code> Additional context <code>{reflection}</code> Optional reflection from a previous attempt"},{"location":"guides/prompts/#reflector-prompt","title":"Reflector Prompt","text":"Variable Description <code>{skillbook}</code> Current skillbook in TOON format <code>{question}</code> The original question <code>{agent_output}</code> The agent's response <code>{ground_truth}</code> Expected answer <code>{feedback}</code> Environment feedback"},{"location":"guides/prompts/#skillmanager-prompt","title":"SkillManager Prompt","text":"Variable Description <code>{skillbook}</code> Current skillbook in TOON format <code>{reflection}</code> Reflector's analysis <code>{question_context}</code> Description of the task domain <code>{progress}</code> Current training progress"},{"location":"guides/prompts/#custom-prompts","title":"Custom Prompts","text":"<p>You can provide your own prompt templates. They must include the required template variables:</p> <pre><code>custom_agent_prompt = \"\"\"\nSkillbook: {skillbook}\nQuestion: {question}\nContext: {context}\n\nGenerate a JSON response with:\n- reasoning: Your step-by-step thought process\n- skill_ids: List of skillbook IDs you used\n- final_answer: Your answer\n\"\"\"\n\nagent = Agent(llm, prompt_template=custom_agent_prompt)\n</code></pre>"},{"location":"guides/prompts/#formatting-skillbook-for-external-agents","title":"Formatting Skillbook for External Agents","text":"<p>Integration runners inject the skillbook into external agent prompts using a wrapper function:</p> <pre><code>from ace_next import wrap_skillbook_context\n\ncontext = wrap_skillbook_context(skillbook)\n# Returns formatted strategies with usage instructions\n</code></pre>"},{"location":"guides/prompts/#troubleshooting","title":"Troubleshooting","text":"Problem Solution JSON parse failures Increase <code>max_tokens</code>, use Instructor, or try v2.1 prompts Empty skill_ids Agent not citing skills \u2014 check skillbook has content Poor answer quality Switch to v2.1 prompts or try a larger model"},{"location":"guides/prompts/#what-to-read-next","title":"What to Read Next","text":"<ul> <li>Full Pipeline Guide \u2014 use prompts in a complete pipeline</li> <li>The Skillbook \u2014 what goes into <code>{skillbook}</code></li> </ul>"},{"location":"guides/testing/","title":"Testing","text":""},{"location":"guides/testing/#running-tests","title":"Running Tests","text":"pytest (recommended)unittest <pre><code>uv run pytest                         # All tests\nuv run pytest -m unit                 # Unit tests only\nuv run pytest -m integration          # Integration tests only\nuv run pytest tests/test_skillbook.py # Specific file\nuv run pytest -v                      # Verbose output\n</code></pre> <pre><code>python -m unittest discover -s tests\npython -m unittest discover -s tests -v  # Verbose\n</code></pre>"},{"location":"guides/testing/#testing-without-api-calls","title":"Testing Without API Calls","text":"<p>Use a mock LLM to test pipeline wiring without making real API calls. Any object with <code>complete()</code> and <code>complete_structured()</code> methods satisfies the <code>LLMClientLike</code> protocol:</p> <pre><code>from unittest.mock import MagicMock\nfrom ace_next import Agent, Reflector, SkillManager\n\nmock_llm = MagicMock()\nmock_llm.complete.return_value = '{\"reasoning\": \"test\", \"final_answer\": \"4\", \"skill_ids\": []}'\n\nagent = Agent(mock_llm)\nreflector = Reflector(mock_llm)\nskill_manager = SkillManager(mock_llm)\n</code></pre>"},{"location":"guides/testing/#unit-testing","title":"Unit Testing","text":""},{"location":"guides/testing/#testing-the-skillbook","title":"Testing the Skillbook","text":"<pre><code>from ace_next import Skillbook\n\ndef test_add_skill():\n    skillbook = Skillbook()\n    skill = skillbook.add_skill(\n        section=\"Test\",\n        content=\"Test strategy\",\n        metadata={\"helpful\": 0, \"harmful\": 0, \"neutral\": 0},\n    )\n    assert len(skillbook.skills()) == 1\n    assert skill.content == \"Test strategy\"\n\ndef test_save_load(tmp_path):\n    skillbook = Skillbook()\n    skillbook.add_skill(section=\"Test\", content=\"Strategy\")\n\n    path = str(tmp_path / \"test.json\")\n    skillbook.save_to_file(path)\n\n    loaded = Skillbook.load_from_file(path)\n    assert len(loaded.skills()) == 1\n</code></pre>"},{"location":"guides/testing/#testing-the-agent","title":"Testing the Agent","text":"<pre><code>from unittest.mock import MagicMock\nfrom ace_next import Agent, Skillbook\n\ndef test_agent_generate():\n    mock_llm = MagicMock()\n    mock_llm.complete.return_value = '{\"reasoning\": \"2+2=4\", \"final_answer\": \"4\", \"skill_ids\": []}'\n\n    agent = Agent(mock_llm)\n    output = agent.generate(\n        question=\"What is 2+2?\",\n        context=\"\",\n        skillbook=Skillbook(),\n    )\n    assert output.final_answer is not None\n    assert output.reasoning is not None\n</code></pre>"},{"location":"guides/testing/#testing-reflector-and-skillmanager","title":"Testing Reflector and SkillManager","text":"<pre><code>from unittest.mock import MagicMock\nfrom ace_next import Agent, Reflector, SkillManager, Skillbook\n\ndef make_mock_llm():\n    mock = MagicMock()\n    mock.complete.return_value = '{\"reasoning\": \"test\", \"final_answer\": \"4\", \"skill_ids\": []}'\n    return mock\n\ndef test_reflector():\n    mock_llm = make_mock_llm()\n    reflector = Reflector(mock_llm)\n    agent = Agent(mock_llm)\n\n    output = agent.generate(question=\"Test\", context=\"\", skillbook=Skillbook())\n    reflection = reflector.reflect(\n        question=\"Test\",\n        agent_output=output,\n        skillbook=Skillbook(),\n        ground_truth=\"expected\",\n        feedback=\"Correct\",\n    )\n    assert reflection.key_insight is not None\n\ndef test_skill_manager():\n    sm = SkillManager(make_mock_llm())\n    # ... similar pattern with reflection input\n</code></pre>"},{"location":"guides/testing/#integration-testing","title":"Integration Testing","text":""},{"location":"guides/testing/#end-to-end-learning-cycle","title":"End-to-End Learning Cycle","text":"<pre><code>from unittest.mock import MagicMock\nfrom ace_next import (\n    ACE, Agent, Reflector, SkillManager,\n    Sample, SimpleEnvironment,\n)\n\ndef test_full_learning_cycle():\n    mock_llm = MagicMock()\n    mock_llm.complete.return_value = '{\"reasoning\": \"test\", \"final_answer\": \"answer\", \"skill_ids\": []}'\n\n    runner = ACE.from_roles(\n        agent=Agent(mock_llm),\n        reflector=Reflector(mock_llm),\n        skill_manager=SkillManager(mock_llm),\n        environment=SimpleEnvironment(),\n    )\n\n    samples = [Sample(question=\"Test\", context=\"\", ground_truth=\"answer\")]\n    results = runner.run(samples, epochs=1)\n\n    assert len(results) == 1\n</code></pre>"},{"location":"guides/testing/#testing-checkpoints","title":"Testing Checkpoints","text":"<pre><code>def test_checkpoints(tmp_path):\n    mock_llm = MagicMock()\n    mock_llm.complete.return_value = '{\"reasoning\": \"test\", \"final_answer\": \"A\", \"skill_ids\": []}'\n\n    runner = ACE.from_roles(\n        agent=Agent(mock_llm),\n        reflector=Reflector(mock_llm),\n        skill_manager=SkillManager(mock_llm),\n        environment=SimpleEnvironment(),\n        checkpoint_dir=str(tmp_path),\n        checkpoint_interval=1,\n    )\n\n    samples = [Sample(question=\"Q\", context=\"\", ground_truth=\"A\")]\n    runner.run(samples, epochs=1)\n\n    # Check that checkpoint files were created\n    checkpoints = list(tmp_path.glob(\"ace_*.json\"))\n    assert len(checkpoints) &gt; 0\n</code></pre>"},{"location":"guides/testing/#common-test-patterns","title":"Common Test Patterns","text":""},{"location":"guides/testing/#fixtures","title":"Fixtures","text":"<pre><code>import pytest\nfrom unittest.mock import MagicMock\nfrom ace_next import Agent, Reflector, SkillManager, Skillbook\n\n@pytest.fixture\ndef mock_llm():\n    mock = MagicMock()\n    mock.complete.return_value = '{\"reasoning\": \"test\", \"final_answer\": \"4\", \"skill_ids\": []}'\n    return mock\n\n@pytest.fixture\ndef skillbook():\n    return Skillbook()\n\n@pytest.fixture\ndef agent(mock_llm):\n    return Agent(mock_llm)\n</code></pre>"},{"location":"guides/testing/#mocking-llm-responses","title":"Mocking LLM Responses","text":"<pre><code>from unittest.mock import MagicMock\n\ndef test_with_mock():\n    mock_llm = MagicMock()\n    mock_llm.complete.return_value = '{\"reasoning\": \"...\", \"final_answer\": \"4\", \"skill_ids\": []}'\n\n    agent = Agent(mock_llm)\n    # ...\n</code></pre>"},{"location":"guides/testing/#ci-configuration","title":"CI Configuration","text":"<pre><code># .github/workflows/test.yml\nname: Tests\non: [push, pull_request]\njobs:\n  test:\n    runs-on: ubuntu-latest\n    steps:\n      - uses: actions/checkout@v4\n      - uses: astral-sh/setup-uv@v4\n      - run: uv sync\n      - run: uv run pytest -v\n</code></pre>"},{"location":"guides/testing/#code-quality","title":"Code Quality","text":"<pre><code>uv run black ace/ tests/ examples/     # Format\nuv run mypy ace/                       # Type check\nuv run pre-commit run --all-files      # All hooks\n</code></pre>"},{"location":"guides/testing/#troubleshooting","title":"Troubleshooting","text":"Problem Solution Import errors Run <code>uv sync</code> to install all dependencies API key errors in tests Use <code>MagicMock</code> for unit tests (see above) Flaky async tests Increase timeout or use <code>wait_for_background()</code> Coverage too low <code>--cov-fail-under=25</code> is the threshold"},{"location":"guides/testing/#what-to-read-next","title":"What to Read Next","text":"<ul> <li>Full Pipeline Guide \u2014 what you're testing</li> <li>Async Learning \u2014 testing async pipelines</li> </ul>"},{"location":"integrations/","title":"Integrations Overview","text":"<p>ACE provides runners for popular agentic frameworks. Each runner adds self-improving learning to an existing agent with minimal code changes.</p>"},{"location":"integrations/#available-integrations","title":"Available Integrations","text":"Runner Framework Input Insight Level <code>ACELiteLLM</code> LiteLLM (100+ providers) Questions Micro <code>LangChain</code> LangChain Runnables Chain inputs Meso <code>BrowserUse</code> browser-use Task strings Meso <code>ClaudeCode</code> Claude Code CLI Task strings Meso Opik Opik observability \u2014 Monitoring"},{"location":"integrations/#the-pattern","title":"The Pattern","text":"<p>All integration runners follow the same three-step pattern:</p> <pre><code>1. INJECT   \u2014 Add skillbook strategies to the agent's context\n2. EXECUTE  \u2014 Run the external agent normally\n3. LEARN    \u2014 Reflector + SkillManager update the skillbook\n</code></pre>"},{"location":"integrations/#quick-construction","title":"Quick Construction","text":"<p>Every runner offers a <code>from_model()</code> factory that builds ACE roles automatically:</p> <pre><code>from ace_next import BrowserUse, LangChain, ClaudeCode\n\n# Browser automation\nbrowser = BrowserUse.from_model(browser_llm=my_llm, ace_model=\"gpt-4o-mini\")\n\n# LangChain chain/agent\nchain = LangChain.from_model(my_runnable, ace_model=\"gpt-4o-mini\")\n\n# Claude Code CLI\ncoder = ClaudeCode.from_model(working_dir=\"./project\", ace_model=\"gpt-4o-mini\")\n</code></pre>"},{"location":"integrations/#shared-features","title":"Shared Features","text":"<p>All runners share these capabilities:</p> <ul> <li>Skillbook persistence \u2014 <code>save()</code> / load via <code>skillbook_path</code></li> <li>Checkpointing \u2014 automatic saves during long runs</li> <li>Deduplication \u2014 prevent duplicate skills</li> <li>Background learning \u2014 <code>wait=False</code> for async learning</li> <li>Progress tracking \u2014 <code>learning_stats</code> property</li> </ul>"},{"location":"integrations/#which-integration-should-i-use","title":"Which Integration Should I Use?","text":"<ul> <li>Building a Q&amp;A or reasoning agent? Use ACELiteLLM</li> <li>Have an existing LangChain chain or agent? Use LangChain</li> <li>Automating browser tasks? Use BrowserUse</li> <li>Running coding tasks with Claude Code? Use ClaudeCode</li> <li>Want to monitor costs and traces? Add Opik</li> <li>Using a different framework? See the Integration Guide to build a custom runner</li> </ul>"},{"location":"integrations/browser-use/","title":"Browser-Use Integration","text":"<p>The <code>BrowserUse</code> runner wraps browser-use with ACE learning. The agent automates browser tasks and learns strategies from each run \u2014 improving navigation, element selection, and error recovery over time.</p>"},{"location":"integrations/browser-use/#installation","title":"Installation","text":"<pre><code>pip install ace-framework[browser-use]\n</code></pre>"},{"location":"integrations/browser-use/#quick-start","title":"Quick Start","text":"<pre><code>from ace_next import BrowserUse\nfrom langchain_openai import ChatOpenAI\n\nrunner = BrowserUse.from_model(\n    browser_llm=ChatOpenAI(model=\"gpt-4o\"),\n    ace_model=\"gpt-4o-mini\",\n)\n\nresults = runner.run(\"Find the top post on Hacker News\")\nrunner.save(\"browser_expert.json\")\n</code></pre>"},{"location":"integrations/browser-use/#parameters","title":"Parameters","text":""},{"location":"integrations/browser-use/#from_model","title":"from_model()","text":"Parameter Type Default Description <code>browser_llm</code> <code>Any</code> \u2014 LLM for browser-use execution <code>ace_model</code> <code>str</code> <code>\"gpt-4o-mini\"</code> Model for Reflector + SkillManager <code>ace_max_tokens</code> <code>int</code> <code>2048</code> Max tokens for ACE LLM <code>ace_llm</code> <code>LLMClientLike</code> <code>None</code> Pre-built LLM for ACE roles"},{"location":"integrations/browser-use/#from_roles","title":"from_roles()","text":"Parameter Type Default Description <code>browser_llm</code> <code>Any</code> \u2014 LLM for browser-use execution <code>reflector</code> <code>ReflectorLike</code> \u2014 Reflector instance <code>skill_manager</code> <code>SkillManagerLike</code> \u2014 SkillManager instance <code>skillbook_path</code> <code>str</code> <code>None</code> Load saved skillbook <code>browser</code> <code>Browser</code> <code>None</code> browser-use Browser instance <code>agent_kwargs</code> <code>dict</code> <code>None</code> Extra kwargs for browser-use Agent <code>dedup_config</code> <code>DeduplicationConfig</code> <code>None</code> Deduplication config <code>checkpoint_dir</code> <code>str</code> <code>None</code> Checkpoint directory"},{"location":"integrations/browser-use/#methods","title":"Methods","text":"<pre><code>results = runner.run(tasks, epochs=1)       # Run with learning\nrunner.save(\"path.json\")                    # Save skillbook\nrunner.wait_for_background()                # Wait for async learning\nrunner.get_strategies()                     # View learned strategies\n</code></pre>"},{"location":"integrations/browser-use/#how-it-works","title":"How It Works","text":"<ol> <li>INJECT \u2014 Skillbook strategies are added to the task prompt</li> <li>EXECUTE \u2014 browser-use runs the task (navigation, clicks, form fills)</li> <li>Extract trace \u2014 ACE extracts a chronological trace of agent thoughts, actions, and results</li> <li>LEARN \u2014 Reflector analyzes the full trace, SkillManager updates the skillbook</li> </ol> <p>The extracted trace includes:</p> <ul> <li>Agent reasoning at each step</li> <li>Browser actions taken (click, type, navigate)</li> <li>Page observations</li> <li>Success/failure of each action</li> </ul>"},{"location":"integrations/browser-use/#running-multiple-tasks","title":"Running Multiple Tasks","text":"<pre><code>results = runner.run([\n    \"Find the top post on Hacker News\",\n    \"Search for ACE framework on GitHub\",\n    \"Check the weather in NYC\",\n])\n</code></pre>"},{"location":"integrations/browser-use/#example-domain-checker","title":"Example: Domain Checker","text":"<pre><code>from ace_next import BrowserUse\nfrom langchain_openai import ChatOpenAI\n\nrunner = BrowserUse.from_model(\n    browser_llm=ChatOpenAI(model=\"gpt-4o\"),\n    ace_model=\"gpt-4o-mini\",\n)\n\ndomains = [\"example.com\", \"test.org\", \"sample.net\"]\nfor domain in domains:\n    runner.run(f\"Check if {domain} is available for registration\")\n\n# After several runs, the agent learns:\n# - Which registrar sites to use\n# - How to navigate the domain search UI\n# - How to interpret availability results\nrunner.save(\"domain_checker.json\")\n</code></pre>"},{"location":"integrations/browser-use/#resuming-from-a-saved-skillbook","title":"Resuming from a Saved Skillbook","text":"<pre><code>runner = BrowserUse.from_model(\n    browser_llm=ChatOpenAI(model=\"gpt-4o\"),\n    ace_model=\"gpt-4o-mini\",\n    skillbook_path=\"browser_expert.json\",\n)\n</code></pre>"},{"location":"integrations/browser-use/#what-to-read-next","title":"What to Read Next","text":"<ul> <li>Integration Pattern \u2014 how the INJECT/EXECUTE/LEARN pattern works</li> <li>The Skillbook \u2014 how learned strategies are stored</li> <li>Opik Observability \u2014 monitor browser automation costs</li> </ul>"},{"location":"integrations/claude-code/","title":"Claude Code Integration","text":"<p>The <code>ClaudeCode</code> runner wraps the Claude Code CLI with ACE learning. The agent runs coding tasks in your project directory and learns strategies from each execution \u2014 improving code generation, debugging, and project-specific patterns over time.</p>"},{"location":"integrations/claude-code/#quick-start","title":"Quick Start","text":"<pre><code>from ace_next import ClaudeCode\n\nrunner = ClaudeCode.from_model(working_dir=\"./my_project\")\n\nresults = runner.run(\"Add unit tests for utils.py\")\nrunner.save(\"coding_expert.json\")\n</code></pre>"},{"location":"integrations/claude-code/#installation","title":"Installation","text":"<pre><code>pip install ace-framework[claude-code]\n</code></pre>"},{"location":"integrations/claude-code/#prerequisites","title":"Prerequisites","text":"<ul> <li>Claude Code CLI installed and authenticated</li> <li>A project directory with source code</li> </ul>"},{"location":"integrations/claude-code/#parameters","title":"Parameters","text":""},{"location":"integrations/claude-code/#from_model","title":"from_model()","text":"Parameter Type Default Description <code>working_dir</code> <code>str</code> <code>None</code> Path to the project directory <code>ace_model</code> <code>str</code> <code>\"gpt-4o-mini\"</code> Model for Reflector + SkillManager <code>ace_max_tokens</code> <code>int</code> <code>2048</code> Max tokens for ACE LLM <code>ace_llm</code> <code>LLMClientLike</code> <code>None</code> Pre-built LLM for ACE roles"},{"location":"integrations/claude-code/#from_roles","title":"from_roles()","text":"Parameter Type Default Description <code>reflector</code> <code>ReflectorLike</code> \u2014 Reflector instance <code>skill_manager</code> <code>SkillManagerLike</code> \u2014 SkillManager instance <code>working_dir</code> <code>str</code> <code>None</code> Project directory <code>timeout</code> <code>int</code> <code>600</code> Execution timeout (seconds) <code>model</code> <code>str</code> <code>None</code> Claude model override <code>allowed_tools</code> <code>list[str]</code> <code>None</code> Allowed Claude Code tools <code>skillbook_path</code> <code>str</code> <code>None</code> Load saved skillbook <code>dedup_config</code> <code>DeduplicationConfig</code> <code>None</code> Deduplication config <code>checkpoint_dir</code> <code>str</code> <code>None</code> Checkpoint directory"},{"location":"integrations/claude-code/#methods","title":"Methods","text":"<pre><code>results = runner.run(tasks, epochs=1)       # Run with learning\nrunner.save(\"path.json\")                    # Save skillbook\nrunner.wait_for_background()                # Wait for async learning\nrunner.get_strategies()                     # View learned strategies\n</code></pre>"},{"location":"integrations/claude-code/#how-it-works","title":"How It Works","text":"<ol> <li>INJECT \u2014 Skillbook strategies are written to <code>CLAUDE.md</code> at the project root</li> <li>EXECUTE \u2014 Claude Code CLI runs the task in the project directory</li> <li>Extract trace \u2014 ACE reads the Claude Code execution transcript</li> <li>LEARN \u2014 Reflector analyzes the trace, SkillManager updates the skillbook</li> </ol> <p>The agent learns project-specific patterns like:</p> <ul> <li>Code style and conventions</li> <li>Common debugging approaches</li> <li>Test patterns and frameworks used</li> <li>Module structure and dependencies</li> </ul>"},{"location":"integrations/claude-code/#running-multiple-tasks","title":"Running Multiple Tasks","text":"<pre><code>results = runner.run([\n    \"Add unit tests for utils.py\",\n    \"Fix the bug in the login handler\",\n    \"Refactor the database module to use connection pooling\",\n])\n</code></pre>"},{"location":"integrations/claude-code/#resuming-from-a-saved-skillbook","title":"Resuming from a Saved Skillbook","text":"<pre><code>runner = ClaudeCode.from_model(\n    working_dir=\"./my_project\",\n    skillbook_path=\"coding_expert.json\",\n)\n</code></pre>"},{"location":"integrations/claude-code/#what-to-read-next","title":"What to Read Next","text":"<ul> <li>Integration Pattern \u2014 how the INJECT/EXECUTE/LEARN pattern works</li> <li>The Skillbook \u2014 how learned strategies are stored</li> </ul>"},{"location":"integrations/langchain/","title":"LangChain Integration","text":"<p>The <code>LangChain</code> runner wraps any LangChain Runnable (chains, <code>AgentExecutor</code>, LangGraph graphs) with ACE learning. The runner extracts execution traces and learns strategies from them.</p>"},{"location":"integrations/langchain/#installation","title":"Installation","text":"<pre><code>pip install ace-framework[langchain]\n</code></pre>"},{"location":"integrations/langchain/#quick-start","title":"Quick Start","text":"<pre><code>from ace_next import LangChain\n\nrunner = LangChain.from_model(your_chain, ace_model=\"gpt-4o-mini\")\n\nresults = runner.run([\n    {\"input\": \"Summarize this document\"},\n    {\"input\": \"Extract key entities\"},\n])\n\nrunner.save(\"chain_expert.json\")\n</code></pre>"},{"location":"integrations/langchain/#parameters","title":"Parameters","text":""},{"location":"integrations/langchain/#from_model","title":"from_model()","text":"Parameter Type Default Description <code>runnable</code> <code>Any</code> \u2014 LangChain Runnable (chain, AgentExecutor, graph) <code>ace_model</code> <code>str</code> <code>\"gpt-4o-mini\"</code> Model for Reflector + SkillManager <code>ace_max_tokens</code> <code>int</code> <code>2048</code> Max tokens for ACE LLM <code>ace_llm</code> <code>LLMClientLike</code> <code>None</code> Pre-built LLM for ACE roles"},{"location":"integrations/langchain/#from_roles","title":"from_roles()","text":"Parameter Type Default Description <code>runnable</code> <code>Any</code> \u2014 LangChain Runnable <code>reflector</code> <code>ReflectorLike</code> \u2014 Reflector instance <code>skill_manager</code> <code>SkillManagerLike</code> \u2014 SkillManager instance <code>skillbook_path</code> <code>str</code> <code>None</code> Load saved skillbook <code>output_parser</code> <code>Callable</code> <code>None</code> Custom output extraction <code>dedup_config</code> <code>DeduplicationConfig</code> <code>None</code> Deduplication config <code>checkpoint_dir</code> <code>str</code> <code>None</code> Checkpoint directory"},{"location":"integrations/langchain/#methods","title":"Methods","text":"<pre><code>results = runner.run(inputs, epochs=1)      # Run with learning\nresults = runner.invoke(single_input)       # Single input convenience\nrunner.save(\"path.json\")                    # Save skillbook\nrunner.wait_for_background()                # Wait for async learning\n</code></pre>"},{"location":"integrations/langchain/#how-it-works","title":"How It Works","text":"<ol> <li>INJECT \u2014 Skillbook strategies are added to the chain input</li> <li>EXECUTE \u2014 LangChain runs the chain normally</li> <li>Extract trace \u2014 ACE extracts intermediate steps, tool calls, and reasoning</li> <li>LEARN \u2014 Reflector analyzes the trace, SkillManager updates the skillbook</li> </ol> <p>The runner handles simple chains, <code>AgentExecutor</code> (with <code>intermediate_steps</code>), and LangGraph graphs automatically.</p>"},{"location":"integrations/langchain/#input-types","title":"Input Types","text":"<p>The runner accepts any input your chain expects:</p> <pre><code># String input\nrunner.run([\"What is ACE?\"])\n\n# Dict input\nrunner.run([{\"input\": \"query\", \"context\": \"...\"}])\n\n# Message list\nrunner.run([[HumanMessage(content=\"Hello\")]])\n</code></pre>"},{"location":"integrations/langchain/#resuming-from-a-saved-skillbook","title":"Resuming from a Saved Skillbook","text":"<pre><code>runner = LangChain.from_model(\n    your_chain,\n    ace_model=\"gpt-4o-mini\",\n    skillbook_path=\"chain_expert.json\",\n)\n</code></pre>"},{"location":"integrations/langchain/#what-to-read-next","title":"What to Read Next","text":"<ul> <li>Integration Pattern \u2014 how the INJECT/EXECUTE/LEARN pattern works</li> <li>Insight Levels \u2014 meso-level learning from traces</li> <li>Opik Observability \u2014 monitor chain execution costs</li> </ul>"},{"location":"integrations/litellm/","title":"LiteLLM Integration","text":"<p><code>ACELiteLLM</code> is the simplest way to get a self-improving agent. It bundles Agent, Reflector, SkillManager, and Skillbook into a single class with <code>ask()</code> and <code>learn()</code> methods.</p>"},{"location":"integrations/litellm/#quick-start","title":"Quick Start","text":"<pre><code>from ace_next import ACELiteLLM\n\nagent = ACELiteLLM.from_model(\"gpt-4o-mini\")\n\n# Ask questions \u2014 learns patterns across them\nanswer = agent.ask(\"If all cats are animals, is Felix (a cat) an animal?\")\n\n# Save and reload\nagent.save(\"learned.json\")\n</code></pre>"},{"location":"integrations/litellm/#parameters","title":"Parameters","text":""},{"location":"integrations/litellm/#from_model","title":"from_model()","text":"Parameter Type Default Description <code>model</code> <code>str</code> <code>\"gpt-4o-mini\"</code> LiteLLM model identifier <code>max_tokens</code> <code>int</code> <code>2048</code> Max tokens for responses <code>temperature</code> <code>float</code> <code>0.0</code> Sampling temperature <code>api_key</code> <code>str</code> <code>None</code> API key (or use env variable) <code>base_url</code> <code>str</code> <code>None</code> Custom API endpoint <code>skillbook_path</code> <code>str</code> <code>None</code> Path to load saved skillbook <code>environment</code> <code>TaskEnvironment</code> <code>None</code> Evaluation environment <code>dedup_config</code> <code>DeduplicationConfig</code> <code>None</code> Skill deduplication config <code>is_learning</code> <code>bool</code> <code>True</code> Enable/disable learning <code>opik</code> <code>bool</code> <code>False</code> Enable Opik observability (pipeline traces + LiteLLM per-call cost tracking) <code>opik_project</code> <code>str</code> <code>\"ace-framework\"</code> Opik project name for organizing traces <code>opik_tags</code> <code>list[str]</code> <code>None</code> Tags applied to every Opik trace"},{"location":"integrations/litellm/#methods","title":"Methods","text":""},{"location":"integrations/litellm/#ask","title":"ask()","text":"<p>Direct agent call using the current skillbook:</p> <pre><code>answer = agent.ask(\"Your question\", context=\"Optional context\")\n</code></pre>"},{"location":"integrations/litellm/#learn","title":"learn()","text":"<p>Run the full ACE learning pipeline over samples:</p> <pre><code>from ace_next import Sample, SimpleEnvironment\n\nsamples = [\n    Sample(question=\"What is 2+2?\", context=\"\", ground_truth=\"4\"),\n]\nresults = agent.learn(samples, environment=SimpleEnvironment(), epochs=3)\n</code></pre>"},{"location":"integrations/litellm/#learn_from_feedback","title":"learn_from_feedback()","text":"<p>Learn from the last <code>ask()</code> interaction:</p> <pre><code>agent.ask(\"What is the capital of France?\")\nagent.learn_from_feedback(feedback=\"Correct!\", ground_truth=\"Paris\")\n</code></pre>"},{"location":"integrations/litellm/#learn_from_traces","title":"learn_from_traces()","text":"<p>Learn from pre-recorded execution traces:</p> <pre><code>results = agent.learn_from_traces(traces, epochs=1)\n</code></pre>"},{"location":"integrations/litellm/#lifecycle","title":"Lifecycle","text":"<pre><code>agent.save(\"path.json\")               # Save skillbook\nagent.load(\"path.json\")               # Load skillbook\nagent.enable_learning()               # Turn on learning\nagent.disable_learning()              # Turn off learning\nagent.wait_for_background()           # Wait for async learning\nagent.learning_stats                  # Background progress\nagent.skillbook                       # Current Skillbook\nagent.get_strategies()                # Formatted strategies\n</code></pre>"},{"location":"integrations/litellm/#using-a-cheaper-learning-model","title":"Using a Cheaper Learning Model","text":"<p>Use a strong model for the Agent and a cheaper one for learning:</p> <pre><code>from ace_next import ACELiteLLM, Agent, Reflector, SkillManager, LiteLLMClient\n\nagent_llm = LiteLLMClient(model=\"gpt-4o\")\nlearning_llm = LiteLLMClient(model=\"gpt-4o-mini\")\n\nace = ACELiteLLM(\n    llm=agent_llm,\n    agent=Agent(agent_llm),\n    reflector=Reflector(learning_llm),\n    skill_manager=SkillManager(learning_llm),\n)\n</code></pre>"},{"location":"integrations/litellm/#deduplication","title":"Deduplication","text":"<p>Prevent duplicate skills from accumulating:</p> <pre><code>from ace_next import DeduplicationConfig\n\nagent = ACELiteLLM.from_model(\n    \"gpt-4o-mini\",\n    dedup_config=DeduplicationConfig(\n        enabled=True,\n        embedding_model=\"text-embedding-3-small\",\n        similarity_threshold=0.85,\n    ),\n)\n</code></pre>"},{"location":"integrations/litellm/#supported-providers","title":"Supported Providers","text":"<p>Any model supported by LiteLLM:</p> <pre><code># OpenAI\nagent = ACELiteLLM.from_model(\"gpt-4o-mini\")\n\n# Anthropic\nagent = ACELiteLLM.from_model(\"claude-sonnet-4-5-20250929\")\n\n# Google\nagent = ACELiteLLM.from_model(\"gemini-pro\")\n\n# Local (Ollama)\nagent = ACELiteLLM.from_model(\"ollama/llama2\")\n\n# Custom endpoint\nagent = ACELiteLLM.from_model(\"gpt-4o-mini\", base_url=\"https://your-endpoint.com\")\n</code></pre>"},{"location":"integrations/litellm/#opik-observability","title":"Opik Observability","text":"<p>Enable tracing and cost tracking with a single flag:</p> <pre><code>ace = ACELiteLLM.from_model(\"gpt-4o-mini\", opik=True, opik_project=\"my-experiment\")\n\n# Both tracing modes are enabled:\n# 1. Pipeline traces (OpikStep) \u2014 one trace per sample with ACE context\n# 2. LiteLLM callback \u2014 per-LLM-call token/cost tracking\n\nresults = ace.learn(samples, environment=SimpleEnvironment(), epochs=3)\n# View traces at http://localhost:5173 \u2192 project \"my-experiment\"\n</code></pre> <p>See Opik Observability for full details, environment variables, and manual setup.</p>"},{"location":"integrations/litellm/#what-to-read-next","title":"What to Read Next","text":"<ul> <li>Full Pipeline Guide \u2014 for more control over the pipeline</li> <li>Async Learning \u2014 background learning with <code>wait=False</code></li> <li>Opik Observability \u2014 monitor costs and traces</li> </ul>"},{"location":"integrations/opik/","title":"Opik Observability","text":"<p>ACE integrates with Opik for tracing, cost tracking, and performance monitoring. All Opik tracing is explicit opt-in \u2014 it is never auto-enabled just because the package is installed.</p> <p>Two independent tracing modes:</p> <ol> <li>Pipeline step (<code>OpikStep</code>) \u2014 client-agnostic, logs one Opik trace per sample with ACE context fields.</li> <li>LiteLLM callback (<code>register_opik_litellm_callback</code>) \u2014 LiteLLM-specific, tracks per-LLM-call tokens and costs.</li> </ol>"},{"location":"integrations/opik/#installation","title":"Installation","text":"<pre><code>pip install ace-framework[observability]\n</code></pre>"},{"location":"integrations/opik/#quick-start","title":"Quick Start","text":"<pre><code>from ace_next import ACELiteLLM\n\n# Easiest: ACELiteLLM enables both tracing modes with one flag\nace = ACELiteLLM.from_model(\"gpt-4o-mini\", opik=True, opik_project=\"my-experiment\")\n</code></pre> <pre><code>from ace_next import (\n    ACE, OpikStep,\n    Agent, Reflector, SkillManager,\n    LiteLLMClient, SimpleEnvironment,\n)\n\n# Manual: Add OpikStep via extra_steps\nclient = LiteLLMClient(model=\"gpt-4o-mini\")\n\nrunner = ACE.from_roles(\n    agent=Agent(client),\n    reflector=Reflector(client),\n    skill_manager=SkillManager(client),\n    environment=SimpleEnvironment(),\n    extra_steps=[OpikStep(project_name=\"my-experiment\")],\n)\n</code></pre> <pre><code># LLM-level cost tracking only (no pipeline traces)\nfrom ace_next import register_opik_litellm_callback\n\nregistered = register_opik_litellm_callback(project_name=\"my-experiment\")\n</code></pre>"},{"location":"integrations/opik/#starting-the-opik-server","title":"Starting the Opik Server","text":"Local (Docker)Comet Cloud <pre><code>docker run -d -p 5173:5173 --name opik ghcr.io/comet-ml/opik:latest\n\n# View traces at http://localhost:5173\n</code></pre> <pre><code>export COMET_API_KEY=\"your-api-key\"\n# Traces appear at https://www.comet.com/opik\n</code></pre>"},{"location":"integrations/opik/#opikstep","title":"OpikStep","text":"<p><code>OpikStep</code> is a terminal side-effect step that logs one Opik trace per sample. It reads context fields but never mutates them \u2014 safe to append to any pipeline.</p>"},{"location":"integrations/opik/#parameters","title":"Parameters","text":"Parameter Type Default Description <code>project_name</code> <code>str</code> <code>\"ace-framework\"</code> Opik project for organizing traces <code>tags</code> <code>list[str]</code> <code>None</code> Extra tags attached to every trace"},{"location":"integrations/opik/#what-gets-logged","title":"What Gets Logged","text":"<p>Each trace includes:</p> Field Source Input Question and context from the sample Output Answer, reasoning, and skill IDs from <code>AgentOutput</code> Metadata Epoch, step index, skill count, reflection insights, operation counts Feedback scores Accuracy extracted from environment feedback (correct / incorrect)"},{"location":"integrations/opik/#trace-hierarchy","title":"Trace Hierarchy","text":"<pre><code>graph TD\n    P[\"Project: my-experiment\"]\n    P --&gt; T[\"Trace: sample_run_001\"]\n    T --&gt; I[\"Input: question + context\"]\n    T --&gt; O[\"Output: answer + reasoning + skill_ids\"]\n    T --&gt; M[\"Metadata: epoch=2, skills=12, ops=3\"]\n    T --&gt; F[\"Feedback: accuracy=1.0\"]\n    T --&gt; L[\"LLM Calls (automatic)\"]\n    L --&gt; L1[\"agent_generate \u2014 450 tokens, $0.0003\"]\n    L --&gt; L2[\"reflector_reflect \u2014 620 tokens, $0.0004\"]\n    L --&gt; L3[\"skill_manager_update \u2014 380 tokens, $0.0002\"]</code></pre>"},{"location":"integrations/opik/#llm-cost-tracking","title":"LLM Cost Tracking","text":"<p><code>OpikStep</code> does not register the LiteLLM callback \u2014 the two tracing modes are independent. To get per-LLM-call cost tracking, call <code>register_opik_litellm_callback()</code> separately:</p> <pre><code>from ace_next import register_opik_litellm_callback\n\nsuccess = register_opik_litellm_callback(project_name=\"cost-tracking\")\n# Returns True if registered, False if Opik unavailable\n</code></pre> <p>Every LLM call is then automatically tracked with:</p> <ul> <li>Input / output tokens</li> <li>Model used</li> <li>Cost per call</li> <li>Latency</li> </ul> <p>When using <code>ACELiteLLM</code> with <code>opik=True</code>, both modes are enabled together automatically \u2014 no need to call <code>register_opik_litellm_callback()</code> manually.</p>"},{"location":"integrations/opik/#environment-variables","title":"Environment Variables","text":"Variable Description Default <code>OPIK_PROJECT_NAME</code> Project name for organizing traces <code>ace-framework</code> <code>OPIK_DISABLED=true</code> Disable all Opik tracing Not set <code>OPIK_ENABLED=false</code> Alternative way to disable tracing Not set <code>OPIK_URL_OVERRIDE</code> Custom Opik server URL <code>http://localhost:5173/api</code> <code>OPIK_WORKSPACE</code> Opik workspace name <code>default</code>"},{"location":"integrations/opik/#error-handling","title":"Error Handling","text":"<p>When using <code>ACELiteLLM</code> with <code>opik=True</code>, errors are raised immediately:</p> <ul> <li><code>ImportError</code> if the <code>opik</code> package is not installed</li> <li><code>RuntimeError</code> if the Opik client fails to initialize (bad config, disabled via env vars)</li> </ul> <p>This ensures you know immediately if tracing is broken, rather than discovering missing traces later.</p> <p>When using <code>OpikStep</code> directly via <code>extra_steps</code>, it soft-imports Opik and silently becomes a no-op if the package is absent \u2014 useful for pipelines that should work with or without observability.</p> <pre><code>from ace_next import OPIK_AVAILABLE\n\nif OPIK_AVAILABLE:\n    print(\"Opik tracing is available\")\n</code></pre>"},{"location":"integrations/opik/#troubleshooting-opikconfig","title":"Troubleshooting: <code>~/.opik.config</code>","text":"<p>The Opik SDK stores a global config file at <code>~/.opik.config</code> (created by <code>opik.configure()</code>). This file overrides environment variables and can cause silent failures if it contains stale settings.</p> <p>If traces aren't appearing, check:</p> <pre><code>cat ~/.opik.config\n</code></pre> <p>A correct config for Comet Cloud looks like:</p> <pre><code>[opik]\nurl_override = https://www.comet.com/opik/api/\nworkspace = your-workspace-name\n</code></pre> <p>Common issues:</p> <ul> <li>Wrong URL: <code>https://www.comet.com/api/</code> (missing <code>/opik/</code>) causes 404 errors</li> <li>Wrong workspace: <code>workspace = default</code> instead of your actual workspace name</li> <li>Stale config: Re-run <code>opik.configure()</code> or edit the file directly to fix</li> </ul>"},{"location":"integrations/opik/#disabling-tracing","title":"Disabling Tracing","text":"<pre><code># In CI or tests\nOPIK_DISABLED=true pytest tests/\n\n# Or via the alternative variable\nOPIK_ENABLED=false python my_script.py\n</code></pre>"},{"location":"integrations/opik/#full-example","title":"Full Example","text":"ACELiteLLM (easiest)ACE runner (manual) <pre><code>from ace_next import ACELiteLLM, Sample, SimpleEnvironment\n\nace = ACELiteLLM.from_model(\"gpt-4o-mini\", opik=True, opik_project=\"ace-training\")\n\nsamples = [\n    Sample(question=\"What is 2+2?\", context=\"\", ground_truth=\"4\"),\n    Sample(question=\"Capital of France?\", context=\"\", ground_truth=\"Paris\"),\n]\n\nresults = ace.learn(samples, environment=SimpleEnvironment(), epochs=3)\nace.save(\"trained.json\")\n\n# View traces at http://localhost:5173 \u2192 project \"ace-training\"\n</code></pre> <pre><code>from ace_next import (\n    ACE, Agent, Reflector, SkillManager, Skillbook,\n    LiteLLMClient, SimpleEnvironment, Sample, OpikStep,\n    register_opik_litellm_callback,\n)\n\nclient = LiteLLMClient(model=\"gpt-4o-mini\")\n\nrunner = ACE.from_roles(\n    agent=Agent(client),\n    reflector=Reflector(client),\n    skill_manager=SkillManager(client),\n    environment=SimpleEnvironment(),\n    extra_steps=[OpikStep(project_name=\"ace-training\")],\n)\n\n# Optionally add LLM-level cost tracking\nregister_opik_litellm_callback(project_name=\"ace-training\")\n\nsamples = [\n    Sample(question=\"What is 2+2?\", context=\"\", ground_truth=\"4\"),\n    Sample(question=\"Capital of France?\", context=\"\", ground_truth=\"Paris\"),\n]\n\nresults = runner.run(samples, epochs=3)\nrunner.save(\"trained.json\")\n</code></pre>"},{"location":"integrations/opik/#what-to-read-next","title":"What to Read Next","text":"<ul> <li>Integration Pattern \u2014 how runners compose pipeline steps</li> <li>Full Pipeline Guide \u2014 building pipelines from scratch</li> <li>Async Learning \u2014 background learning with cost monitoring</li> </ul>"},{"location":"pipeline/","title":"Pipeline Engine","text":"<p>A generic, composable step runner for ordered and parallel data processing.</p>"},{"location":"pipeline/#what-is-the-pipeline-engine","title":"What is the Pipeline Engine?","text":"<p>The Pipeline Engine is a lightweight, domain-agnostic framework for composing processing steps into pipelines. It provides contract validation, immutable context passing, and built-in concurrency control \u2014 all in ~300 lines of pure Python with no external dependencies beyond the standard library.</p> <p>Everything composes from three primitives:</p> <p>Sequential \u2014 steps run one after another:</p> <pre><code>graph LR\n    A1[Step A] --&gt; B1[Step B] --&gt; C1[Step C]</code></pre> <p>Branch \u2014 fork, run in parallel, join:</p> <pre><code>graph LR\n    A2[Step A] --&gt; B2[Step B] &amp; C2[Step C] --&gt; D2[Step D]</code></pre> <p>Nesting \u2014 a pipeline used as a step:</p> <pre><code>graph LR\n    A3[Step A] --&gt; P3[[Inner Pipeline]] --&gt; D3[Step D]</code></pre> <p>Steps declare what data they read and write. The pipeline validates ordering at construction time \u2014 before any data flows \u2014 so wiring errors surface immediately, not at runtime.</p>"},{"location":"pipeline/#core-principles","title":"Core Principles","text":"<ul> <li>Three primitives \u2014 Sequential steps, parallel branches, and nested pipelines cover every composition pattern</li> <li>Contracts \u2014 Steps declare <code>requires</code> and <code>provides</code> fields; the pipeline validates ordering at construction time</li> <li>Immutable context \u2014 Steps receive a frozen context and return a new one via <code>.replace()</code>, making concurrent execution safe by default</li> <li>Declared concurrency \u2014 Parallelism is configured on the step (<code>max_workers</code>, <code>async_boundary</code>), not the pipeline</li> <li>Per-sample error isolation \u2014 One failing sample never blocks others; every sample produces a result</li> </ul>"},{"location":"pipeline/#architecture-at-a-glance","title":"Architecture at a Glance","text":"<pre><code>classDiagram\n    class StepProtocol {\n        &lt;&lt;protocol&gt;&gt;\n        +requires: set[str]\n        +provides: set[str]\n        +__call__(ctx: StepContext) StepContext\n    }\n\n    class Pipeline {\n        +then(step) Pipeline\n        +branch(*pipelines) Pipeline\n        +run(samples) list~SampleResult~\n        +run_async(samples) list~SampleResult~\n    }\n\n    class Branch {\n        +merge: MergeStrategy\n    }\n\n    class YourStep {\n        +requires: set[str]\n        +provides: set[str]\n        +__call__(ctx) StepContext\n    }\n\n    class StepContext {\n        &lt;&lt;frozen dataclass&gt;&gt;\n        +sample: str\n        +metadata: MappingProxyType\n        +replace(**kw) StepContext\n    }\n\n    class SampleResult {\n        &lt;&lt;dataclass&gt;&gt;\n        +context: StepContext\n        +error: Exception?\n        +ok: bool\n    }\n\n    StepProtocol &lt;|.. Pipeline : satisfies\n    StepProtocol &lt;|.. Branch : satisfies\n    StepProtocol &lt;|.. YourStep : satisfies\n    Pipeline *-- \"1..*\" StepProtocol : contains steps\n    Branch *-- \"2..*\" Pipeline : contains pipelines\n    StepProtocol ..&gt; StepContext : receives &amp; returns\n    Pipeline ..&gt; SampleResult : produces</code></pre> <p><code>Pipeline</code> and <code>Branch</code> both satisfy <code>StepProtocol</code> through structural typing \u2014 no inheritance required. This means a <code>Pipeline</code> can be used as a step inside another pipeline, and a <code>Branch</code> slots into any step position.</p> Concept What it is Threading Data flow Step Single unit of work Sync internally Receives and returns <code>StepContext</code> Pipeline Ordered chain of steps <code>workers=N</code> across samples Passes <code>StepContext</code> step-to-step Branch Parallel fork/join One thread per branch Copies context in, merges outputs Nested Pipeline Pipeline used as a step Inherits parent threading Same <code>StepContext</code> flow"},{"location":"pipeline/#async-boundary-background-processing","title":"Async Boundary \u2014 Background Processing","text":"<p>One of the engine's key features is the async boundary: a way to split a pipeline into foreground (fast return) and background (fire-and-forget) stages.</p> <pre><code>graph LR\n    S1[\"Step A\"] --&gt; S2[\"Step B\"] --&gt; AB{{\"async_boundary\"}} --&gt; S3[\"Step C&lt;br/&gt;&lt;small&gt;background&lt;/small&gt;\"] --&gt; S4[\"Step D&lt;br/&gt;&lt;small&gt;background&lt;/small&gt;\"]\n\n    style S1 fill:#6366f1,stroke:#4f46e5,color:#fff\n    style S2 fill:#6366f1,stroke:#4f46e5,color:#fff\n    style AB fill:#f59e0b,stroke:#d97706,color:#000\n    style S3 fill:#3b82f6,stroke:#2563eb,color:#fff\n    style S4 fill:#3b82f6,stroke:#2563eb,color:#fff</code></pre> <p>Mark any step with <code>async_boundary = True</code> \u2014 the pipeline returns results immediately after the foreground steps, while everything from the boundary onward continues in background threads. Use <code>pipe.wait_for_background()</code> when you need the final results.</p> <p>This is critical for pipelines where early steps produce user-facing output quickly but later steps (analysis, logging, scoring) are slow and don't need to block the caller. See Execution Model for full details.</p>"},{"location":"pipeline/#when-to-use","title":"When to Use","text":"<p>Good fit</p> <ul> <li>Ordered multi-step processing with explicit data dependencies</li> <li>Parallel fork/join patterns (multiple independent operations on the same data)</li> <li>Fire-and-forget background processing with <code>async_boundary</code></li> <li>Any pipeline where you want construction-time contract validation</li> </ul> <p>Not designed for</p> <ul> <li>DAG scheduling with complex dependency graphs</li> <li>Distributed computing across multiple machines</li> <li>Stream processing with backpressure</li> <li>ETL pipelines requiring a data catalog</li> </ul>"},{"location":"pipeline/#installation","title":"Installation","text":"<p>The pipeline engine is included in the project with no extra dependencies:</p> <pre><code>from pipeline import Pipeline, Branch, StepContext, MergeStrategy\n</code></pre>"},{"location":"pipeline/#whats-next","title":"What's Next","text":"<ul> <li>Quick Start \u2014 Build and run your first pipeline in under 30 lines</li> <li>Core Concepts \u2014 Understand Step, Context, and the contract system</li> <li>Execution Model \u2014 Three types of async, workers, and background processing</li> <li>Branching &amp; Parallelism \u2014 Parallel fork/join with merge strategies</li> <li>Error Handling \u2014 Per-sample isolation, SampleResult, and error types</li> <li>Building Custom Steps \u2014 Create your own steps with dependency injection</li> <li>API Reference \u2014 Complete signatures for all public classes</li> </ul>"},{"location":"pipeline/api-reference/","title":"API Reference","text":"<p>Complete reference for all public classes, methods, and enums in the pipeline engine.</p>"},{"location":"pipeline/api-reference/#pipelinecontext","title":"<code>pipeline.context</code>","text":""},{"location":"pipeline/api-reference/#stepcontext","title":"<code>StepContext</code>","text":"<p>Frozen dataclass passed from step to step. The pipeline engine only reads <code>sample</code> and <code>metadata</code> \u2014 domain-specific fields are added by subclassing.</p> <pre><code>@dataclass(frozen=True)\nclass StepContext:\n    sample: Any = None\n    metadata: MappingProxyType = field(\n        default_factory=lambda: MappingProxyType({})\n    )\n</code></pre> Method Signature Description <code>replace</code> <code>(**changes: Any) -&gt; StepContext</code> Return a new context with the given fields replaced. Uses <code>dataclasses.replace</code> internally. <p>Behavior:</p> <ul> <li><code>metadata</code> is auto-coerced from <code>dict</code> to <code>MappingProxyType</code> in <code>__post_init__</code></li> <li>Subclasses inherit <code>.replace()</code> \u2014 it works on all fields including subclass-defined ones</li> </ul>"},{"location":"pipeline/api-reference/#pipelineprotocol","title":"<code>pipeline.protocol</code>","text":""},{"location":"pipeline/api-reference/#stepprotocol","title":"<code>StepProtocol</code>","text":"<p>Structural protocol that every step (and Pipeline/Branch) must satisfy.</p> <pre><code>@runtime_checkable\nclass StepProtocol(Protocol):\n    requires: AbstractSet[str]\n    provides: AbstractSet[str]\n\n    def __call__(self, ctx: StepContext) -&gt; StepContext: ...\n</code></pre> Attribute Type Description <code>requires</code> <code>AbstractSet[str]</code> Metadata keys the step reads <code>provides</code> <code>AbstractSet[str]</code> Metadata keys the step writes <code>__call__</code> <code>(StepContext) -&gt; StepContext</code> Execute the step <p>Notes:</p> <ul> <li><code>AbstractSet[str]</code> accepts both <code>set</code> and <code>frozenset</code></li> <li><code>@runtime_checkable</code> enables <code>isinstance(step, StepProtocol)</code> checks</li> </ul>"},{"location":"pipeline/api-reference/#sampleresult","title":"<code>SampleResult</code>","text":"<p>Outcome for one sample after the pipeline has run.</p> <pre><code>@dataclass\nclass SampleResult:\n    sample: Any\n    output: StepContext | None\n    error: Exception | None\n    failed_at: str | None\n    cause: Exception | None = None\n</code></pre> Field Type Description <code>sample</code> <code>Any</code> The original input sample <code>output</code> <code>StepContext \\| None</code> Final context (<code>None</code> if any step failed) <code>error</code> <code>Exception \\| None</code> The exception (<code>None</code> if succeeded) <code>failed_at</code> <code>str \\| None</code> Class name of the step that raised (<code>None</code> if succeeded) <code>cause</code> <code>Exception \\| None</code> Inner exception for <code>BranchError</code> failures (default <code>None</code>) <p>Notes:</p> <ul> <li>Mutable \u2014 background threads update it in-place when background steps complete</li> <li>For background steps, <code>output</code>/<code>error</code> may be <code>None</code> until <code>wait_for_background()</code> completes</li> </ul>"},{"location":"pipeline/api-reference/#pipelinepipeline","title":"<code>pipeline.pipeline</code>","text":""},{"location":"pipeline/api-reference/#pipeline","title":"<code>Pipeline</code>","text":"<p>Ordered sequence of steps. Satisfies <code>StepProtocol</code> \u2014 can be nested inside other pipelines.</p>"},{"location":"pipeline/api-reference/#constructor","title":"Constructor","text":"<pre><code>Pipeline(steps: list | None = None)\n</code></pre> Parameter Type Default Description <code>steps</code> <code>list \\| None</code> <code>None</code> Optional initial list of steps <p>Validates step ordering and infers contracts at construction time.</p>"},{"location":"pipeline/api-reference/#attributes","title":"Attributes","text":"Attribute Type Description <code>requires</code> <code>frozenset[str]</code> Fields the pipeline needs from external context (auto-inferred) <code>provides</code> <code>frozenset[str]</code> Fields the pipeline writes (auto-inferred, union of all steps)"},{"location":"pipeline/api-reference/#methods","title":"Methods","text":""},{"location":"pipeline/api-reference/#then","title":"<code>then</code>","text":"<pre><code>def then(self, step: object) -&gt; Pipeline\n</code></pre> <p>Append a step and return <code>self</code> for chaining. Validates ordering immediately.</p> Parameter Type Description <code>step</code> <code>object</code> Any object satisfying <code>StepProtocol</code> <p>Returns: <code>self</code> (for method chaining)</p> <p>Raises: <code>PipelineOrderError</code> if the step requires a field produced by a later step</p>"},{"location":"pipeline/api-reference/#branch","title":"<code>branch</code>","text":"<pre><code>def branch(\n    self,\n    *pipelines: object,\n    merge: MergeStrategy | Callable = MergeStrategy.RAISE_ON_CONFLICT,\n) -&gt; Pipeline\n</code></pre> <p>Append a <code>Branch</code> step and return <code>self</code> for chaining. Shorthand for <code>.then(Branch(*pipelines, merge=merge))</code>.</p> <p>Returns: <code>self</code> (for method chaining)</p>"},{"location":"pipeline/api-reference/#run","title":"<code>run</code>","text":"<pre><code>def run(\n    self,\n    contexts: Iterable[StepContext],\n    workers: int = 1,\n) -&gt; list[SampleResult]\n</code></pre> <p>Process contexts through the pipeline (sync entry point).</p> Parameter Type Default Description <code>contexts</code> <code>Iterable[StepContext]</code> \u2014 Input contexts to process <code>workers</code> <code>int</code> <code>1</code> Max concurrent samples in foreground steps <p>Returns: <code>list[SampleResult]</code> \u2014 one result per input context</p> <p>Notes: Calls <code>asyncio.run(self.run_async(...))</code> internally. For background steps, call <code>wait_for_background()</code> after this returns.</p>"},{"location":"pipeline/api-reference/#run_async","title":"<code>run_async</code>","text":"<pre><code>async def run_async(\n    self,\n    contexts: Iterable[StepContext],\n    workers: int = 1,\n) -&gt; list[SampleResult]\n</code></pre> <p>Async entry point. Use <code>await pipe.run_async(contexts)</code> from coroutine contexts.</p> <p>Same parameters and return type as <code>run()</code>.</p>"},{"location":"pipeline/api-reference/#__call__","title":"<code>__call__</code>","text":"<pre><code>def __call__(self, ctx: StepContext) -&gt; StepContext\n</code></pre> <p>Run all steps sequentially on a single context. Used when the pipeline is nested as a step inside another pipeline.</p> <p>Notes: <code>async_boundary</code> markers are ignored in this mode \u2014 all steps run to completion.</p>"},{"location":"pipeline/api-reference/#wait_for_background","title":"<code>wait_for_background</code>","text":"<pre><code>def wait_for_background(self, timeout: float | None = None) -&gt; None\n</code></pre> <p>Block until all background tasks complete.</p> Parameter Type Default Description <code>timeout</code> <code>float \\| None</code> <code>None</code> Max seconds to wait. <code>None</code> = wait indefinitely. <p>Raises: <code>TimeoutError</code> if timeout elapses before completion</p>"},{"location":"pipeline/api-reference/#background_stats","title":"<code>background_stats</code>","text":"<pre><code>def background_stats(self) -&gt; dict[str, int]\n</code></pre> <p>Return a snapshot of background task progress. Thread-safe.</p> <p>Returns: <code>{\"active\": int, \"completed\": int}</code></p>"},{"location":"pipeline/api-reference/#pipelinebranch","title":"<code>pipeline.branch</code>","text":""},{"location":"pipeline/api-reference/#mergestrategy","title":"<code>MergeStrategy</code>","text":"<p>Enum of built-in merge strategies for <code>Branch</code> outputs.</p> <pre><code>class MergeStrategy(Enum):\n    RAISE_ON_CONFLICT = \"raise_on_conflict\"\n    LAST_WRITE_WINS = \"last_write_wins\"\n    NAMESPACED = \"namespaced\"\n</code></pre> Value Behavior <code>RAISE_ON_CONFLICT</code> Raises <code>ValueError</code> if two branches write different values to the same named field. Metadata merges with last-writer-wins. <code>LAST_WRITE_WINS</code> Last branch's value wins for every conflicting field. <code>NAMESPACED</code> Each branch's output stored at <code>metadata[\"branch_N\"]</code>. No conflict possible."},{"location":"pipeline/api-reference/#branch_1","title":"<code>Branch</code>","text":"<p>Runs multiple pipelines in parallel, then merges their outputs. Satisfies <code>StepProtocol</code>.</p>"},{"location":"pipeline/api-reference/#constructor_1","title":"Constructor","text":"<pre><code>Branch(\n    *pipelines: object,\n    merge: MergeStrategy | Callable = MergeStrategy.RAISE_ON_CONFLICT,\n)\n</code></pre> Parameter Type Default Description <code>*pipelines</code> <code>object</code> \u2014 Child pipelines to run in parallel (at least one required) <code>merge</code> <code>MergeStrategy \\| Callable</code> <code>RAISE_ON_CONFLICT</code> Merge strategy or custom <code>fn(list[StepContext]) -&gt; StepContext</code> <p>Raises: <code>ValueError</code> if no pipelines are provided</p>"},{"location":"pipeline/api-reference/#attributes_1","title":"Attributes","text":"Attribute Type Description <code>requires</code> <code>frozenset[str]</code> Union of all children's requires <code>provides</code> <code>frozenset[str]</code> Union of all children's provides <code>pipelines</code> <code>list</code> The child pipelines"},{"location":"pipeline/api-reference/#methods_1","title":"Methods","text":""},{"location":"pipeline/api-reference/#__call___1","title":"<code>__call__</code>","text":"<pre><code>def __call__(self, ctx: StepContext) -&gt; StepContext\n</code></pre> <p>Sync fan-out via <code>ThreadPoolExecutor</code>. All branches run to completion before any failure is raised.</p> <p>Raises: <code>BranchError</code> if any branch fails</p>"},{"location":"pipeline/api-reference/#__call_async__","title":"<code>__call_async__</code>","text":"<pre><code>async def __call_async__(self, ctx: StepContext) -&gt; StepContext\n</code></pre> <p>Async fan-out via <code>asyncio.gather</code>. Sync children are wrapped with <code>asyncio.to_thread</code>.</p> <p>Raises: <code>BranchError</code> if any branch fails</p>"},{"location":"pipeline/api-reference/#pipelineerrors","title":"<code>pipeline.errors</code>","text":""},{"location":"pipeline/api-reference/#pipelineordererror","title":"<code>PipelineOrderError</code>","text":"<pre><code>class PipelineOrderError(Exception): ...\n</code></pre> <p>A step requires a field that no earlier step provides (but a later step does). Raised at construction time.</p>"},{"location":"pipeline/api-reference/#pipelineconfigerror","title":"<code>PipelineConfigError</code>","text":"<pre><code>class PipelineConfigError(Exception): ...\n</code></pre> <p>Invalid pipeline wiring. Raised at construction time. Examples:</p> <ul> <li>More than one <code>async_boundary = True</code> step in the same pipeline</li> <li>An <code>async_boundary = True</code> step inside a <code>Branch</code> child</li> </ul>"},{"location":"pipeline/api-reference/#brancherror","title":"<code>BranchError</code>","text":"<pre><code>class BranchError(Exception):\n    failures: list[BaseException]\n</code></pre> <p>One or more branch pipelines failed. All branches always run to completion before this is raised. Raised at runtime.</p> Attribute Type Description <code>failures</code> <code>list[BaseException]</code> One exception per failed branch"},{"location":"pipeline/api-reference/#step-class-attributes","title":"Step class attributes","text":"<p>Optional attributes a step class can declare to control pipeline behavior:</p> Attribute Type Default Description <code>requires</code> <code>set[str] \\| frozenset[str]</code> (required) Metadata keys the step reads <code>provides</code> <code>set[str] \\| frozenset[str]</code> (required) Metadata keys the step writes <code>async_boundary</code> <code>bool</code> <code>False</code> Marks the foreground/background split point <code>max_workers</code> <code>int</code> <code>1</code> Max concurrent background threads for this step class"},{"location":"pipeline/branching/","title":"Branching &amp; Parallelism","text":"<p>A <code>Branch</code> runs multiple pipelines in parallel on the same input, then merges their outputs before the next step. It is itself a step \u2014 it satisfies <code>StepProtocol</code> and can be used anywhere a step is expected.</p>"},{"location":"pipeline/branching/#what-is-a-branch","title":"What is a Branch?","text":"<pre><code>graph LR\n    T[Tokenize] --&gt; U[Uppercase]\n    T --&gt; R[Reverse]\n    U --&gt; S[Summarize]\n    R --&gt; S</code></pre> <p>The branch forks the context to both child pipelines, runs them in parallel, and joins (merges) the results before <code>Summarize</code> runs. The join is implicit \u2014 any step after a <code>Branch</code> waits for all branches to complete.</p>"},{"location":"pipeline/branching/#creating-branches","title":"Creating branches","text":"<p>Two equivalent APIs:</p> Fluent shorthandExplicit Branch <pre><code>pipe = (\n    Pipeline()\n    .then(Tokenize())\n    .branch(\n        Pipeline().then(Uppercase()),\n        Pipeline().then(Reverse()),\n        merge=MergeStrategy.RAISE_ON_CONFLICT,\n    )\n    .then(Summarize())\n)\n</code></pre> <pre><code>from pipeline import Branch, MergeStrategy\n\nbranch_step = Branch(\n    Pipeline().then(Uppercase()),\n    Pipeline().then(Reverse()),\n    merge=MergeStrategy.RAISE_ON_CONFLICT,\n)\n\npipe = (\n    Pipeline()\n    .then(Tokenize())\n    .then(branch_step)\n    .then(Summarize())\n)\n</code></pre>"},{"location":"pipeline/branching/#merge-strategies","title":"Merge strategies","text":"<p>When all branches complete, their output contexts must be merged back into one. The <code>merge</code> parameter controls how conflicts are resolved.</p>"},{"location":"pipeline/branching/#raise_on_conflict-default","title":"<code>RAISE_ON_CONFLICT</code> (default)","text":"<p>Raises <code>ValueError</code> if two branches write different values to the same named field. Disjoint fields pass through without conflict.</p> <pre><code>pipe = (\n    Pipeline()\n    .then(Tokenize())\n    .branch(\n        Pipeline().then(Uppercase()),    # provides: {\"upper_tokens\"}\n        Pipeline().then(Reverse()),      # provides: {\"reversed_tokens\"}\n        merge=MergeStrategy.RAISE_ON_CONFLICT,\n    )\n)\n\n# Works \u2014 branches write to different fields\nresults = pipe.run([StepContext(sample=\"hello world\")])\n</code></pre> <p>Tip</p> <p>In practice, branches that write disjoint fields (which is the common case) never conflict and the merge is a no-op.</p>"},{"location":"pipeline/branching/#last_write_wins","title":"<code>LAST_WRITE_WINS</code>","text":"<p>The last branch's value wins for every conflicting field. Simple but lossy.</p> <pre><code>pipe = Pipeline().then(Tokenize()).branch(\n    Pipeline().then(Uppercase()),\n    Pipeline().then(Reverse()),\n    merge=MergeStrategy.LAST_WRITE_WINS,\n)\n</code></pre>"},{"location":"pipeline/branching/#namespaced","title":"<code>NAMESPACED</code>","text":"<p>Each branch's output is stored at <code>metadata[\"branch_0\"]</code>, <code>metadata[\"branch_1\"]</code>, etc. No conflict is possible.</p> <pre><code>pipe = Pipeline().then(Tokenize()).branch(\n    Pipeline().then(Uppercase()),\n    Pipeline().then(Reverse()),\n    merge=MergeStrategy.NAMESPACED,\n)\n\nresults = pipe.run([StepContext(sample=\"hello world\")])\nmeta = results[0].output.metadata\nprint(meta[\"branch_0\"])  # context from Uppercase branch\nprint(meta[\"branch_1\"])  # context from Reverse branch\n</code></pre>"},{"location":"pipeline/branching/#custom-merge-function","title":"Custom merge function","text":"<p>For full control, pass a callable:</p> <pre><code>def priority_merge(ctxs: list[StepContext]) -&gt; StepContext:\n    \"\"\"First branch wins for all fields.\"\"\"\n    base = ctxs[0]\n    merged_meta = dict(base.metadata)\n    for ctx in ctxs[1:]:\n        for k, v in ctx.metadata.items():\n            merged_meta.setdefault(k, v)  # first writer wins\n    return base.replace(metadata=MappingProxyType(merged_meta))\n\npipe = Pipeline().then(Tokenize()).branch(\n    Pipeline().then(Uppercase()),\n    Pipeline().then(Reverse()),\n    merge=priority_merge,\n)\n</code></pre>"},{"location":"pipeline/branching/#how-context-flows-through-branches","title":"How context flows through branches","text":"<ol> <li>All branches receive the same frozen context \u2014 no copy needed since <code>StepContext</code> is immutable</li> <li>Each branch runs its pipeline and returns a new context</li> <li>The merge function receives the list of output contexts and returns a single merged context</li> <li>The merged context is passed to the next step in the outer pipeline</li> </ol> <p>Immutability is what makes this safe. No branch can corrupt another branch's input.</p>"},{"location":"pipeline/branching/#execution-model","title":"Execution model","text":"SyncAsync <p>Branches run in a <code>ThreadPoolExecutor</code> with <code>max_workers=len(pipelines)</code>:</p> <pre><code># All branches get their own thread\nwith ThreadPoolExecutor(max_workers=len(self.pipelines)) as executor:\n    futures = [executor.submit(p, ctx) for p in self.pipelines]\n    results = [f.result() for f in futures]\nreturn self._merge_fn(results)\n</code></pre> <p>Branches run via <code>asyncio.gather</code>. Sync child pipelines are wrapped with <code>asyncio.to_thread</code>:</p> <pre><code>results = await asyncio.gather(\n    *[run_child(p) for p in self.pipelines],\n    return_exceptions=True,\n)\nreturn self._merge_fn(results)\n</code></pre> <p>In both cases, all branches run to completion even if one fails.</p>"},{"location":"pipeline/branching/#error-handling-in-branches","title":"Error handling in branches","text":"<p>When one or more branches fail, a <code>BranchError</code> is raised \u2014 but only after all branches have completed:</p> <pre><code>from pipeline.errors import BranchError\n\ntry:\n    results = pipe.run(contexts)\nexcept BranchError as e:\n    print(f\"{len(e.failures)} branch(es) failed\")\n    for failure in e.failures:\n        print(f\"  - {type(failure).__name__}: {failure}\")\n</code></pre> <ul> <li><code>BranchError.failures</code> contains one exception per failed branch</li> <li>Successful branches are not lost \u2014 their contexts are still available</li> <li><code>SampleResult.failed_at</code> is set to <code>\"Branch\"</code> and <code>SampleResult.cause</code> carries the inner exception</li> </ul> <p>See Error Handling for the full error model.</p>"},{"location":"pipeline/branching/#contract-inference","title":"Contract inference","text":"<p>A <code>Branch</code> computes its own <code>requires</code> and <code>provides</code> from the union of its children:</p> <pre><code>branch = Branch(\n    Pipeline().then(Uppercase()),    # requires: {\"tokens\"}, provides: {\"upper_tokens\"}\n    Pipeline().then(Reverse()),      # requires: {\"tokens\"}, provides: {\"reversed_tokens\"}\n)\n\n# Inferred:\n# branch.requires = {\"tokens\"}\n# branch.provides = {\"upper_tokens\", \"reversed_tokens\"}\n</code></pre> <p>The outer pipeline validates against these aggregated contracts at construction time, so nesting branches inside pipelines works seamlessly.</p>"},{"location":"pipeline/core-concepts/","title":"Core Concepts","text":"<p>The pipeline engine is built on four foundational concepts: the Step protocol, the StepContext, the contract system, and the Pipeline compositor. Understanding these gives you the mental model for everything else.</p>"},{"location":"pipeline/core-concepts/#stepprotocol","title":"StepProtocol","text":"<p>A Step is any Python object that satisfies the <code>StepProtocol</code> \u2014 a structural (duck-typed) interface. No base class is required.</p> <pre><code>from collections.abc import Set as AbstractSet\nfrom typing import Protocol, runtime_checkable\n\n@runtime_checkable\nclass StepProtocol(Protocol):\n    requires: AbstractSet[str]   # metadata keys this step reads\n    provides: AbstractSet[str]   # metadata keys this step writes\n\n    def __call__(self, ctx: StepContext) -&gt; StepContext: ...\n</code></pre> <p>Any object with <code>requires</code>, <code>provides</code>, and a <code>__call__</code> method is a valid step:</p> <pre><code>class Tokenize:\n    requires = frozenset()                     # no dependencies\n    provides = frozenset({\"tokens\", \"word_count\"})\n\n    def __call__(self, ctx: StepContext) -&gt; StepContext:\n        tokens = str(ctx.sample).split()\n        return ctx.replace(\n            metadata=MappingProxyType({\n                **ctx.metadata,\n                \"tokens\": tokens,\n                \"word_count\": len(tokens),\n            })\n        )\n</code></pre> <p>Key details:</p> <ul> <li><code>AbstractSet[str]</code> accepts both <code>set</code> and <code>frozenset</code>. Steps can use plain set literals \u2014 the pipeline normalizes them to <code>frozenset</code> at construction time.</li> <li><code>@runtime_checkable</code> lets the pipeline use <code>isinstance(step, StepProtocol)</code> at construction time to catch missing attributes early, rather than failing at call time.</li> <li><code>Pipeline</code> and <code>Branch</code> both satisfy this protocol, so they can be nested wherever a step is expected.</li> </ul>"},{"location":"pipeline/core-concepts/#stepcontext","title":"StepContext","text":"<p><code>StepContext</code> is the data carrier passed from step to step. It is a frozen dataclass \u2014 steps never mutate the incoming context.</p> <pre><code>from dataclasses import dataclass, field\nfrom types import MappingProxyType\n\n@dataclass(frozen=True)\nclass StepContext:\n    sample: Any = None\n    metadata: MappingProxyType = field(\n        default_factory=lambda: MappingProxyType({})\n    )\n\n    def replace(self, **changes) -&gt; \"StepContext\":\n        return dataclasses.replace(self, **changes)\n</code></pre> <p>The engine only reads <code>sample</code> and <code>metadata</code>. All domain-specific fields are added by subclassing.</p>"},{"location":"pipeline/core-concepts/#the-replace-pattern","title":"The <code>.replace()</code> pattern","text":"<p>Steps create new contexts \u2014 they never mutate the incoming one:</p> <pre><code>def __call__(self, ctx: StepContext) -&gt; StepContext:\n    result = process(ctx.sample)\n    return ctx.replace(\n        metadata=MappingProxyType({**ctx.metadata, \"result\": result})\n    )\n</code></pre> <p>This is the only way to \"modify\" a context. <code>frozen=True</code> makes mutation a hard error at runtime rather than a subtle bug.</p>"},{"location":"pipeline/core-concepts/#metadata-auto-coercion","title":"Metadata auto-coercion","text":"<p>If a caller passes a plain <code>dict</code> as metadata, <code>StepContext.__post_init__</code> automatically wraps it in <code>MappingProxyType</code>, ensuring mutation is always a runtime error:</p> <pre><code># Both of these produce identical immutable metadata:\nctx = StepContext(sample=\"hello\", metadata={\"key\": \"value\"})\nctx = StepContext(sample=\"hello\", metadata=MappingProxyType({\"key\": \"value\"}))\n</code></pre>"},{"location":"pipeline/core-concepts/#subclassing-for-domain-fields","title":"Subclassing for domain fields","text":"<p>Applications subclass <code>StepContext</code> to add named fields for concepts shared across their pipelines:</p> <pre><code>@dataclass(frozen=True)\nclass MLContext(StepContext):\n    # Shared configuration\n    model_config: dict | None = None\n\n    # Produced by steps (None until the providing step runs)\n    predictions: list | None = None\n    scores: dict | None = None\n    report: str | None = None\n</code></pre> <p>Use named fields for data shared across multiple steps in the pipeline. Use <code>metadata</code> for integration-specific or step-specific transient data that doesn't warrant a dedicated field.</p> <p>When to use which</p> <ul> <li>Named field: <code>predictions</code>, <code>scores</code> \u2014 shared by multiple steps, type-checkable</li> <li>Metadata: <code>metadata[\"debug_log\"]</code>, <code>metadata[\"cache_key\"]</code> \u2014 step-specific, doesn't pollute the class</li> </ul>"},{"location":"pipeline/core-concepts/#why-immutability","title":"Why immutability?","text":"<ul> <li>Branch safety \u2014 All branches receive the same frozen context. No deep copy is needed since no branch can mutate what it receives.</li> <li>Thread safety \u2014 Steps running concurrently (via <code>workers</code> or <code>Branch</code>) can safely share context objects.</li> <li>Debugging \u2014 Each step returns a new context, creating a clear trace of data transformations.</li> </ul>"},{"location":"pipeline/core-concepts/#contracts-requires-and-provides","title":"Contracts: requires and provides","text":"<p>Every step declares:</p> <ul> <li><code>requires</code> \u2014 the set of field names it reads from the context</li> <li><code>provides</code> \u2014 the set of field names it writes to the context</li> </ul> <p>The pipeline validates these at construction time.</p>"},{"location":"pipeline/core-concepts/#how-validation-works","title":"How validation works","text":"<p>When you build a pipeline with <code>.then()</code>, the engine checks step ordering immediately:</p> <pre><code>pipe = (\n    Pipeline()\n    .then(Tokenize())      # provides: {\"tokens\", \"word_count\"}\n    .then(Uppercase())     # requires: {\"tokens\"} \u2713 \u2014 Tokenize provides it\n)\n</code></pre> <p>If a step requires a field that a later step provides, the pipeline raises <code>PipelineOrderError</code>:</p> <pre><code># This raises PipelineOrderError at construction time:\npipe = Pipeline().then(Uppercase()).then(Tokenize())\n# \u2191 Uppercase requires \"tokens\", but Tokenize (which provides it) comes after\n</code></pre>"},{"location":"pipeline/core-concepts/#external-inputs-vs-internal-dependencies","title":"External inputs vs internal dependencies","text":"<p>Fields not produced by any step in the pipeline are treated as external inputs \u2014 they must be present in the initial <code>StepContext</code> passed to <code>run()</code>. These do not trigger ordering errors:</p> <pre><code>class ScoreStep:\n    requires = frozenset({\"predictions\"})  # external input\n    provides = frozenset({\"scores\"})\n\n# No error \u2014 \"predictions\" is expected to come from the initial context\npipe = Pipeline().then(ScoreStep())\n</code></pre>"},{"location":"pipeline/core-concepts/#contract-inference-for-nested-pipelines","title":"Contract inference for nested pipelines","text":"<p>When a <code>Pipeline</code> is used as a step inside another pipeline, its <code>requires</code> and <code>provides</code> are computed automatically from its inner steps:</p> <pre><code>inner = Pipeline().then(Tokenize()).then(Uppercase())\n\n# Inferred automatically:\n# inner.requires = frozenset()              \u2014 Tokenize needs nothing external\n# inner.provides = frozenset({\"tokens\", \"word_count\", \"upper_tokens\"})\n\nouter = Pipeline().then(inner).then(Summarize())\n# Summarize's requirements validated against inner.provides\n</code></pre> <p>The inference algorithm:</p> <ol> <li>Walk steps in order, tracking what has been provided so far</li> <li><code>requires</code> = fields needed by steps that no earlier step provides (external dependencies)</li> <li><code>provides</code> = union of all fields any step writes</li> </ol> <p>All Branch children always run</p> <p>The contract system assumes all <code>Branch</code> children execute. There is no concept of conditional branches where only some children run \u2014 all branches always run. If a branch provides a field that a later step requires, validation passes; if that branch were to not run, the pipeline would fail at runtime.</p>"},{"location":"pipeline/core-concepts/#pipeline","title":"Pipeline","text":"<p>A <code>Pipeline</code> is an ordered list of steps that runs sequentially for a single input. It satisfies the <code>StepProtocol</code>, so it can be nested inside other pipelines.</p>"},{"location":"pipeline/core-concepts/#building-a-pipeline","title":"Building a pipeline","text":"<p>Two equivalent forms:</p> Fluent builder (preferred)Constructor list <pre><code>pipe = (\n    Pipeline()\n    .then(Tokenize())\n    .then(Uppercase())\n    .then(Summarize())\n)\n</code></pre> <pre><code>pipe = Pipeline([\n    Tokenize(),\n    Uppercase(),\n    Summarize(),\n])\n</code></pre> <p>Both validate step ordering at construction time. The fluent builder validates after each <code>.then()</code> call, giving precise error messages about which step caused the violation.</p>"},{"location":"pipeline/core-concepts/#the-branch-shorthand","title":"The <code>.branch()</code> shorthand","text":"<p>Instead of manually creating a <code>Branch</code>, use the fluent shorthand:</p> <pre><code>pipe = (\n    Pipeline()\n    .then(Tokenize())\n    .branch(\n        Pipeline().then(Uppercase()),\n        Pipeline().then(Reverse()),\n        merge=MergeStrategy.RAISE_ON_CONFLICT,\n    )\n    .then(Summarize())\n)\n</code></pre> <p>This is equivalent to <code>.then(Branch(...))</code>.</p>"},{"location":"pipeline/core-concepts/#nesting","title":"Nesting","text":"<p>A pipeline used as a step is a black box \u2014 the outer pipeline sees only its aggregated <code>requires</code> and <code>provides</code>:</p> <pre><code>preprocessing = Pipeline().then(Tokenize()).then(Uppercase())\npostprocessing = Pipeline().then(Summarize()).then(FormatStep())\n\nfull = Pipeline().then(preprocessing).then(postprocessing)\n</code></pre> <p>Inner pipeline as a fan-out step</p> <p>A step receives one context and must return one context \u2014 but nothing prevents it from internally expanding to multiple sub-inputs:</p> <pre><code>class MultiSearchStep:\n    requires = frozenset()\n    provides = frozenset({\"search_results\"})\n\n    def __call__(self, ctx: StepContext) -&gt; StepContext:\n        queries = generate_queries(ctx.sample)\n        sub_ctxs = [StepContext(sample=q) for q in queries]\n        sub_pipe = Pipeline().then(FetchStep())\n        results = sub_pipe.run(sub_ctxs, workers=len(queries))\n        merged = merge_results(results)\n        return ctx.replace(\n            metadata=MappingProxyType({**ctx.metadata, \"search_results\": merged})\n        )\n</code></pre> <p>From the outer pipeline's perspective, <code>MultiSearchStep</code> is a single step. The fan-out is an internal implementation detail.</p>"},{"location":"pipeline/custom-steps/","title":"Building Custom Steps","text":"<p>This guide covers everything you need to create your own pipeline steps \u2014 from the minimal contract to advanced patterns like dependency injection, async execution, and testing.</p>"},{"location":"pipeline/custom-steps/#the-step-contract","title":"The step contract","text":"<p>Any Python object with <code>requires</code>, <code>provides</code>, and <code>__call__</code> is a valid step. No base class needed.</p> <pre><code>from types import MappingProxyType\nfrom pipeline import StepContext\n\n\nclass MyStep:\n    requires = frozenset({\"input_field\"})\n    provides = frozenset({\"output_field\"})\n\n    def __call__(self, ctx: StepContext) -&gt; StepContext:\n        result = process(ctx.metadata[\"input_field\"])\n        return ctx.replace(\n            metadata=MappingProxyType({**ctx.metadata, \"output_field\": result})\n        )\n</code></pre> <p>Rules:</p> <ul> <li><code>requires</code> and <code>provides</code> can be <code>set</code> or <code>frozenset</code> \u2014 the pipeline normalizes to <code>frozenset</code></li> <li><code>__call__</code> receives a <code>StepContext</code> and must return a <code>StepContext</code></li> <li>Never mutate the incoming context \u2014 always use <code>.replace()</code></li> </ul>"},{"location":"pipeline/custom-steps/#sync-vs-async-steps","title":"Sync vs async steps","text":"SyncAsync <pre><code>class ComputeStep:\n    requires = frozenset({\"data\"})\n    provides = frozenset({\"result\"})\n\n    def __call__(self, ctx: StepContext) -&gt; StepContext:\n        result = expensive_computation(ctx.metadata[\"data\"])\n        return ctx.replace(\n            metadata=MappingProxyType({**ctx.metadata, \"result\": result})\n        )\n</code></pre> <pre><code>class FetchStep:\n    requires = frozenset({\"url\"})\n    provides = frozenset({\"response\"})\n\n    async def __call__(self, ctx: StepContext) -&gt; StepContext:\n        async with aiohttp.ClientSession() as session:\n            resp = await session.get(ctx.metadata[\"url\"])\n            data = await resp.json()\n        return ctx.replace(\n            metadata=MappingProxyType({**ctx.metadata, \"response\": data})\n        )\n</code></pre> <p>Use async steps for I/O-bound work (HTTP requests, API calls, file I/O). The pipeline detects and handles both transparently.</p>"},{"location":"pipeline/custom-steps/#dependency-injection","title":"Dependency injection","text":"<p>Steps that need external collaborators receive them via <code>__init__</code>. The <code>__call__</code> method stays stateless \u2014 it only uses <code>self.*</code> for injected dependencies and <code>ctx</code> for data.</p> <pre><code>class ScoringStep:\n    requires = frozenset({\"predictions\"})\n    provides = frozenset({\"scores\"})\n\n    def __init__(self, scorer, threshold: float = 0.5):\n        self.scorer = scorer\n        self.threshold = threshold\n\n    def __call__(self, ctx: StepContext) -&gt; StepContext:\n        raw_scores = self.scorer.evaluate(ctx.metadata[\"predictions\"])\n        filtered = {k: v for k, v in raw_scores.items() if v &gt;= self.threshold}\n        return ctx.replace(\n            metadata=MappingProxyType({**ctx.metadata, \"scores\": filtered})\n        )\n</code></pre> <p>This makes testing easy \u2014 inject mocks:</p> <pre><code>pipe = Pipeline().then(ScoringStep(scorer=mock_scorer, threshold=0.8))\n</code></pre>"},{"location":"pipeline/custom-steps/#declaring-concurrency","title":"Declaring concurrency","text":"<p>Two optional class attributes control how a step participates in concurrent execution:</p>"},{"location":"pipeline/custom-steps/#async_boundary","title":"<code>async_boundary</code>","text":"<p>Marks the foreground/background split point. Everything from this step onward runs in a background thread:</p> <pre><code>class AnalyzeStep:\n    requires = frozenset({\"data\"})\n    provides = frozenset({\"analysis\"})\n    async_boundary = True  # background from here\n\n    def __call__(self, ctx: StepContext) -&gt; StepContext: ...\n</code></pre> <p>See Execution Model \u2014 Async Boundary for details.</p>"},{"location":"pipeline/custom-steps/#max_workers","title":"<code>max_workers</code>","text":"<p>Controls the per-step-class thread pool size for background execution:</p> <pre><code>class ParallelAnalyzeStep:\n    requires = frozenset({\"data\"})\n    provides = frozenset({\"analysis\"})\n    async_boundary = True\n    max_workers = 4  # up to 4 concurrent analyses\n\n    def __call__(self, ctx: StepContext) -&gt; StepContext: ...\n</code></pre> <p>Default is <code>max_workers = 1</code> (serialized).</p> <p>Warning</p> <p>Steps that write shared state (e.g. updating an external database or accumulating results into a shared object) must use <code>max_workers = 1</code> to avoid race conditions.</p>"},{"location":"pipeline/custom-steps/#subclassing-stepcontext","title":"Subclassing StepContext","text":"<p>When <code>metadata</code> becomes unwieldy, subclass <code>StepContext</code> to add named fields:</p> <pre><code>from dataclasses import dataclass\n\n@dataclass(frozen=True)\nclass MLContext(StepContext):\n    predictions: list | None = None\n    scores: dict | None = None\n    report: str | None = None\n</code></pre> <p>Steps write to named fields using <code>.replace()</code>:</p> <pre><code>class PredictStep:\n    requires = frozenset()\n    provides = frozenset({\"predictions\"})\n\n    def __init__(self, model):\n        self.model = model\n\n    def __call__(self, ctx: MLContext) -&gt; MLContext:\n        preds = self.model.predict(ctx.sample)\n        return ctx.replace(predictions=preds)\n</code></pre> <p>When to subclass</p> <ul> <li>Named fields: Data shared across multiple steps that benefits from type checking</li> <li>Metadata: Step-specific or integration-specific transient data (e.g. <code>metadata[\"cache_key\"]</code>)</li> </ul> <p>The <code>requires</code>/<code>provides</code> validation works on attribute names, so it's subclass-agnostic. A step declaring <code>requires = {\"predictions\"}</code> works with any context subclass that has a <code>predictions</code> attribute.</p>"},{"location":"pipeline/custom-steps/#testing-steps","title":"Testing steps","text":""},{"location":"pipeline/custom-steps/#unit-test-step-in-isolation","title":"Unit test \u2014 step in isolation","text":"<pre><code>from types import MappingProxyType\nfrom pipeline import StepContext\n\n\ndef test_tokenize_splits_words():\n    step = Tokenize()\n    ctx = StepContext(sample=\"hello world\")\n\n    result = step(ctx)\n\n    assert result.metadata[\"tokens\"] == [\"hello\", \"world\"]\n    assert result.metadata[\"word_count\"] == 2\n\n\ndef test_uppercase_transforms_tokens():\n    step = Uppercase()\n    ctx = StepContext(\n        metadata=MappingProxyType({\"tokens\": [\"hello\", \"world\"]})\n    )\n\n    result = step(ctx)\n\n    assert result.metadata[\"upper_tokens\"] == [\"HELLO\", \"WORLD\"]\n</code></pre>"},{"location":"pipeline/custom-steps/#protocol-compliance","title":"Protocol compliance","text":"<pre><code>from pipeline import StepProtocol\n\n\ndef test_step_satisfies_protocol():\n    step = Tokenize()\n    assert isinstance(step, StepProtocol)\n    assert hasattr(step, \"requires\")\n    assert hasattr(step, \"provides\")\n    assert callable(step)\n</code></pre>"},{"location":"pipeline/custom-steps/#pipeline-integration-test","title":"Pipeline integration test","text":"<pre><code>from pipeline import Pipeline, StepContext\n\n\ndef test_full_pipeline():\n    pipe = Pipeline().then(Tokenize()).then(Uppercase())\n    results = pipe.run([StepContext(sample=\"hello world\")])\n\n    assert len(results) == 1\n    assert results[0].error is None\n    assert results[0].output.metadata[\"upper_tokens\"] == [\"HELLO\", \"WORLD\"]\n</code></pre>"},{"location":"pipeline/custom-steps/#common-patterns","title":"Common patterns","text":""},{"location":"pipeline/custom-steps/#map-reduce-step","title":"Map-reduce step","text":"<p>A step that internally fans out to multiple sub-inputs:</p> <pre><code>class MultiSearchStep:\n    requires = frozenset()\n    provides = frozenset({\"search_results\"})\n\n    def __call__(self, ctx: StepContext) -&gt; StepContext:\n        queries = generate_queries(ctx.sample)                   # 1 \u2192 N\n        sub_ctxs = [StepContext(sample=q) for q in queries]\n        sub_pipe = Pipeline().then(FetchStep())\n        results = sub_pipe.run(sub_ctxs, workers=len(queries))  # parallel\n        merged = merge_results(results)                          # N \u2192 1\n        return ctx.replace(\n            metadata=MappingProxyType({**ctx.metadata, \"search_results\": merged})\n        )\n</code></pre> <p>From the outer pipeline's perspective, this is a black box that takes one context and returns one.</p>"},{"location":"pipeline/custom-steps/#logging-observability-step","title":"Logging / observability step","text":"<p>A pass-through step that logs without modifying data:</p> <pre><code>class LogStep:\n    requires = frozenset()\n    provides = frozenset()\n\n    def __init__(self, logger):\n        self.logger = logger\n\n    def __call__(self, ctx: StepContext) -&gt; StepContext:\n        self.logger.info(f\"Processing sample: {ctx.sample}\")\n        self.logger.debug(f\"Metadata keys: {list(ctx.metadata.keys())}\")\n        return ctx  # pass through unchanged\n</code></pre>"},{"location":"pipeline/custom-steps/#retry-wrapper","title":"Retry wrapper","text":"<p>A step that wraps another step with retry logic:</p> <pre><code>import time\n\n\nclass RetryStep:\n    def __init__(self, inner, max_retries: int = 3, delay: float = 1.0):\n        self.inner = inner\n        self.max_retries = max_retries\n        self.delay = delay\n        self.requires = inner.requires\n        self.provides = inner.provides\n\n    def __call__(self, ctx: StepContext) -&gt; StepContext:\n        for attempt in range(self.max_retries):\n            try:\n                return self.inner(ctx)\n            except Exception:\n                if attempt == self.max_retries - 1:\n                    raise\n                time.sleep(self.delay * (attempt + 1))\n</code></pre> <p>Usage:</p> <pre><code>pipe = Pipeline().then(RetryStep(FlakyAPIStep(), max_retries=3))\n</code></pre>"},{"location":"pipeline/error-handling/","title":"Error Handling","text":"<p>The pipeline engine guarantees that every sample produces a <code>SampleResult</code> \u2014 nothing is dropped silently. One failing sample never blocks others. Retry logic is the responsibility of individual steps, not the pipeline.</p>"},{"location":"pipeline/error-handling/#sampleresult","title":"SampleResult","text":"<p>Every sample that enters <code>run()</code> produces exactly one <code>SampleResult</code>:</p> <pre><code>@dataclass\nclass SampleResult:\n    sample: Any                      # the original input\n    output: StepContext | None       # final context (None if failed)\n    error: Exception | None          # the exception (None if succeeded)\n    failed_at: str | None            # step class name where error occurred\n    cause: Exception | None = None   # inner exception for BranchError\n</code></pre> Field On success On failure <code>sample</code> original input original input <code>output</code> final <code>StepContext</code> <code>None</code> <code>error</code> <code>None</code> the exception <code>failed_at</code> <code>None</code> class name of the failing step (e.g. <code>\"Tokenize\"</code>) <code>cause</code> <code>None</code> inner exception when <code>failed_at == \"Branch\"</code> <p>Background steps</p> <p>For steps after an <code>async_boundary</code>, <code>output</code> and <code>error</code> may still be <code>None</code> when <code>run()</code> returns. Call <code>pipe.wait_for_background()</code> to block until all background work completes and results are finalized.</p>"},{"location":"pipeline/error-handling/#construction-time-errors","title":"Construction-time errors","text":"<p>These are caught before any data flows \u2014 they surface immediately when you build the pipeline.</p>"},{"location":"pipeline/error-handling/#pipelineordererror","title":"PipelineOrderError","text":"<p>Raised when a step requires a field that is produced by a later step in the pipeline:</p> <pre><code>from pipeline import Pipeline\nfrom pipeline.errors import PipelineOrderError\n\nclass Uppercase:\n    requires = frozenset({\"tokens\"})\n    provides = frozenset({\"upper_tokens\"})\n    def __call__(self, ctx): ...\n\nclass Tokenize:\n    requires = frozenset()\n    provides = frozenset({\"tokens\"})\n    def __call__(self, ctx): ...\n\ntry:\n    Pipeline().then(Uppercase()).then(Tokenize())\nexcept PipelineOrderError as e:\n    print(e)\n    # Uppercase requires {\"tokens\"} but it is provided by a later step\n</code></pre> <p>Tip</p> <p><code>PipelineOrderError</code> is always a bug \u2014 reorder your steps. Fields not produced by any step in the pipeline are treated as external inputs and do not trigger this error.</p>"},{"location":"pipeline/error-handling/#pipelineconfigerror","title":"PipelineConfigError","text":"<p>Raised for invalid pipeline wiring:</p> <p>Multiple async boundaries:</p> <pre><code>from pipeline.errors import PipelineConfigError\n\nclass StepA:\n    requires = frozenset()\n    provides = frozenset({\"a\"})\n    async_boundary = True\n    def __call__(self, ctx): ...\n\nclass StepB:\n    requires = frozenset({\"a\"})\n    provides = frozenset({\"b\"})\n    async_boundary = True\n    def __call__(self, ctx): ...\n\ntry:\n    Pipeline().then(StepA()).then(StepB())\nexcept PipelineConfigError:\n    print(\"Only one async_boundary per pipeline is allowed\")\n</code></pre> <p>Async boundary inside a Branch child:</p> <pre><code>try:\n    Pipeline().branch(\n        Pipeline().then(StepA()),  # async_boundary = True inside branch\n    )\nexcept PipelineConfigError:\n    print(\"async_boundary inside Branch children is not allowed\")\n</code></pre>"},{"location":"pipeline/error-handling/#runtime-errors","title":"Runtime errors","text":""},{"location":"pipeline/error-handling/#foreground-failures","title":"Foreground failures","text":"<p>When a step before the <code>async_boundary</code> (or in a pipeline with no boundary) raises an exception, the pipeline catches it per-sample and records it in the <code>SampleResult</code>:</p> <pre><code>class Boom:\n    requires = frozenset()\n    provides = frozenset()\n\n    def __call__(self, ctx):\n        raise RuntimeError(f\"Failed on {ctx.sample!r}\")\n\npipe = Pipeline().then(Tokenize()).then(Boom())\n\nresults = pipe.run([\n    StepContext(sample=\"good\"),\n    StepContext(sample=\"also good\"),\n])\n\nfor r in results:\n    if r.error:\n        print(f\"Sample '{r.sample}' failed at {r.failed_at}: {r.error}\")\n    else:\n        print(f\"Sample '{r.sample}' succeeded\")\n</code></pre> <pre><code>Sample 'good' failed at Boom: Failed on 'good'\nSample 'also good' failed at Boom: Failed on 'also good'\n</code></pre> <p>Each sample is processed independently \u2014 one failure does not prevent others from running.</p>"},{"location":"pipeline/error-handling/#background-failures","title":"Background failures","text":"<p>When a step after the <code>async_boundary</code> raises, the caller has already moved on. The exception is captured and attached to the <code>SampleResult</code> in-place:</p> <pre><code>pipe = Pipeline().then(Tokenize()).then(BrokenBackgroundStep())\n\nresults = pipe.run(samples)\n# results returned immediately \u2014 background still running\n\npipe.wait_for_background(timeout=10.0)\n\n# Now check for background failures\nfor r in results:\n    if r.error:\n        print(f\"Background failure at {r.failed_at}: {r.error}\")\n</code></pre>"},{"location":"pipeline/error-handling/#brancherror","title":"BranchError","text":"<p>When one or more branch pipelines fail, a <code>BranchError</code> is raised with the full list of failures:</p> <pre><code>from pipeline.errors import BranchError\n\nresults = pipe.run(contexts)\n\nfor r in results:\n    if isinstance(r.error, BranchError):\n        print(f\"{len(r.error.failures)} branch(es) failed:\")\n        for f in r.error.failures:\n            print(f\"  {type(f).__name__}: {f}\")\n    elif r.error:\n        print(f\"Step failure at {r.failed_at}: {r.error}\")\n</code></pre> <p>All branches run to completion before <code>BranchError</code> is raised \u2014 no branch is cancelled when another fails. The <code>SampleResult.cause</code> field carries the inner exception from the failing branch.</p>"},{"location":"pipeline/error-handling/#inspecting-results","title":"Inspecting results","text":"<p>The standard pattern after <code>run()</code>:</p> <pre><code>results = pipe.run(contexts)\npipe.wait_for_background()  # if using async_boundary\n\nsucceeded = [r for r in results if r.error is None]\nfailed = [r for r in results if r.error is not None]\n\nprint(f\"{len(succeeded)} succeeded, {len(failed)} failed\")\n\nfor r in failed:\n    print(f\"  Sample: {r.sample}\")\n    print(f\"  Failed at: {r.failed_at}\")\n    print(f\"  Error: {r.error}\")\n</code></pre>"},{"location":"pipeline/error-handling/#background-monitoring","title":"Background monitoring","text":""},{"location":"pipeline/error-handling/#wait_for_background","title":"<code>wait_for_background()</code>","text":"<p>Blocks until all background tasks complete:</p> <pre><code># Wait indefinitely\npipe.wait_for_background()\n\n# Wait with timeout \u2014 raises TimeoutError if not done\npipe.wait_for_background(timeout=30.0)\n</code></pre> <p>Completed threads are removed from the tracking list after this call.</p>"},{"location":"pipeline/error-handling/#background_stats","title":"<code>background_stats()</code>","text":"<p>Returns a snapshot of background task progress. Thread-safe \u2014 can be called from any thread while the pipeline is running:</p> <pre><code>stats = pipe.background_stats()\nprint(stats)\n# {'active': 2, 'completed': 8}\n</code></pre>"},{"location":"pipeline/execution/","title":"Execution Model","text":"<p>\"Async\" means three different things in this framework. They operate at different levels and solve different problems. Keeping them separate is key to understanding the concurrency model.</p>"},{"location":"pipeline/execution/#three-types-of-concurrency","title":"Three types of concurrency","text":"Type Level Problem it solves Async steps single step Don't block the thread during I/O <code>async_boundary</code> across samples Start the next sample before the current one finishes Branch parallelism within one sample Run independent work simultaneously on the same data <p>Each mechanism is independent. They compose freely \u2014 you can have async steps inside branches, behind an <code>async_boundary</code>, run with multiple workers.</p>"},{"location":"pipeline/execution/#entry-points-run-and-run_async","title":"Entry points: <code>run()</code> and <code>run_async()</code>","text":"SyncAsync <pre><code># For regular (non-async) callers\nresults = pipe.run(contexts, workers=4)\n</code></pre> <pre><code># For async callers (e.g. inside an async framework)\nresults = await pipe.run_async(contexts, workers=4)\n</code></pre> <p><code>run()</code> is a thin wrapper that calls <code>asyncio.run(self.run_async(...))</code>. Both accept the same parameters and return <code>list[SampleResult]</code>.</p>"},{"location":"pipeline/execution/#workers-sample-level-parallelism","title":"Workers: sample-level parallelism","text":"<p>The <code>workers</code> parameter on <code>run()</code> / <code>run_async()</code> controls how many samples are processed through foreground steps simultaneously:</p> <pre><code>import time\nfrom pipeline import Pipeline, StepContext\n\n\nclass SlowStep:\n    requires = frozenset()\n    provides = frozenset({\"result\"})\n\n    def __call__(self, ctx: StepContext) -&gt; StepContext:\n        time.sleep(0.1)  # Simulate expensive work\n        return ctx.replace(\n            metadata=MappingProxyType({**ctx.metadata, \"result\": \"done\"})\n        )\n\n\npipe = Pipeline().then(SlowStep())\nsamples = [StepContext(sample=f\"s{i}\") for i in range(6)]\n\n# Sequential: 6 \u00d7 0.1s \u2248 0.6s\nresults = pipe.run(samples, workers=1)\n\n# Parallel: 0.1s (all 6 run at once)\nresults = pipe.run(samples, workers=6)\n</code></pre> <p>Under the hood, <code>workers</code> creates an <code>asyncio.Semaphore</code> \u2014 at most N samples flow through the foreground steps at any given time.</p>"},{"location":"pipeline/execution/#async-steps-non-blocking-io","title":"Async steps \u2014 non-blocking I/O","text":"<p>A step that makes network calls (HTTP requests, API calls, subprocess) can be defined as a coroutine to avoid blocking the thread:</p> Sync stepAsync step <pre><code>class FetchStep:\n    requires = frozenset()\n    provides = frozenset({\"response\"})\n\n    def __call__(self, ctx: StepContext) -&gt; StepContext:\n        response = requests.get(ctx.sample)  # blocks the thread\n        return ctx.replace(\n            metadata=MappingProxyType({**ctx.metadata, \"response\": response})\n        )\n</code></pre> <pre><code>class FetchStep:\n    requires = frozenset()\n    provides = frozenset({\"response\"})\n\n    async def __call__(self, ctx: StepContext) -&gt; StepContext:\n        async with aiohttp.ClientSession() as session:\n            response = await session.get(ctx.sample)  # yields the thread\n        return ctx.replace(\n            metadata=MappingProxyType({**ctx.metadata, \"response\": response})\n        )\n</code></pre> <p>The pipeline detects async steps automatically via <code>asyncio.iscoroutinefunction</code> and awaits them. Sync steps are wrapped with <code>asyncio.to_thread()</code> so they're safe in an async context too.</p> <p>Note</p> <p>Async steps are about not blocking the thread, not about parallelism. The pipeline is still sequential \u2014 it just yields the thread during I/O waits.</p>"},{"location":"pipeline/execution/#async-boundary-fire-and-forget-background","title":"Async boundary \u2014 fire-and-forget background","text":"<p>Problem: Some steps are slow (e.g. LLM calls for analysis). Waiting for them before starting the next sample hurts throughput.</p> <p>Solution: A step declares <code>async_boundary = True</code>. Everything from that step onward runs in a background thread. The pipeline loop moves to the next sample immediately.</p> <pre><code>class SlowScoreStep:\n    requires = frozenset({\"tokens\"})\n    provides = frozenset({\"score\"})\n    async_boundary = True    # hand off to background from here\n    max_workers = 3          # up to 3 scoring threads in parallel\n\n    def __call__(self, ctx: StepContext) -&gt; StepContext:\n        time.sleep(0.5)  # Expensive scoring\n        score = len(ctx.metadata[\"tokens\"]) * 10\n        return ctx.replace(\n            metadata=MappingProxyType({**ctx.metadata, \"score\": score})\n        )\n</code></pre> <pre><code>graph LR\n    A1[Tokenize] --&gt; B1[Uppercase] --&gt;|async_boundary| C1[SlowScore]\n\n    style A1 fill:#6366f1,stroke:#4f46e5,color:#fff\n    style B1 fill:#6366f1,stroke:#4f46e5,color:#fff\n    style C1 fill:#3b82f6,stroke:#2563eb,color:#fff</code></pre> <p>Indigo = foreground (returns immediately) \u00b7 Blue = background (fire-and-forget)</p> <p>Multiple samples flow through this simultaneously \u2014 sample 2 starts its foreground steps while sample 1's background steps are still running.</p>"},{"location":"pipeline/execution/#using-the-boundary","title":"Using the boundary","text":"<pre><code>pipe = Pipeline().then(Tokenize()).then(Uppercase()).then(SlowScoreStep())\n\n# run() returns immediately after foreground steps (Tokenize + Uppercase)\nresults = pipe.run(samples, workers=4)\n\n# Background scoring continues \u2014 results not yet populated\nprint(pipe.background_stats())\n# {'active': 3, 'completed': 1}\n\n# Block until all background work finishes\npipe.wait_for_background(timeout=30.0)\n\n# Now all SampleResult.output fields are fully populated\nfor r in results:\n    print(r.output.metadata[\"score\"])\n</code></pre>"},{"location":"pipeline/execution/#background-pool-model","title":"Background pool model","text":"<p>Each step class has a single shared <code>ThreadPoolExecutor</code>:</p> <ul> <li><code>SlowScoreStep.max_workers = 3</code> means one pool of 3 threads for all <code>SlowScoreStep</code> instances, regardless of how many pipelines are running</li> <li>The pool is created lazily at first use and persists for the process lifetime</li> <li>If two users need different concurrency limits for the same step type, they should subclass</li> </ul> <p>Boundary rules</p> <ul> <li>One boundary per pipeline. If multiple steps declare <code>async_boundary = True</code>, the pipeline raises <code>PipelineConfigError</code> at construction time.</li> <li>No boundary inside Branch children. A boundary inside a branch child raises <code>PipelineConfigError</code>. Branch children always block until joined \u2014 detaching mid-branch is incoherent.</li> <li>Nested pipeline boundary is ignored. When a pipeline is used as a step inside another pipeline, <code>async_boundary</code> is warned and ignored \u2014 there is no \"next sample\" to move to from the outer pipeline's perspective.</li> </ul>"},{"location":"pipeline/execution/#workers-vs-max_workers-independent-pools","title":"<code>workers</code> vs <code>max_workers</code> \u2014 independent pools","text":"<p>These two knobs control different thread pools and do not interact:</p> Knob Pool Controls <code>pipe.run(contexts, workers=N)</code> foreground pool How many samples run through pre-boundary steps simultaneously <code>step.max_workers = K</code> background pool (per step class) How many instances of that step run in the background simultaneously <p>A sample leaves the foreground pool when it crosses the <code>async_boundary</code> and enters the background step's pool.</p> <p>Mental model: <code>workers</code> controls throughput into the pipeline; <code>max_workers</code> controls throughput through each background step.</p> <p>Rate limits</p> <p><code>workers</code> and <code>max_workers</code> are independent pools, but total concurrent outbound calls = foreground calls + background calls. With <code>workers=4</code> and <code>max_workers=3</code>, up to 7 requests may be in-flight simultaneously. Account for this when configuring per-provider rate limits.</p>"},{"location":"pipeline/execution/#rule-of-thumb","title":"Rule of thumb","text":"Question Answer Does the step wait on I/O? <code>async def __call__</code> Do I want to process more samples while previous ones are still in background steps? <code>async_boundary = True</code> on the handoff step Can two steps on the same sample run simultaneously? <code>Branch</code> Do I want N samples going through the pipeline at the same time? <code>workers=N</code> on <code>run()</code>"},{"location":"pipeline/quick-start/","title":"Quick Start","text":"<p>Build and run your first pipeline in under 30 lines.</p>"},{"location":"pipeline/quick-start/#define-two-steps","title":"Define two steps","text":"<p>Every step needs three things: <code>requires</code>, <code>provides</code>, and a <code>__call__</code> method.</p> <pre><code>from types import MappingProxyType\nfrom pipeline import Pipeline, StepContext\n\n\nclass Tokenize:\n    \"\"\"Split text into tokens and count words.\"\"\"\n    requires = frozenset()\n    provides = frozenset({\"tokens\", \"word_count\"})\n\n    def __call__(self, ctx: StepContext) -&gt; StepContext:\n        tokens = str(ctx.sample).split()\n        return ctx.replace(\n            metadata=MappingProxyType({\n                **ctx.metadata,\n                \"tokens\": tokens,\n                \"word_count\": len(tokens),\n            })\n        )\n\n\nclass Uppercase:\n    \"\"\"Convert tokens to uppercase.\"\"\"\n    requires = frozenset({\"tokens\"})\n    provides = frozenset({\"upper_tokens\"})\n\n    def __call__(self, ctx: StepContext) -&gt; StepContext:\n        upper = [t.upper() for t in ctx.metadata[\"tokens\"]]\n        return ctx.replace(\n            metadata=MappingProxyType({**ctx.metadata, \"upper_tokens\": upper})\n        )\n</code></pre>"},{"location":"pipeline/quick-start/#build-and-run","title":"Build and run","text":"<p>Chain steps with <code>.then()</code> and run with a list of contexts:</p> <pre><code>pipe = Pipeline().then(Tokenize()).then(Uppercase())\n\nresults = pipe.run([\n    StepContext(sample=\"hello world\"),\n    StepContext(sample=\"pipeline engine demo\"),\n])\n</code></pre> <p>The pipeline validates ordering at construction time \u2014 if <code>Uppercase</code> came before <code>Tokenize</code>, you'd get a <code>PipelineOrderError</code> immediately, not at runtime.</p>"},{"location":"pipeline/quick-start/#inspect-results","title":"Inspect results","text":"<p>Every sample produces exactly one <code>SampleResult</code>:</p> <pre><code>for r in results:\n    if r.error:\n        print(f\"Failed at {r.failed_at}: {r.error}\")\n    else:\n        print(f\"Sample: {r.sample}\")\n        print(f\"Tokens: {r.output.metadata['upper_tokens']}\")\n        print(f\"Count:  {r.output.metadata['word_count']}\")\n</code></pre> <pre><code>Sample: hello world\nTokens: ['HELLO', 'WORLD']\nCount:  2\n\nSample: pipeline engine demo\nTokens: ['PIPELINE', 'ENGINE', 'DEMO']\nCount:  3\n</code></pre>"},{"location":"pipeline/quick-start/#add-parallelism-with-branch","title":"Add parallelism with Branch","text":"<p>Run independent steps simultaneously with <code>Branch</code>:</p> <pre><code>from pipeline import MergeStrategy\n\n\nclass Reverse:\n    \"\"\"Reverse each token.\"\"\"\n    requires = frozenset({\"tokens\"})\n    provides = frozenset({\"reversed_tokens\"})\n\n    def __call__(self, ctx: StepContext) -&gt; StepContext:\n        rev = [t[::-1] for t in ctx.metadata[\"tokens\"]]\n        return ctx.replace(\n            metadata=MappingProxyType({**ctx.metadata, \"reversed_tokens\": rev})\n        )\n\n\npipe = (\n    Pipeline()\n    .then(Tokenize())\n    .branch(\n        Pipeline().then(Uppercase()),    # runs in parallel\n        Pipeline().then(Reverse()),      # runs in parallel\n        merge=MergeStrategy.RAISE_ON_CONFLICT,\n    )\n)\n\nresults = pipe.run([StepContext(sample=\"fork join\")])\n\nmeta = results[0].output.metadata\nprint(meta[\"upper_tokens\"])     # ['FORK', 'JOIN']\nprint(meta[\"reversed_tokens\"])  # ['krof', 'nioj']\n</code></pre> <p>Both branches write to different fields (<code>upper_tokens</code> vs <code>reversed_tokens</code>), so <code>RAISE_ON_CONFLICT</code> passes through without raising.</p>"},{"location":"pipeline/quick-start/#fire-and-forget-with-async_boundary","title":"Fire-and-forget with async_boundary","text":"<p>Some steps are slow and don't need to block the caller. Mark a step with <code>async_boundary = True</code> to hand everything from that point onward to a background thread \u2014 <code>run()</code> returns immediately after the foreground steps.</p> <pre><code>import time\n\n\nclass SlowScore:\n    \"\"\"Expensive scoring that runs in the background.\"\"\"\n    requires = frozenset({\"tokens\"})\n    provides = frozenset({\"score\"})\n    async_boundary = True   # everything from here runs in background\n    max_workers = 3         # up to 3 background threads\n\n    def __call__(self, ctx: StepContext) -&gt; StepContext:\n        time.sleep(0.5)  # simulate slow work\n        score = ctx.metadata[\"word_count\"] * 10\n        return ctx.replace(\n            metadata=MappingProxyType({**ctx.metadata, \"score\": score})\n        )\n\n\npipe = Pipeline().then(Tokenize()).then(SlowScore())\n\n# Returns immediately \u2014 only Tokenize runs in the foreground\nresults = pipe.run([\n    StepContext(sample=\"hello world\"),\n    StepContext(sample=\"background processing demo\"),\n])\n\n# Background scoring still running...\nprint(pipe.background_stats())  # {'active': 2, 'completed': 0}\n\n# Block until background work finishes\npipe.wait_for_background(timeout=10.0)\n\n# Now results are fully populated\nfor r in results:\n    print(f\"{r.sample}: score={r.output.metadata['score']}\")\n</code></pre> <pre><code>hello world: score=20\nbackground processing demo: score=30\n</code></pre> <p>See Execution Model for the full concurrency model \u2014 <code>workers</code> vs <code>max_workers</code>, async steps, and boundary rules.</p>"},{"location":"pipeline/quick-start/#try-it-interactively","title":"Try it interactively","text":"<p>All the examples on this page (and more) are available as a runnable Jupyter notebook:</p> <p> Open the Pipeline Demo Notebook</p>"},{"location":"pipeline/quick-start/#next-steps","title":"Next steps","text":"<ul> <li>Core Concepts \u2014 Understand the contract system and how validation works</li> <li>Execution Model \u2014 Learn about async steps, <code>async_boundary</code>, and workers</li> <li>Branching &amp; Parallelism \u2014 Deep dive into merge strategies and error handling</li> <li>Building Custom Steps \u2014 Dependency injection, testing, and common patterns</li> </ul>"}]}